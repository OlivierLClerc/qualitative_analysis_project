{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will perform **automatic classification of textual data** using **Large Language Models (LLMs)**.\n",
    "\n",
    "The dataset we'll be working with requires binary labels (`0` or `1`), meaning this is a **binary classification task**, where each data entry is assigned to one of two predefined categories.\n",
    "In our case, the classification process is **sequential**: multiple binary decisions are made sequentially, and the final label depends on the outcome of earlier ones.  \n",
    "Because of this interdependency between steps, we refer to it as a **complex case classification**.\n",
    "\n",
    "To help you navigate this notebook, here is a step-by-step outline of what we will do:\n",
    "\n",
    "1. **Getting started**  \n",
    "   - Download and install the project and its dependencies, load import and your API key.\n",
    "\n",
    "2. **Load and preprocess the dataset**  \n",
    "   - Upload, explore and pre-process the dataset, with the sample dataset (recommended for a first use) or your own data.\n",
    "\n",
    "3. **Prompt construction and classification on manually annotated data**  \n",
    "\n",
    "4. **Evaluating Model Performance Against Human Annotations**  \n",
    "   - Compute metrics (e.g., **Cohen's Kappa**, **Alt-Test**, ...)\n",
    "\n",
    "5. **Final Step: Classify the Full Dataset**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Before we begin, let's set up the environment by cloning the project and installing the necessary dependencies.\n",
    "\n",
    "### Step 1: Clone the Project\n",
    "Run the following cell to download the project files.\n",
    "This will download the project folder into Colab and switch the working directory to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/OlivierLClerc/qualitative_analysis_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Install Required Libraries\n",
    "Now, install the project and its dependencies.\n",
    "\n",
    "‚ö†Ô∏è Note:\n",
    "\n",
    "- This will install all required libraries for the notebook to run.\n",
    "- If Colab suggests restarting the runtime, click \"Restart Runtime\" and re-run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd qualitative_analysis_project\n",
    "%pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load Your API Key\n",
    "\n",
    "To use an LLM for analysis, you need to provide your **API key**. This key allows secure access to the API.\n",
    "\n",
    "You can use this pipeline with an **OpenAI**, **Gemini**, or **Anthropic** key.  \n",
    "The code in the cell below is currently configured for **OpenAI**.\n",
    "\n",
    "If you're using another provider, simply replace all occurrences of `OPENAI_API_KEY` with the corresponding variable name:  \n",
    "- For **Gemini** ‚Üí `GEMINI_API_KEY`  \n",
    "- For **Anthropic** ‚Üí `ANTHROPIC_API_KEY`\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "1. Click on the **üîë \"Key\" icon** on the left sidebar in Colab (**‚öôÔ∏è Settings** > **Secrets**).  \n",
    "2. Click **\"Add a new secret\"**.  \n",
    "3. Enter the following:  \n",
    "   - **Name** ‚Üí `OPENAI_API_KEY`\n",
    "   - **Value** ‚Üí *Your API Key* (Get it from [OpenAI](https://platform.openai.com/account/api-keys))  \n",
    "4. Click **\"Save\"**.  \n",
    "\n",
    "#### Troubleshooting\n",
    "\n",
    "- **API Key not found?**  \n",
    "  - Double-check that the secret name is exactly **`OPENAI_API_KEY`**.  \n",
    "  - If the issue persists, **refresh the page** and rerun the cell.  \n",
    "\n",
    "- **Is My Key Secure?**  \n",
    "  - Yes! Colab's **Secrets Manager** encrypts your key and keeps it safe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Retrieve API keys securely from Colab Secrets\n",
    "API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Check if the API key was loaded\n",
    "if API_KEY:\n",
    "    print(\"‚úÖ API Key loaded successfully!\")\n",
    "    os.environ['OPENAI_API_KEY'] = API_KEY\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è API Key not found. Please check the Secrets panel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import Project Modules\n",
    "\n",
    "Now that the project is installed, let's import the necessary modules and functions from the `qualitative_analysis` package. These tools will help us load data, process text, and perform binary classification analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qualitative_analysis import (\n",
    "    load_data,\n",
    "    clean_and_normalize,\n",
    "    sanitize_dataframe,\n",
    ")\n",
    "import os\n",
    "from qualitative_analysis.scenario_runner import run_scenarios\n",
    "from qualitative_analysis.evaluation import (\n",
    "    compute_kappa_metrics,\n",
    "    run_alt_test_on_results,\n",
    "    compute_classification_metrics_from_results\n",
    ")\n",
    "from qualitative_analysis.metrics.krippendorff import (\n",
    "    compute_krippendorff_non_inferiority,\n",
    "    print_non_inferiority_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the data (can be a CSV file or an xlsx file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>key</th>\n",
       "      <th>reference</th>\n",
       "      <th>IDENTIFY</th>\n",
       "      <th>GUESS</th>\n",
       "      <th>SEEK</th>\n",
       "      <th>ASSESS</th>\n",
       "      <th>identify_cues</th>\n",
       "      <th>guess_cues</th>\n",
       "      <th>seek_cues</th>\n",
       "      <th>assess_cues</th>\n",
       "      <th>Identify_validity</th>\n",
       "      <th>Guess_validity</th>\n",
       "      <th>Seek_validity</th>\n",
       "      <th>Assess_validity</th>\n",
       "      <th>mechanical_rating</th>\n",
       "      <th>Rater_Oli</th>\n",
       "      <th>Rater_Gaia</th>\n",
       "      <th>Rater_Chloe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bc4</td>\n",
       "      <td>1</td>\n",
       "      <td>bc4_1</td>\n",
       "      <td>√Ä des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu‚Äôon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan. On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es. Au del√† de 10 000 ans, on peut dire que le volcan est √©teint.</td>\n",
       "      <td>le magma</td>\n",
       "      <td>La temp√©rature du magma d√©passe les 500c</td>\n",
       "      <td>Quelle est la temp√©rature du magma?</td>\n",
       "      <td>La temp√©rature du magma atteint les 1000c</td>\n",
       "      <td>{\"1\":\"Les autres couches de la terre\",\"2\":\"La temp√©rature du magma\",\"3\":\"La temp√©rature √† l'√©ruption\"}</td>\n",
       "      <td>{\"1\":\"Il existe d'autres couches dans la terre √† part le 'manteau'\",\"2\":\"La temp√©rature du magma d√©passe les 500¬∞C\",\"3\":\"La temp√©rature √† l'√©ruption d√©passe la temp√©rature du magma\"}</td>\n",
       "      <td>{\"1\":\"Quelles sont les autres couches de la terre ?\",\"2\":\"Quelle est la temp√©rature du magma ?\",\"3\":\"Quelle est la temp√©rature √† l'√©ruption d'un volcan ?\"}</td>\n",
       "      <td>{\"1\":\"La terre contient 7 couches\",\"2\":\"La temp√©rature du magma atteint les 1000¬∞C\",\"3\":\"La temp√©rature √† l'√©ruption d'un volcan est √† 700¬∞C\"}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bc4</td>\n",
       "      <td>6</td>\n",
       "      <td>bc4_6</td>\n",
       "      <td>La religion de la Gr√®ce antique comprend plusieurs dieux. Les aventures de ces dieux forment la mythologie grecque, l'une des mythologies les plus d√©velopp√©es de l'histoire antique. D'apr√®s les Grecs, les 12 principaux dieux vivent sur l'Olympe et ont une apparence et un comportement comparables √† celui des humains. Les dieux les plus connus sont : Ath√©na, d√©esse de la guerre, Zeus, roi des dieux et dieu de la foudre. H√©ra, d√©esse du mariage. Pos√©idon, dieu des oc√©ans.</td>\n",
       "      <td>Une mythologie</td>\n",
       "      <td>Une mythologie est un ensemble de contes imaginaires</td>\n",
       "      <td>Qu'est-ce qu'une mythologie</td>\n",
       "      <td>Une mythologie est un ensemble de l√©gendes li√©s √† une civilisation bien pr√©cise</td>\n",
       "      <td>{\"1\":\"Une mythologie\",\"2\":\"Les autres mythologies\",\"3\":\"L'Olympe\"}</td>\n",
       "      <td>{\"1\":\"Une mythologie est un ensemble de contes imaginaires\",\"2\":\"Il y a plusieurs autres mythologies; comme l'Egyptienne\",\"3\":\"L'Olympe est le ch√¢teau des dieux grecs\"}</td>\n",
       "      <td>{\"1\":\"Qu'est ce qu'une mythologie ?\",\"2\":\"Quelles sont les autres mythologies ?\",\"3\":\"Qu'est ce que l'Olympe ?\"}</td>\n",
       "      <td>{\"1\":\"Une mythologie est un ensemble de l√©gendes li√©es √† une civilisation\",\"2\":\"L'histoire comprend beaucoup de mythologies\",\"3\":\"L'Olympe est la plus haute montagne de Gr√®ce\"}</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bc4</td>\n",
       "      <td>7</td>\n",
       "      <td>bc4_7</td>\n",
       "      <td>La Rome antique d√©signe l'histoire de la cit√© de Rome pendant l'Antiquit√©. Selon la l√©gende, Rome aurait √©t√© fond√©e par Romulus qui aurai donn√© son nom √† la ville. Au d√©part, ce n'√©tait qu'un groupe de quelques villages, puis l'Empire romain va couvrir une grande partie de l'Europe et entourer toute la mer M√©diterran√©e. La religion romaine comptait de nombreux dieux, inspir√©s en partie des dieux grecs.</td>\n",
       "      <td>Romulus</td>\n",
       "      <td>Romulus est un roi de Rome</td>\n",
       "      <td>Qui et Romulus mythologie √†</td>\n",
       "      <td>Dans la l√©gende Romulus et le premier roi de Rome</td>\n",
       "      <td>{\"1\":\"Romulus\",\"2\":\"Date du d√©but de l'antiquit√©\",\"3\":\"La mer M√©diterran√©e\"}</td>\n",
       "      <td>{\"1\":\"Romulus est un roi de Rome\",\"2\":\"L'Antiquit√© a commenc√© avant j.C\",\"3\":\"La mer M√©diterran√©e entoure l'Europe\"}</td>\n",
       "      <td>{\"1\":\"Qui est Romulus ?\",\"2\":\"Quand est-ce qu' a commenc√© l'√¢ge antique ?\",\"3\":\"O√π se trouve la mer M√©diterran√©e ?\"}</td>\n",
       "      <td>{\"1\":\"Dans la l√©gende, Romulus est le premier roi de Rome\",\"2\":\"L'Antiquit√© s'est √©tendue entre 3200 avant J.C, jusqu'√† l'ann√©e 476\",\"4\":\"La M√©diterran√©e touche plusieurs continents\"}</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bc4</td>\n",
       "      <td>8</td>\n",
       "      <td>bc4_8</td>\n",
       "      <td>La for√™t tropicale humide a un climat qui fournit beaucoup d'eau et une temp√©rature √©lev√©e toute l'ann√©e. Cela favorise la densit√© et la croissance permanente des plantes de ces for√™ts.  En Asie, on appelle ce genre de for√™t 'la jungle'. Elle est parfois appel√©e for√™t dense, bien que cette expression signifie seulement que les arbres sont tr√®s proches les uns des autres. Dans les for√™ts tropicales, les arbres formant trois √©tages de hauteur diff√©rentes, sont tr√®s serr√©s et perdent leurs feuilles irr√©guli√®rement.</td>\n",
       "      <td>Les plantes de for√™t tropicales</td>\n",
       "      <td>Les for√™t tropicale contiennent tout type de plantes</td>\n",
       "      <td>Quelle plante peut-on voir dans les for√™ts</td>\n",
       "      <td>Les for√™t tropicale sont toujours vertes car il pleut tout le temps</td>\n",
       "      <td>{\"1\":\"Les plantes des for√™ts tropicales\",\"2\":\"Temp√©rature des for√™ts tropicales\",\"3\":\"Lieux des for√™ts tropicales\"}</td>\n",
       "      <td>{\"1\":\"Les for√™ts tropicales contiennent toutes les plantes, comme les autres for√™ts\",\"2\":\"La temp√©rature dans les for√™ts tropicales d√©passe les 20¬∞C\",\"3\":\"Les for√™ts tropicales sont seulement en Asie\"}</td>\n",
       "      <td>{\"1\":\"Quelles plantes peut-on voir dans les for√™ts tropicales ?\",\"2\":\"Quelle est la temp√©rature dans une for√™ts tropicale ?\",\"3\":\"O√π se trouvent les for√™ts tropicales ? \"}</td>\n",
       "      <td>{\"4\":\"Les for√™ts tropicales sont toujours vertes car il pleut tout le temps\",\"2\":\"La temp√©rature dans les for√™ts tropicales est entre 30¬∞ et 40¬∞C\",\"3\":\"La for√™t tropicale se trouve en Asie, en Am√©rique du Sud et en Afrique\"}</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bc5</td>\n",
       "      <td>1</td>\n",
       "      <td>bc5_1</td>\n",
       "      <td>√Ä des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu‚Äôon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan. On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es. Au del√† de 10 000 ans, on peut dire que le volcan est √©teint.</td>\n",
       "      <td>Pourqoi on l'appelle le manteau.</td>\n",
       "      <td>le magma et une pierre.</td>\n",
       "      <td>quel et la temperature magma</td>\n",
       "      <td>1 000c</td>\n",
       "      <td>{\"1\":\"Les autres couches de la terre\",\"2\":\"La temp√©rature du magma\",\"3\":\"La temp√©rature √† l'√©ruption\"}</td>\n",
       "      <td>{\"1\":\"Il existe d'autres couches dans la terre √† part le 'manteau'\",\"2\":\"La temp√©rature du magma d√©passe les 500¬∞C\",\"3\":\"La temp√©rature √† l'√©ruption d√©passe la temp√©rature du magma\"}</td>\n",
       "      <td>{\"1\":\"Quelles sont les autres couches de la terre ?\",\"2\":\"Quelle est la temp√©rature du magma ?\",\"3\":\"Quelle est la temp√©rature √† l'√©ruption d'un volcan ?\"}</td>\n",
       "      <td>{\"1\":\"La terre contient 7 couches\",\"2\":\"La temp√©rature du magma atteint les 1000¬∞C\",\"3\":\"La temp√©rature √† l'√©ruption d'un volcan est √† 700¬∞C\"}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name  Iteration    key  \\\n",
       "0  bc4          1  bc4_1   \n",
       "1  bc4          6  bc4_6   \n",
       "2  bc4          7  bc4_7   \n",
       "3  bc4          8  bc4_8   \n",
       "4  bc5          1  bc5_1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               reference  \\\n",
       "0                         √Ä des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu‚Äôon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan. On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es. Au del√† de 10 000 ans, on peut dire que le volcan est √©teint.   \n",
       "1                                              La religion de la Gr√®ce antique comprend plusieurs dieux. Les aventures de ces dieux forment la mythologie grecque, l'une des mythologies les plus d√©velopp√©es de l'histoire antique. D'apr√®s les Grecs, les 12 principaux dieux vivent sur l'Olympe et ont une apparence et un comportement comparables √† celui des humains. Les dieux les plus connus sont : Ath√©na, d√©esse de la guerre, Zeus, roi des dieux et dieu de la foudre. H√©ra, d√©esse du mariage. Pos√©idon, dieu des oc√©ans.   \n",
       "2                                                                                                                  La Rome antique d√©signe l'histoire de la cit√© de Rome pendant l'Antiquit√©. Selon la l√©gende, Rome aurait √©t√© fond√©e par Romulus qui aurai donn√© son nom √† la ville. Au d√©part, ce n'√©tait qu'un groupe de quelques villages, puis l'Empire romain va couvrir une grande partie de l'Europe et entourer toute la mer M√©diterran√©e. La religion romaine comptait de nombreux dieux, inspir√©s en partie des dieux grecs.   \n",
       "3  La for√™t tropicale humide a un climat qui fournit beaucoup d'eau et une temp√©rature √©lev√©e toute l'ann√©e. Cela favorise la densit√© et la croissance permanente des plantes de ces for√™ts.  En Asie, on appelle ce genre de for√™t 'la jungle'. Elle est parfois appel√©e for√™t dense, bien que cette expression signifie seulement que les arbres sont tr√®s proches les uns des autres. Dans les for√™ts tropicales, les arbres formant trois √©tages de hauteur diff√©rentes, sont tr√®s serr√©s et perdent leurs feuilles irr√©guli√®rement.   \n",
       "4                         √Ä des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu‚Äôon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan. On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es. Au del√† de 10 000 ans, on peut dire que le volcan est √©teint.   \n",
       "\n",
       "                           IDENTIFY  \\\n",
       "0                          le magma   \n",
       "1                    Une mythologie   \n",
       "2                           Romulus   \n",
       "3   Les plantes de for√™t tropicales   \n",
       "4  Pourqoi on l'appelle le manteau.   \n",
       "\n",
       "                                                  GUESS  \\\n",
       "0              La temp√©rature du magma d√©passe les 500c   \n",
       "1  Une mythologie est un ensemble de contes imaginaires   \n",
       "2                            Romulus est un roi de Rome   \n",
       "3  Les for√™t tropicale contiennent tout type de plantes   \n",
       "4                               le magma et une pierre.   \n",
       "\n",
       "                                         SEEK  \\\n",
       "0         Quelle est la temp√©rature du magma?   \n",
       "1                 Qu'est-ce qu'une mythologie   \n",
       "2                 Qui et Romulus mythologie √†   \n",
       "3  Quelle plante peut-on voir dans les for√™ts   \n",
       "4                quel et la temperature magma   \n",
       "\n",
       "                                                                            ASSESS  \\\n",
       "0                                        La temp√©rature du magma atteint les 1000c   \n",
       "1  Une mythologie est un ensemble de l√©gendes li√©s √† une civilisation bien pr√©cise   \n",
       "2                                Dans la l√©gende Romulus et le premier roi de Rome   \n",
       "3              Les for√™t tropicale sont toujours vertes car il pleut tout le temps   \n",
       "4                                                                           1 000c   \n",
       "\n",
       "                                                                                                         identify_cues  \\\n",
       "0               {\"1\":\"Les autres couches de la terre\",\"2\":\"La temp√©rature du magma\",\"3\":\"La temp√©rature √† l'√©ruption\"}   \n",
       "1                                                   {\"1\":\"Une mythologie\",\"2\":\"Les autres mythologies\",\"3\":\"L'Olympe\"}   \n",
       "2                                         {\"1\":\"Romulus\",\"2\":\"Date du d√©but de l'antiquit√©\",\"3\":\"La mer M√©diterran√©e\"}   \n",
       "3  {\"1\":\"Les plantes des for√™ts tropicales\",\"2\":\"Temp√©rature des for√™ts tropicales\",\"3\":\"Lieux des for√™ts tropicales\"}   \n",
       "4               {\"1\":\"Les autres couches de la terre\",\"2\":\"La temp√©rature du magma\",\"3\":\"La temp√©rature √† l'√©ruption\"}   \n",
       "\n",
       "                                                                                                                                                                                                  guess_cues  \\\n",
       "0                     {\"1\":\"Il existe d'autres couches dans la terre √† part le 'manteau'\",\"2\":\"La temp√©rature du magma d√©passe les 500¬∞C\",\"3\":\"La temp√©rature √† l'√©ruption d√©passe la temp√©rature du magma\"}   \n",
       "1                                   {\"1\":\"Une mythologie est un ensemble de contes imaginaires\",\"2\":\"Il y a plusieurs autres mythologies; comme l'Egyptienne\",\"3\":\"L'Olympe est le ch√¢teau des dieux grecs\"}   \n",
       "2                                                                                       {\"1\":\"Romulus est un roi de Rome\",\"2\":\"L'Antiquit√© a commenc√© avant j.C\",\"3\":\"La mer M√©diterran√©e entoure l'Europe\"}   \n",
       "3  {\"1\":\"Les for√™ts tropicales contiennent toutes les plantes, comme les autres for√™ts\",\"2\":\"La temp√©rature dans les for√™ts tropicales d√©passe les 20¬∞C\",\"3\":\"Les for√™ts tropicales sont seulement en Asie\"}   \n",
       "4                     {\"1\":\"Il existe d'autres couches dans la terre √† part le 'manteau'\",\"2\":\"La temp√©rature du magma d√©passe les 500¬∞C\",\"3\":\"La temp√©rature √† l'√©ruption d√©passe la temp√©rature du magma\"}   \n",
       "\n",
       "                                                                                                                                                                     seek_cues  \\\n",
       "0                  {\"1\":\"Quelles sont les autres couches de la terre ?\",\"2\":\"Quelle est la temp√©rature du magma ?\",\"3\":\"Quelle est la temp√©rature √† l'√©ruption d'un volcan ?\"}   \n",
       "1                                                             {\"1\":\"Qu'est ce qu'une mythologie ?\",\"2\":\"Quelles sont les autres mythologies ?\",\"3\":\"Qu'est ce que l'Olympe ?\"}   \n",
       "2                                                         {\"1\":\"Qui est Romulus ?\",\"2\":\"Quand est-ce qu' a commenc√© l'√¢ge antique ?\",\"3\":\"O√π se trouve la mer M√©diterran√©e ?\"}   \n",
       "3  {\"1\":\"Quelles plantes peut-on voir dans les for√™ts tropicales ?\",\"2\":\"Quelle est la temp√©rature dans une for√™ts tropicale ?\",\"3\":\"O√π se trouvent les for√™ts tropicales ? \"}   \n",
       "4                  {\"1\":\"Quelles sont les autres couches de la terre ?\",\"2\":\"Quelle est la temp√©rature du magma ?\",\"3\":\"Quelle est la temp√©rature √† l'√©ruption d'un volcan ?\"}   \n",
       "\n",
       "                                                                                                                                                                                                                        assess_cues  \\\n",
       "0                                                                                    {\"1\":\"La terre contient 7 couches\",\"2\":\"La temp√©rature du magma atteint les 1000¬∞C\",\"3\":\"La temp√©rature √† l'√©ruption d'un volcan est √† 700¬∞C\"}   \n",
       "1                                                  {\"1\":\"Une mythologie est un ensemble de l√©gendes li√©es √† une civilisation\",\"2\":\"L'histoire comprend beaucoup de mythologies\",\"3\":\"L'Olympe est la plus haute montagne de Gr√®ce\"}   \n",
       "2                                           {\"1\":\"Dans la l√©gende, Romulus est le premier roi de Rome\",\"2\":\"L'Antiquit√© s'est √©tendue entre 3200 avant J.C, jusqu'√† l'ann√©e 476\",\"4\":\"La M√©diterran√©e touche plusieurs continents\"}   \n",
       "3  {\"4\":\"Les for√™ts tropicales sont toujours vertes car il pleut tout le temps\",\"2\":\"La temp√©rature dans les for√™ts tropicales est entre 30¬∞ et 40¬∞C\",\"3\":\"La for√™t tropicale se trouve en Asie, en Am√©rique du Sud et en Afrique\"}   \n",
       "4                                                                                    {\"1\":\"La terre contient 7 couches\",\"2\":\"La temp√©rature du magma atteint les 1000¬∞C\",\"3\":\"La temp√©rature √† l'√©ruption d'un volcan est √† 700¬∞C\"}   \n",
       "\n",
       "   Identify_validity  Guess_validity  Seek_validity  Assess_validity  \\\n",
       "0                NaN             2.0            2.0              2.0   \n",
       "1                1.0             1.0            1.0              1.0   \n",
       "2                1.0             1.0            1.0              1.0   \n",
       "3                1.0             1.0            1.0              4.0   \n",
       "4                NaN             NaN            2.0              NaN   \n",
       "\n",
       "   mechanical_rating  Rater_Oli  Rater_Gaia  Rater_Chloe  \n",
       "0                NaN          1           1            1  \n",
       "1                1.0          1           1            1  \n",
       "2                1.0          1           1            1  \n",
       "3                0.0          0           0            0  \n",
       "4                NaN          0           0            0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define data directory\n",
    "data_dir = 'data/complex_user_case'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Define the path to your dataset\n",
    "data_file_path = os.path.join(data_dir, 'complex_data_sample.xlsx')\n",
    "\n",
    "# Load the data\n",
    "data = load_data(data_file_path, file_type='xlsx', delimiter=';')\n",
    "\n",
    "# Preview the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    "The dataset provided for this notebook is an anonymized subset from the study available at [X]. In the original experiment, children were tasked with reading a reference text and engaging in four sequential interactions with an interactive app. The goal of these steps was to help the children formulate a divergent question. A question is considered divergent if its answer is not explicitly stated in the reference text.\n",
    "\n",
    "The four steps, include:\n",
    "1. **Identify**: The child identifies a knowledge gap related to the reference text.\n",
    "2. **Guess**: The child makes a guess about what the answer to the knowledge gap could be.\n",
    "3. **Seek**: The child formulates a question to seek the answer.\n",
    "4. **Assess**: The child evaluates whether the app provides an answer to their question.\n",
    "\n",
    "This process is called a **cycle**. An annotator evaluates the validity of a cycle by answering a series of binary Yes/No questions (binary classifications). A cycle is deemed valid if all binary questions can be answered by \"Yes\"; otherwise, it is considered invalid. For more details, see the codebook provided in the prompt cell.\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "The dataset includes the following key components:\n",
    "- **`ref`**: The reference text the child read before interacting with the app  \n",
    "- **`IDENTIFY`**: The child's response in the Identify step  \n",
    "- **`GUESS`**: The child's guess in the Guess step  \n",
    "- **`SEEK`**: The question formulated in the Seek step  \n",
    "- **`ASSESS`**: The child's evaluation of the app's response\n",
    "\n",
    "To classify a cycle, both the reference text and the entries from all four steps are required.\n",
    "\n",
    "Each cycle is labeled by **three independent annotators**.  \n",
    "These annotations allow us to compute **inter-annotator agreement** and assess the **reliability** of the evaluation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Preprocessing  (Optional, improve clarity and consistency of text data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Rename key columns**  \n",
    "   Give important columns more descriptive names  \n",
    "   (e.g. `ref` ‚Üí `reference`).\n",
    "\n",
    "2. **Clean textual data**  \n",
    "   For each text column, run `clean_and_normalize(series)` to  \n",
    "   - trim leading/trailing spaces  \n",
    "   - convert accented characters to plain ASCII (e.g. `'√©'` ‚Üí `'e'`).\n",
    "\n",
    "3. **Convert to integers**  \n",
    "   Convert selected columns to integers using `pd.to_numeric(...).astype(\"Int64\")` to preserve missing values.\n",
    "\n",
    "4. **Sanitize line breaks**  \n",
    "   Run `sanitize_dataframe(df)` to replace newline (`\\n`) and carriage‚Äëreturn (`\\r`) characters with a single space in every string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a) Define a mapping from old column names to new names\n",
    "rename_map = {\n",
    "    \"ref\": \"reference\",\n",
    "    \"IDENTIFY\": \"identify\",\n",
    "    \"GUESS\": \"guess\",\n",
    "    \"SEEK\": \"seek\",\n",
    "    \"ASSESS\": \"assess\"\n",
    "}\n",
    "\n",
    "# 1b) Rename the columns in the DataFrame\n",
    "data = data.rename(columns=rename_map)\n",
    "\n",
    "# 2) Now define the new column names for cleaning\n",
    "text_columns = [\"reference\", \"identify\", \"guess\", \"seek\", \"assess\"]\n",
    "integer_columns = [\"Rater_Oli\", \"Rater_Chloe\", \"Rater_Gaia\", \"Identify_validity\", \"Guess_validity\", \"Seek_validity\", \"Assess_validity\", \"mechanical_rating\"]\n",
    "\n",
    "# 3) Clean and normalize the new columns\n",
    "for col in text_columns:\n",
    "    data[col] = clean_and_normalize(data[col])\n",
    "\n",
    "# 4) Convert selected columns to integers, preserving NaNs\n",
    "for col in integer_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# 5) Sanitize the DataFrame\n",
    "data = sanitize_dataframe(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Combine texts and questions\n",
    "\n",
    "To prepare the data for the LLM, we gather exactly the information a human annotator would need‚Äîplus the **ID** so we can merge results back into the original DataFrame.  \n",
    "The concatenated block of fields is called a **verbatim**.\n",
    "\n",
    "#### Create the `verbatim` field\n",
    "\n",
    "1. **Build verbatims**  \n",
    "   For every row we create a multi‚Äëline string containing:  \n",
    "   - the respondent **ID**  \n",
    "   - the cleaned **reference** text  \n",
    "   - the five cleaned prompt‚Äëresponse fields (**Identify**, **Guess**, **Seek**, **Assess**)  \n",
    "   Each section is separated by a blank line for readability, and the result is written to a new column named `verbatim`.\n",
    "\n",
    "2. **Sanity‚Äëcheck**  \n",
    "   - Print the total number of verbatims to ensure every row was processed.  \n",
    "   - Display the first verbatim as a spot‚Äëcheck of the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of verbatims: 12\n",
      "Verbatim example:\n",
      "Id: bc4_1\n",
      "\n",
      "Text: A des dizaines de kilometres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une eruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune eruption dans les 10 000 dernieres annees. Au dela de 10 000 ans, on peut dire que le volcan est eteint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temperature du magma depasse les 500c\n",
      "\n",
      "Seek: Quelle est la temperature du magma?\n",
      "\n",
      "Assess: La temperature du magma atteint les 1000c\n",
      "\n",
      "assess Cues: {\"1\":\"La terre contient 7 couches\",\"2\":\"La temp√©rature du magma atteint les 1000¬∞C\",\"3\":\"La temp√©rature √† l'√©ruption d'un volcan est √† 700¬∞C\"}\n",
      "\n",
      "Identify Validity: <NA>\n",
      "\n",
      "Guess Validity: 2\n",
      "\n",
      "Seek Validity: 2\n",
      "\n",
      "Assess Validity: 2\n",
      "\n",
      "mechanical Rating: <NA>\n"
     ]
    }
   ],
   "source": [
    "# Combine texts and entries\n",
    "\n",
    "data['verbatim'] = data.apply(\n",
    "    lambda row: (\n",
    "        f\"Id: {row['key']}\\n\\n\"\n",
    "        f\"Text: {row['reference']}\\n\\n\"\n",
    "        f\"Identify: {row['identify']}\\n\\n\"\n",
    "        f\"Guess: {row['guess']}\\n\\n\"\n",
    "        f\"Seek: {row['seek']}\\n\\n\"\n",
    "        f\"Assess: {row['assess']}\\n\\n\"\n",
    "        f\"assess Cues: {row['assess_cues']}\\n\\n\"\n",
    "        f\"Identify Validity: {row['Identify_validity']}\\n\\n\"\n",
    "        f\"Guess Validity: {row['Guess_validity']}\\n\\n\"\n",
    "        f\"Seek Validity: {row['Seek_validity']}\\n\\n\"\n",
    "        f\"Assess Validity: {row['Assess_validity']}\\n\\n\"\n",
    "        f\"mechanical Rating: {row['mechanical_rating']}\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Extract the list of verbatims\n",
    "verbatims = data['verbatim'].tolist()\n",
    "\n",
    "print(f\"Total number of verbatims: {len(verbatims)}\")\n",
    "print(f\"Verbatim example:\\n{verbatims[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt construction and classification on manually annotated data\n",
    "\n",
    "This framework allows you to evaluate different configurations to determine which prompt, model, and parameters yield the most accurate classification. These configurations are stored in the scenarios list.\n",
    "\n",
    "The snippet defines two **classification scenarios** for evaluating participants‚Äô ‚ÄúIdentify‚ÄØ‚Üí‚ÄØGuess‚ÄØ‚Üí‚ÄØSeek‚ÄØ‚Üí‚ÄØAssess‚Äù reasoning cycles with a Large Language Model (LLM).\n",
    "\n",
    "Each scenario is a dictionary inside the `scenarios` list and can be seen as a self‚Äëcontained _experiment_: it specifies\n",
    "\n",
    "* which LLM to call (`provider_llm1`, `model_name_llm1`, `temperature_llm1`);\n",
    "* the **prompt template** that tells the LLM how to judge a single data row;\n",
    "* the expected JSON output (fields listed in `selected_fields`);\n",
    "* optional settings for **prompt‚Äërefinement** by a second LLM (`provider_llm2`, ‚Ä¶).\n",
    "\n",
    "Running the pipeline iterates over every scenario and evaluates every (or a subsample of) data rows, then writes the chosen output fields back to your dataframe or file.\n",
    "\n",
    "### LLM Settings\n",
    "\n",
    "- `provider_llm1`: The LLM provider used for classification (`azure`, `openai`, `anthropic`, `gemini`)\n",
    "- `model_name_llm1`: The model used for classification. This depends on the provider.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "- **For** `azure` ‚Üí `\"gpt-4o\"` or `\"gpt-4o-mini\"`\n",
    "- **For** `openai` ‚Üí `\"gpt-4o\"` or `\"gpt-4o-mini\"`\n",
    "- **For** `anthropic` ‚Üí `\"claude-3-7-sonnet-20250219\"`, `\"claude-3-5-haiku-20241022\"`\n",
    "- **For** `gemini` ‚Üí `\"gemini-2.0-flash-001\"`, `\"gemini-2.5-pro-preview-03-25\"`\n",
    "\n",
    "- `temperature_llm1`: Controls output variability. Set to `0` for deterministic responses. Higher values add randomness (not recommended for evaluation tasks).\n",
    "- `subsample_size`: Number of entries to evaluate. Set to `-1` to use the entire dataset.\n",
    "\n",
    "### Prompt Configuration\n",
    "\n",
    "- `prompt_name`: A short name identifying the scenario, used in performance tracking.\n",
    "- `template`: The full prompt used to guide the LLM. It could include:\n",
    "  - The **role** of the assistant\n",
    "  - A **description** of the input columns\n",
    "  - The **evaluation codebook** (la mani√®re dont les donn√©es doivent etre classifi√©es)\n",
    "  - Optionally, **examples**\n",
    "  - ‚ö†Ô∏è **Must contain** the `{verbatim_text}` placeholder for the entry being evaluated\n",
    "\n",
    "### Output\n",
    "\n",
    "- `selected_fields`: The fields to extract from the LLM‚Äôs output (e.g., `\"Classification\"`, `\"Reasoning\"`).  \n",
    "  You can modify this to include or exclude elements (like adding confidence scores, removing reasonning).\n",
    "- `prefix`: The key to look for in the LLM output that contains the classification label (e.g., `\"Classification\"`).\n",
    "Nous sp√©cifions donc cela pour que le parsing du verdict soit plus facile, pour r√©cuperer les labels de classification.\n",
    "- `label_type`: Data type of the classification label. Typically `\"int\"` for binary classification (`0` or `1`),  \n",
    "  but can be changed to `\"float\"` or `\"str\"` as needed.\n",
    "- `response_template`: The required format of the LLM output (e.g., JSON). This ensures correct parsing. It is recommended not to change this format request.\n",
    "- `json_output`: If `True`, the LLM must respond in JSON. Disabling this is not recommended. If you do, you will have to  \n",
    "  change the `response_template` accordingly.\n",
    "\n",
    "\n",
    "### Prompt Optimization (In developpement - better not to change anything)\n",
    "\n",
    "This section enables **automatic prompt refinement** using a second LLM. It attempts to generate an improved version of the prompt to reduce classification errors.\n",
    "\n",
    "- A second model (`llm2`) is used to review the prompt given to the first model (`llm1`) and suggest changes based on classification failures.\n",
    "- If the new prompt performs better (fewer classification errors), it replaces the original.\n",
    "\n",
    "**Warning**: This can lead to overfitting ‚Äî the new prompt may work well on the training data but generalize poorly.  \n",
    "It's highly recommended to **use a validation set** when using this feature.\n",
    "\n",
    "### Prompt Optimization\n",
    "\n",
    "- `provider_llm2`: LLM provider used for prompt improvement\n",
    "- `model_name_llm2`: Name of the refinement model\n",
    "- `temperature_llm2`: Temperature for the prompt-refiner LLM\n",
    "- `max_iterations`: How many times the prompt should be revised.\n",
    "For example, if you choose 3, each data entry will be classified three times: once with the original prompt, and twice with newly generated prompts.\n",
    "- `use_validation_set`: Whether to use a separate validation set to monitor prompt overfitting (Boolean)\n",
    "- `validation_size`: Number of samples in the validation set\n",
    "- `random_state`: Random seed for reproducible train/validation split\n",
    "\n",
    "### Majority vote\n",
    "\n",
    "- `n_completions`: Number of completions per entry. \n",
    "It is possible to generate multiple responses for each entry using the same LLM. This will produce several classification labels for the same data point.\n",
    "The final label is determined by majority vote. Generating multiple completions can improve robustness but also increases cost.\n",
    "\n",
    "### Example\n",
    "\n",
    "In the current example, we define two scenarios:\n",
    "\n",
    "**Scenario 1**: Includes examples in the prompt (*few-shot*)\n",
    "\n",
    "**Scenario 2**: Contains only the codebook and instructions (*zero-shot*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "    {\n",
    "        # LLM settings\n",
    "        \"provider_llm1\": \"azure\",\n",
    "        \"model_name_llm1\": \"gpt-4o\",\n",
    "        \"temperature_llm1\": 0,\n",
    "        \"prompt_name\": \"few_shot\",\n",
    "        \"subsample_size\": -1,  # Size of data subset to use\n",
    "\n",
    "        # Prompt configuration\n",
    "        \"template\": \"\"\"\n",
    "Tu es un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
    "\n",
    "Les donn√©es comportent les colonnes suivantes :\n",
    "- \"key\" : Identifiant unique\n",
    "- \"Text\" : Le texte de r√©f√©rence que les participants doivent lire au pr√©alable. Leurs r√©ponses aux diff√©rentes √©tapes doivent √™tre s√©mantiquement li√©es √† ce texte (m√™me th√®me), mais la r√©ponse √† la question qu‚Äôils posent ne doit pas se trouver dans le texte.\n",
    "- \"Identify\" : R√©ponse pour l‚Äô√©tape IDENTIFY\n",
    "- \"Guess\" : R√©ponse pour l‚Äô√©tape GUESS\n",
    "- \"Seek\" : R√©ponse pour l‚Äô√©tape SEEK\n",
    "- \"Assess\" : R√©ponse pour l‚Äô√©tape ASSESS\n",
    "- \"assess_cues\" : R√©ponses possibles qui ont √©t√© propos√©es √† l‚Äô√©tape ASSESS\n",
    "- \"mechanical_rating\" : Si une valeur num√©rique est d√©j√† pr√©sente, elle doit √™tre utilis√©e comme √©tiquette finale (elle remplace toute autre logique du codebook)\n",
    "\n",
    "Voici une entr√©e √† √©valuer :\n",
    "{verbatim_text}\n",
    "\n",
    "Si une valeur num√©rique est pr√©sente dans la colonne mechanical_rating, copie cette valeur comme √©tiquette finale.\n",
    "Si elle est vide, tu dois d√©terminer la validit√© globale du cycle (0 ou 1) en suivant le codebook ci-dessous :\n",
    "\n",
    "Un cycle est consid√©r√© comme valide si tu peux r√©pondre ¬´ oui ¬ª √† toutes les questions suivantes :\n",
    "\n",
    "- Identify Step : L‚Äô√©tape IDENTIFY indique-t-elle un sujet d‚Äôint√©r√™t ?\n",
    "- Guess Step : L‚Äô√©tape GUESS propose-t-elle une explication possible ?\n",
    "- Seek Step : L‚Äô√©tape SEEK est-elle formul√©e sous forme de question ?\n",
    "- Assess Step : L‚Äô√©tape ASSESS propose-t-elle une r√©ponse possible ou indique-t-elle qu‚Äôaucune r√©ponse n‚Äôa √©t√© trouv√©e (¬´ no ¬ª est acceptable) ?\n",
    "- Consistency : Les √©tapes IDENTIFY, GUESS et SEEK sont-elles li√©es √† la m√™me question ?\n",
    "- Reference Link : Les √©tapes IDENTIFY, GUESS et SEEK sont-elles en lien avec le th√®me du texte de r√©f√©rence ?\n",
    "- Seek Question Originality : La r√©ponse √† la question pos√©e dans SEEK ne se trouve-t-elle pas (m√™me vaguement) dans le texte de r√©f√©rence ?\n",
    "- Resolving Answer : Si l‚Äô√©tape ASSESS contient une r√©ponse, r√©pond-elle bien √† la question pos√©e dans SEEK ?\n",
    "- Valid Answer : Si l‚Äô√©tape ASSESS affirme qu‚Äôune r√©ponse a √©t√© trouv√©e, cette r√©ponse figure-t-elle effectivement dans assess_cues ? ‚Üí Sinon, aucune r√©ponse n‚Äôa r√©ellement √©t√© trouv√©e, et le cycle est invalide.\n",
    "- Valid Non : Si l‚Äô√©tape ASSESS indique qu‚Äôaucune r√©ponse n‚Äôa √©t√© trouv√©e, v√©rifie que la r√©ponse √† la question SEEK ne figure effectivement pas dans assess_cues. ‚Üí Si le participant affirme qu‚Äôil n‚Äôy a pas de r√©ponse, mais qu‚Äôelle se trouve en r√©alit√© dans assess_cues, le cycle est invalide.\n",
    "\n",
    "Si tous ces crit√®res sont remplis, le cycle est valide.\n",
    "La validit√© est exprim√©e comme suit :\n",
    "1 : Cycle valide\n",
    "0 : Cycle invalide\n",
    "\n",
    "Examples\n",
    "Example 1:\n",
    "\n",
    "Id: bc4_1\n",
    "Text:\n",
    "A des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu'on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma.\n",
    "Le magma est responsable de l'explosion des volcans : c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan.\n",
    "On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es.\n",
    "Au-del√† de 10 000 ans, on peut dire que le volcan est √©teint.\n",
    "Identify: le magma\n",
    "Guess: La temp√©rature du magma d√©passe les 500‚ÄØ¬∞C\n",
    "Seek: Quelle est la temp√©rature du magma ?\n",
    "Assess: La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\n",
    "Assess Cues:\n",
    "‚ÄÉ\"1\" : \"La terre contient 7 couches\",\n",
    "‚ÄÉ\"2\" : \"La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\",\n",
    "‚ÄÉ\"3\" : \"La temp√©rature √† l'√©ruption d'un volcan est √† 700‚ÄØ¬∞C\"\n",
    "Identify Validity: <NA>\n",
    "Guess Validity: 2\n",
    "Seek Validity: 2\n",
    "Assess Validity: 2\n",
    "Mechanical Rating: <NA>\n",
    "\n",
    "Reasoning:\n",
    "Identify nomme ¬´ le magma ¬ª, indiquant clairement le sujet.\n",
    "Guess propose une affirmation plausible sur la temp√©rature du magma (>500‚ÄØ¬∞C).\n",
    "Seek est une question bien formul√©e qui interroge sur la temp√©rature du magma.\n",
    "Assess fournit une r√©ponse explicite (‚âà1000‚ÄØ¬∞C) qui r√©pond directement √† la question pos√©e dans Seek et correspond exactement √† l‚Äôoption 2 des assess_cues, donc la r√©ponse est consid√©r√©e comme ¬´ trouv√©e ¬ª.\n",
    "Les √©tapes Identify/Guess/Seek/Assess sont toutes centr√©es sur le m√™me th√®me (la temp√©rature du magma) et restent li√©es au sujet du texte de r√©f√©rence (magma/volcans), bien que la temp√©rature sp√©cifique ne soit pas donn√©e dans le texte, ce qui respecte le crit√®re d‚Äôoriginalit√©.\n",
    "Les drapeaux num√©riques marquent d√©j√† Guess, Seek et Assess comme valides ; Identify est √©galement valide de mani√®re ind√©pendante.\n",
    "Ainsi, chaque crit√®re de validit√© du codebook est respect√©.\n",
    "\n",
    "Classification: 1\n",
    "\n",
    "Example 2:\n",
    "\n",
    "Id : bc5_3\n",
    "Text :\n",
    "Toutankhamon √©tait un pharaon, un roi de l'√âgypte antique.\n",
    "Il est tr√®s connu aujourd‚Äôhui parce que des arch√©ologues ont retrouv√© son cercueil intact avec tous ses tr√©sors, en 1922.\n",
    "Pour les √âgyptiens, il y avait une vie apr√®s la mort, une vie √©ternelle.\n",
    "C‚Äôest pour cela que le corps devait √™tre conserv√© dans le meilleur √©tat possible : c‚Äôest ce qu‚Äôon appelle la momification.\n",
    "C‚Äôest aussi pour cela que l‚Äôon retrouve aujourd‚Äôhui de la nourriture, des armes ou des tr√©sors dans les tombeaux.\n",
    "Ces objets accompagnaient le pharaon dans sa vie apr√®s la mort.\n",
    "Identify : L'√âgypte antique\n",
    "Guess : Les arch√©ologues sont des chercheurs scientifiques\n",
    "Seek : Qu‚Äôest-ce que l‚Äô√âgypte antique ?\n",
    "Assess : L‚Äô√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord\n",
    "Assess Cues :\n",
    "‚ÄÉ\"1\" : \"L'√âgypte antique est une ancienne civilisation de l'Afrique du Nord\",\n",
    "‚ÄÉ\"2\" : \"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\n",
    "‚ÄÉ\"3\" : \"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"\n",
    "Identify Validity : 1\n",
    "Guess Validity : <NA>\n",
    "Seek Validity : 1\n",
    "Assess Validity : 1\n",
    "Mechanical Rating : <NA>\n",
    "\n",
    "Reasoning :\n",
    "L‚Äô√©tape Identify (¬´ L‚Äô√âgypte antique ¬ª) est d√©j√† marqu√©e comme valide et identifie clairement le sujet.\n",
    "L‚Äô√©tape Seek est une question bien formul√©e (¬´ Qu‚Äôest-ce que l‚Äô√âgypte antique ? ¬ª), et elle est originale car le texte de r√©f√©rence ne d√©finit pas directement ce terme.\n",
    "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 1 des assess_cues, donc cette r√©ponse est correcte.\n",
    "Cependant, l‚Äô√©tape Guess (¬´ Les arch√©ologues sont des chercheurs scientifiques ¬ª) ne tente pas de r√©pondre √† la question pos√©e en Seek et n‚Äôest que vaguement li√©e au texte (elle parle des arch√©ologues, pas de l‚Äô√âgypte antique en soi).\n",
    "Comme l‚Äô√©tape Guess ne propose pas une explication pertinente en lien avec la m√™me question, la cha√Æne Identify‚ÄìGuess‚ÄìSeek n‚Äôest pas coh√©rente.\n",
    "L‚Äô√©chec √† l‚Äô√©tape Guess et le manque de coh√©rence invalident tout le cycle.\n",
    "\n",
    "Classification : 0\n",
    "\n",
    "Example 3:\n",
    "\n",
    "Id : mc12_8\n",
    "Text :\n",
    "La for√™t tropicale humide a un climat qui fournit beaucoup d'eau et une temp√©rature √©lev√©e toute l'ann√©e.\n",
    "Cela favorise la densit√© et la croissance permanente des plantes de ces for√™ts.\n",
    "En Asie, on appelle ce genre de for√™t ¬´ la jungle ¬ª.\n",
    "Elle est parfois appel√©e for√™t dense, bien que cette expression signifie seulement que les arbres sont tr√®s proches les uns des autres.\n",
    "Dans les for√™ts tropicales, les arbres formant trois √©tages de hauteurs diff√©rentes sont tr√®s serr√©s et perdent leurs feuilles irr√©guli√®rement.\n",
    "Identify : lieux des for√™ts tropicales\n",
    "Guess : les for√™ts tropicales se trouvent dans diff√©rents endroits du monde\n",
    "Seek : o√π se trouvent les for√™ts tropicales du monde ?\n",
    "Assess : les for√™ts se trouvent en Asie, Australie, Am√©rique centrale, Am√©rique du Sud et en Afrique\n",
    "Assess Cues :\n",
    "‚ÄÉ\"4\" : \"Les for√™ts tropicales sont toujours vertes car il pleut tout le temps\",\n",
    "‚ÄÉ\"2\" : \"La temp√©rature dans les for√™ts tropicales est entre 30¬∞ et 40¬∞C\",\n",
    "‚ÄÉ\"3\" : \"La for√™t tropicale se trouve en Asie, en Am√©rique du Sud et en Afrique\"\n",
    "Identify Validity : 3\n",
    "Guess Validity : <NA>\n",
    "Seek Validity : 3\n",
    "Assess Validity : 3\n",
    "Mechanical Rating : <NA>\n",
    "\n",
    "Reasoning :\n",
    "L‚Äô√©tape Identify (¬´ lieux des for√™ts tropicales ¬ª) est marqu√©e comme valide et indique clairement le sujet.\n",
    "L‚Äô√©tape Guess (¬´ les for√™ts tropicales se trouvent dans diff√©rents endroits du monde ¬ª) propose une explication g√©n√©rale, ce qui est suffisant pour valider cette √©tape.\n",
    "L‚Äô√©tape Seek est bien formul√©e sous forme de question (¬´ o√π se trouvent les for√™ts tropicales du monde ? ¬ª).\n",
    "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 3 dans les assess_cues (¬´ La for√™t tropicale se trouve en Asie, en Am√©rique du Sud et en Afrique ¬ª), bien que l‚Äô√©num√©ration soit un peu plus large.\n",
    "On peut consid√©rer que cette r√©ponse correspond √† la bonne option.\n",
    "Les √©tapes Identify, Guess et Seek sont coh√©rentes entre elles, elles portent toutes sur la localisation des for√™ts tropicales.\n",
    "Le texte de r√©f√©rence ne donne pas la liste des continents ou r√©gions concern√©es, donc la question Seek est originale.\n",
    "La r√©ponse donn√©e dans Assess est directement en lien avec la question pos√©e.\n",
    "Tous les crit√®res de validit√© sont donc remplis.\n",
    "\n",
    "Classification : 1\n",
    "\"\"\",\n",
    "        # Output\n",
    "        \"selected_fields\": [\"Classification\", \"Reasoning\"],\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\":\n",
    "        \"\"\"\n",
    "Please follow the JSON format below:\n",
    "```json\n",
    "{{\n",
    "  \"Reasoning\": \"Your text here\",\n",
    "  \"Classification\": \"Your integer here\"\n",
    "}}\n",
    "\"\"\",\n",
    "        \"json_output\": True,\n",
    "\n",
    "        # Prompt optimization\n",
    "        \"provider_llm2\": \"azure\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "        \"max_iterations\": 1,\n",
    "        \"use_validation_set\": False,\n",
    "        \"validation_size\": 10,\n",
    "        \"random_state\": 42,\n",
    "\n",
    "        # Majority vote\n",
    "        \"n_completions\": 1,\n",
    "\n",
    "    },\n",
    "    {\n",
    "        # LLM settings\n",
    "        \"provider_llm1\": \"azure\",\n",
    "        \"model_name_llm1\": \"gpt-4o\",\n",
    "        \"temperature_llm1\": 0,\n",
    "        \"prompt_name\": \"zero_shot\",\n",
    "        \"subsample_size\": -1,  # Size of data subset to use\n",
    "\n",
    "        # Prompt configuration\n",
    "        \"template\": \"\"\"\n",
    "You are an assistant that evaluates data entries.\n",
    "\n",
    "The data has the following columns:\n",
    "- \"key\": Unique identifiant\n",
    "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
    "- \"Identify\": Response for the IDENTIFY step\n",
    "- \"Guess\": Response for the GUESS step\n",
    "- \"Seek\": Response for the SEEK step\n",
    "- \"Assess\": Response for the ASSESS step\n",
    "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
    "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
    "\n",
    "Here is an entry to evaluate:\n",
    "{verbatim_text}\n",
    "\n",
    "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
    "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
    "\n",
    "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
    "\n",
    "- Identify Step: Does the Identify step indicate a topic of interest?\n",
    "- Guess Step: Does the Guess step suggest a possible explanation?\n",
    "- Seek Step: Is the Seek step formulated as a question?\n",
    "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
    "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
    "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
    "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
    "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
    "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
    "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
    "\n",
    "If all these criteria are met, the cycle is valid.\n",
    "Validity is expressed as:\n",
    "1: Valid cycle\n",
    "0: Invalid cycle\n",
    "\n",
    "Examples\n",
    "Example 1:\n",
    "\n",
    "Id: bc4_1\n",
    "Text:\n",
    "A des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu'on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma.\n",
    "Le magma est responsable de l'explosion des volcans : c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan.\n",
    "On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es.\n",
    "Au-del√† de 10 000 ans, on peut dire que le volcan est √©teint.\n",
    "Identify: le magma\n",
    "Guess: La temp√©rature du magma d√©passe les 500‚ÄØ¬∞C\n",
    "Seek: Quelle est la temp√©rature du magma ?\n",
    "Assess: La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\n",
    "Assess Cues:\n",
    "‚ÄÉ\"1\" : \"La terre contient 7 couches\",\n",
    "‚ÄÉ\"2\" : \"La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\",\n",
    "‚ÄÉ\"3\" : \"La temp√©rature √† l'√©ruption d'un volcan est √† 700‚ÄØ¬∞C\"\n",
    "Identify Validity: <NA>\n",
    "Guess Validity: 2\n",
    "Seek Validity: 2\n",
    "Assess Validity: 2\n",
    "Mechanical Rating: <NA>\n",
    "\n",
    "Reasoning:\n",
    "Identify nomme ¬´ le magma ¬ª, indiquant clairement le sujet.\n",
    "Guess propose une affirmation plausible sur la temp√©rature du magma (>500‚ÄØ¬∞C).\n",
    "Seek est une question bien formul√©e qui interroge sur la temp√©rature du magma.\n",
    "Assess fournit une r√©ponse explicite (‚âà1000‚ÄØ¬∞C) qui r√©pond directement √† la question pos√©e dans Seek et correspond exactement √† l‚Äôoption 2 des assess_cues, donc la r√©ponse est consid√©r√©e comme ¬´ trouv√©e ¬ª.\n",
    "Les √©tapes Identify/Guess/Seek/Assess sont toutes centr√©es sur le m√™me th√®me (la temp√©rature du magma) et restent li√©es au sujet du texte de r√©f√©rence (magma/volcans), bien que la temp√©rature sp√©cifique ne soit pas donn√©e dans le texte, ce qui respecte le crit√®re d‚Äôoriginalit√©.\n",
    "Les drapeaux num√©riques marquent d√©j√† Guess, Seek et Assess comme valides ; Identify est √©galement valide de mani√®re ind√©pendante.\n",
    "Ainsi, chaque crit√®re de validit√© du codebook est respect√©.\n",
    "\n",
    "Classification: 1\n",
    "\n",
    "Example 2:\n",
    "\n",
    "Id : bc5_3\n",
    "Text :\n",
    "Toutankhamon √©tait un pharaon, un roi de l'√âgypte antique.\n",
    "Il est tr√®s connu aujourd‚Äôhui parce que des arch√©ologues ont retrouv√© son cercueil intact avec tous ses tr√©sors, en 1922.\n",
    "Pour les √âgyptiens, il y avait une vie apr√®s la mort, une vie √©ternelle.\n",
    "C‚Äôest pour cela que le corps devait √™tre conserv√© dans le meilleur √©tat possible : c‚Äôest ce qu‚Äôon appelle la momification.\n",
    "C‚Äôest aussi pour cela que l‚Äôon retrouve aujourd‚Äôhui de la nourriture, des armes ou des tr√©sors dans les tombeaux.\n",
    "Ces objets accompagnaient le pharaon dans sa vie apr√®s la mort.\n",
    "Identify : L'√âgypte antique\n",
    "Guess : Les arch√©ologues sont des chercheurs scientifiques\n",
    "Seek : Qu‚Äôest-ce que l‚Äô√âgypte antique ?\n",
    "Assess : L‚Äô√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord\n",
    "Assess Cues :\n",
    "‚ÄÉ\"1\" : \"L'√âgypte antique est une ancienne civilisation de l'Afrique du Nord\",\n",
    "‚ÄÉ\"2\" : \"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\n",
    "‚ÄÉ\"3\" : \"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"\n",
    "Identify Validity : 1\n",
    "Guess Validity : <NA>\n",
    "Seek Validity : 1\n",
    "Assess Validity : 1\n",
    "Mechanical Rating : <NA>\n",
    "\n",
    "Reasoning :\n",
    "L‚Äô√©tape Identify (¬´ L‚Äô√âgypte antique ¬ª) est d√©j√† marqu√©e comme valide et identifie clairement le sujet.\n",
    "L‚Äô√©tape Seek est une question bien formul√©e (¬´ Qu‚Äôest-ce que l‚Äô√âgypte antique ? ¬ª), et elle est originale car le texte de r√©f√©rence ne d√©finit pas directement ce terme.\n",
    "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 1 des assess_cues, donc cette r√©ponse est correcte.\n",
    "Cependant, l‚Äô√©tape Guess (¬´ Les arch√©ologues sont des chercheurs scientifiques ¬ª) ne tente pas de r√©pondre √† la question pos√©e en Seek et n‚Äôest que vaguement li√©e au texte (elle parle des arch√©ologues, pas de l‚Äô√âgypte antique en soi).\n",
    "Comme l‚Äô√©tape Guess ne propose pas une explication pertinente en lien avec la m√™me question, la cha√Æne Identify‚ÄìGuess‚ÄìSeek n‚Äôest pas coh√©rente.\n",
    "L‚Äô√©chec √† l‚Äô√©tape Guess et le manque de coh√©rence invalident tout le cycle.\n",
    "\n",
    "Classification : 0\n",
    "\n",
    "Example 3:\n",
    "\n",
    "Id : mc12_8\n",
    "Text :\n",
    "La for√™t tropicale humide a un climat qui fournit beaucoup d'eau et une temp√©rature √©lev√©e toute l'ann√©e.\n",
    "Cela favorise la densit√© et la croissance permanente des plantes de ces for√™ts.\n",
    "En Asie, on appelle ce genre de for√™t ¬´ la jungle ¬ª.\n",
    "Elle est parfois appel√©e for√™t dense, bien que cette expression signifie seulement que les arbres sont tr√®s proches les uns des autres.\n",
    "Dans les for√™ts tropicales, les arbres formant trois √©tages de hauteurs diff√©rentes sont tr√®s serr√©s et perdent leurs feuilles irr√©guli√®rement.\n",
    "Identify : lieux des for√™ts tropicales\n",
    "Guess : les for√™ts tropicales se trouvent dans diff√©rents endroits du monde\n",
    "Seek : o√π se trouvent les for√™ts tropicales du monde ?\n",
    "Assess : les for√™ts se trouvent en Asie, Australie, Am√©rique centrale, Am√©rique du Sud et en Afrique\n",
    "Assess Cues :\n",
    "‚ÄÉ\"4\" : \"Les for√™ts tropicales sont toujours vertes car il pleut tout le temps\",\n",
    "‚ÄÉ\"2\" : \"La temp√©rature dans les for√™ts tropicales est entre 30¬∞ et 40¬∞C\",\n",
    "‚ÄÉ\"3\" : \"La for√™t tropicale se trouve en Asie, en Am√©rique du Sud et en Afrique\"\n",
    "Identify Validity : 3\n",
    "Guess Validity : <NA>\n",
    "Seek Validity : 3\n",
    "Assess Validity : 3\n",
    "Mechanical Rating : <NA>\n",
    "\n",
    "Reasoning :\n",
    "L‚Äô√©tape Identify (¬´ lieux des for√™ts tropicales ¬ª) est marqu√©e comme valide et indique clairement le sujet.\n",
    "L‚Äô√©tape Guess (¬´ les for√™ts tropicales se trouvent dans diff√©rents endroits du monde ¬ª) propose une explication g√©n√©rale, ce qui est suffisant pour valider cette √©tape.\n",
    "L‚Äô√©tape Seek est bien formul√©e sous forme de question (¬´ o√π se trouvent les for√™ts tropicales du monde ? ¬ª).\n",
    "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 3 dans les assess_cues (¬´ La for√™t tropicale se trouve en Asie, en Am√©rique du Sud et en Afrique ¬ª), bien que l‚Äô√©num√©ration soit un peu plus large.\n",
    "On peut consid√©rer que cette r√©ponse correspond √† la bonne option.\n",
    "Les √©tapes Identify, Guess et Seek sont coh√©rentes entre elles, elles portent toutes sur la localisation des for√™ts tropicales.\n",
    "Le texte de r√©f√©rence ne donne pas la liste des continents ou r√©gions concern√©es, donc la question Seek est originale.\n",
    "La r√©ponse donn√©e dans Assess est directement en lien avec la question pos√©e.\n",
    "Tous les crit√®res de validit√© sont donc remplis.\n",
    "\n",
    "Classification : 1\n",
    "\n",
    "\"\"\",\n",
    "        # Output\n",
    "        \"selected_fields\": [\"Classification\", \"Reasoning\"],\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\":\n",
    "        \"\"\"\n",
    "Please follow the JSON format below:\n",
    "```json\n",
    "{{\n",
    "  \"Reasoning\": \"Your text here\",\n",
    "  \"Classification\": \"Your integer here\"\n",
    "}}\n",
    "\"\"\",\n",
    "        \"json_output\": True,\n",
    "\n",
    "        # Prompt optimization\n",
    "        \"provider_llm2\": \"azure\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "        \"max_iterations\": 1,\n",
    "        \"use_validation_set\": False,\n",
    "        \"validation_size\": 10,\n",
    "        \"random_state\": 42,\n",
    "\n",
    "        # Majority vote\n",
    "        \"n_completions\": 1,\n",
    "\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run the classification on Annotated Subset\n",
    "\n",
    "Before launching the classification on the entire dataset, we first run it on the subset that has been manually annotated.  \n",
    "This step allows us to compute performance metrics (e.g., **accuracy**, **F1-score**) by comparing LLM predictions to human labels,  \n",
    "and therefore select which (if any) scenario can be used to classify the full, unlabeled dataset.\n",
    "\n",
    "#### Configuration Parameters\n",
    "\n",
    "- `annotation_columns`: The names of the columns containing human annotations.\n",
    "- `labels`: The possible label values (in this case, `[0, 1]` for binary classification).\n",
    "\n",
    "We filter out any rows with missing values in the annotation columns to ensure we're only evaluating on fully labeled data.\n",
    "\n",
    "#### Repeated Runs for Stability\n",
    "\n",
    "LLMs are **stochastic** by nature ‚Äî even with a temperature of `0`, outputs can vary.  \n",
    "To assess how consistent the model is, we introduce the `n_runs` parameter:\n",
    "\n",
    "- `n_runs`: The number of times the classification is repeated for each scenario on the annotated data.\n",
    "\n",
    "We recommend setting `n_runs = 3`, based on findings from **[Paper XX]** (insert reference),  \n",
    "which showed that **three repetitions strike a good balance between stability and cost**.  \n",
    "Running more times improves statistical reliability but increases costs proportionally.\n",
    "\n",
    "#### `n_runs` vs `n_completions`\n",
    "\n",
    "It‚Äôs important to distinguish between these two concepts:\n",
    "\n",
    "- **`n_completions`**:  \n",
    "  Controls how many responses are generated **within a single run** for each data point.  \n",
    "  The final label is determined by **majority vote** over those completions.  \n",
    "  **Example**:  \n",
    "  If `n_completions = 3` and the model returns `[0, 0, 1]`, the selected label will be `0`.\n",
    "\n",
    "- **`n_runs`**:  \n",
    "  Repeats the **entire classification process** multiple times across the same data.  \n",
    "  If you run the scenario three times and get `[0, 0, 1]` for a given entry,  \n",
    "  that variation will be captured when calculating metrics (e.g., **variance**, **disagreement rate**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all labeled data: 12 samples\n",
      "Scenario 'few_shot' - Train size (all data): 12, No validation set\n",
      "\n",
      "=== Processing Verbatim 1/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc4_1\n",
      "\n",
      "Text: A des dizaines de kilometres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une eruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune eruption dans les 10 000 dernieres annees. Au dela de 10 000 ans, on peut dire que le volcan est eteint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temperature du magma depasse les 500c\n",
      "\n",
      "Seek: Quelle est la temperature du magma?\n",
      "\n",
      "Assess: La temperature du magma atteint les 1000c\n",
      "\n",
      "assess Cues: {\"1\":\"La terre contient 7 couches\",\"2\":\"La temp√©rature du magma atteint les 1000¬∞C\",\"3\":\"La temp√©rature √† l'√©ruption d'un volcan est √† 700¬∞C\"}\n",
      "\n",
      "Identify Validity: <NA>\n",
      "\n",
      "Guess Validity: 2\n",
      "\n",
      "Seek Validity: 2\n",
      "\n",
      "Assess Validity: 2\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The entry meets the criteria for a valid cycle. The Identify step indicates a topic of interest, 'le magma', which is related to the reference text about magma and volcanoes. The Guess step suggests a possible explanation, 'La temperature du magma depasse les 500c', which is related to the topic. The Seek step is formulated as a question, 'Quelle est la temperature du magma?', and is consistent with the Identify and Guess steps. The Assess step provides a possible answer, 'La temperature du magma atteint les 1000c', which resolves the Seek question. The answer in the Assess step is found in the assess_cues, confirming that an answer was indeed found. The Seek question's answer is not found in the reference text, ensuring originality. All validity columns (Guess_validity, Seek_validity, Assess_validity) have numeric values, indicating acceptance of those steps without re-checking. Therefore, the cycle is valid.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1209\n",
      "Cost => 0.0046\n",
      "\n",
      "=== Processing Verbatim 2/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc4_6\n",
      "\n",
      "Text: La religion de la Grece antique comprend plusieurs dieux. Les aventures de ces dieux forment la mythologie grecque, l'une des mythologies les plus developpees de l'histoire antique. D'apres les Grecs, les 12 principaux dieux vivent sur l'Olympe et ont une apparence et un comportement comparables a celui des humains. Les dieux les plus connus sont : Athena, deesse de la guerre, Zeus, roi des dieux et dieu de la foudre. Hera, deesse du mariage. Poseidon, dieu des oceans.\n",
      "\n",
      "Identify: Une mythologie\n",
      "\n",
      "Guess: Une mythologie est un ensemble de contes imaginaires\n",
      "\n",
      "Seek: Qu'est-ce qu'une mythologie\n",
      "\n",
      "Assess: Une mythologie est un ensemble de legendes lies a une civilisation bien precise\n",
      "\n",
      "assess Cues: {\"1\":\"Une mythologie est un ensemble de l√©gendes li√©es √† une civilisation\",\"2\":\"L'histoire comprend beaucoup de mythologies\",\"3\":\"L'Olympe est la plus haute montagne de Gr√®ce\"}\n",
      "\n",
      "Identify Validity: 1\n",
      "\n",
      "Guess Validity: 1\n",
      "\n",
      "Seek Validity: 1\n",
      "\n",
      "Assess Validity: 1\n",
      "\n",
      "mechanical Rating: 1\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The entry has a mechanical_rating of 1, which overrides any other logic in the codebook. Therefore, the cycle is automatically considered valid. Additionally, all steps are semantically related to the topic of Greek mythology, and the Seek question 'Qu'est-ce qu'une mythologie' is not answered in the reference text, which discusses specific Greek gods rather than defining mythology itself. The Assess step provides an answer that matches one of the assess_cues, confirming the validity of the cycle.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1130\n",
      "Cost => 0.0037\n",
      "\n",
      "=== Processing Verbatim 3/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc4_7\n",
      "\n",
      "Text: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite. Selon la legende, Rome aurait ete fondee par Romulus qui aurai donne son nom a la ville. Au depart, ce n'etait qu'un groupe de quelques villages, puis l'Empire romain va couvrir une grande partie de l'Europe et entourer toute la mer Mediterranee. La religion romaine comptait de nombreux dieux, inspires en partie des dieux grecs.\n",
      "\n",
      "Identify: Romulus\n",
      "\n",
      "Guess: Romulus est un roi de Rome\n",
      "\n",
      "Seek: Qui et Romulus mythologie a\n",
      "\n",
      "Assess: Dans la legende Romulus et le premier roi de Rome\n",
      "\n",
      "assess Cues: {\"1\":\"Dans la l√©gende, Romulus est le premier roi de Rome\",\"2\":\"L'Antiquit√© s'est √©tendue entre 3200 avant J.C, jusqu'√† l'ann√©e 476\",\"4\":\"La M√©diterran√©e touche plusieurs continents\"}\n",
      "\n",
      "Identify Validity: 1\n",
      "\n",
      "Guess Validity: 1\n",
      "\n",
      "Seek Validity: 1\n",
      "\n",
      "Assess Validity: 1\n",
      "\n",
      "mechanical Rating: 1\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The mechanical_rating column has a numeric value of 1, which overrides any other logic in the codebook. Therefore, the cycle is automatically considered valid without needing to evaluate the individual steps or criteria.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1056\n",
      "Cost => 0.0031\n",
      "\n",
      "=== Processing Verbatim 4/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc4_8\n",
      "\n",
      "Text: La foret tropicale humide a un climat qui fournit beaucoup d'eau et une temperature elevee toute l'annee. Cela favorise la densite et la croissance permanente des plantes de ces forets.  En Asie, on appelle ce genre de foret 'la jungle'. Elle est parfois appelee foret dense, bien que cette expression signifie seulement que les arbres sont tres proches les uns des autres. Dans les forets tropicales, les arbres formant trois etages de hauteur differentes, sont tres serres et perdent leurs feuilles irregulierement.\n",
      "\n",
      "Identify: Les plantes de foret tropicales\n",
      "\n",
      "Guess: Les foret tropicale contiennent tout type de plantes\n",
      "\n",
      "Seek: Quelle plante peut-on voir dans les forets\n",
      "\n",
      "Assess: Les foret tropicale sont toujours vertes car il pleut tout le temps\n",
      "\n",
      "assess Cues: {\"4\":\"Les for√™ts tropicales sont toujours vertes car il pleut tout le temps\",\"2\":\"La temp√©rature dans les for√™ts tropicales est entre 30¬∞ et 40¬∞C\",\"3\":\"La for√™t tropicale se trouve en Asie, en Am√©rique du Sud et en Afrique\"}\n",
      "\n",
      "Identify Validity: 1\n",
      "\n",
      "Guess Validity: 1\n",
      "\n",
      "Seek Validity: 1\n",
      "\n",
      "Assess Validity: 4\n",
      "\n",
      "mechanical Rating: 0\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The mechanical_rating column has a numeric value of 0, which overrides any other logic in the codebook. Therefore, the cycle is classified as invalid without further evaluation of the individual steps or criteria.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1090\n",
      "Cost => 0.0032\n",
      "\n",
      "=== Processing Verbatim 5/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc5_1\n",
      "\n",
      "Text: A des dizaines de kilometres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une eruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune eruption dans les 10 000 dernieres annees. Au dela de 10 000 ans, on peut dire que le volcan est eteint.\n",
      "\n",
      "Identify: Pourqoi on l'appelle le manteau.\n",
      "\n",
      "Guess: le magma et une pierre.\n",
      "\n",
      "Seek: quel et la temperature magma\n",
      "\n",
      "Assess: 1 000c\n",
      "\n",
      "assess Cues: {\"1\":\"La terre contient 7 couches\",\"2\":\"La temp√©rature du magma atteint les 1000¬∞C\",\"3\":\"La temp√©rature √† l'√©ruption d'un volcan est √† 700¬∞C\"}\n",
      "\n",
      "Identify Validity: <NA>\n",
      "\n",
      "Guess Validity: <NA>\n",
      "\n",
      "Seek Validity: 2\n",
      "\n",
      "Assess Validity: <NA>\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The cycle validity is determined by evaluating each step according to the provided criteria. The Identify step asks 'Pourquoi on l'appelle le manteau', which indicates a topic of interest related to the reference text about the Earth's mantle. The Guess step 'le magma et une pierre' suggests a possible explanation, although it is not directly related to the Identify step. The Seek step 'quel et la temperature magma' is formulated as a question and is related to the topic of the reference text, but the answer '1 000c' is found in the assess_cues, which makes the Assess step valid. The Seek Validity is already marked as valid (2), so we accept it without re-checking. The Assess step provides an answer '1 000c', which is indeed in the assess_cues, making it a valid answer. However, the consistency between Identify, Guess, and Seek steps is lacking, as they do not relate to the same question. Additionally, the Seek question originality is compromised because the answer is found in the reference text. Therefore, the cycle is invalid due to inconsistency and lack of originality in the Seek question.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1244\n",
      "Cost => 0.0050\n",
      "\n",
      "=== Processing Verbatim 6/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc5_2\n",
      "\n",
      "Text: La memoire, cest une fonction de notre cerveau qui nous permet de garder les informations pour ensuite les recuperer et les utiliser. Tables de multiplication, poesies, ... notre cerveau est capable denregistrer et d'organiser enormement dinformations. Il recolte ces informations grace a nos 5 sens. Les informations sont ensuite envoyees dans differentes zones du cerveau.  Il y a donc une zone pour les sons, une zone pour les images ... Et la memoire, cest comme un muscle : en sentrainant, on peut retenir plus de choses et plus facilement !\n",
      "\n",
      "Identify: quelle et les 5sens\n",
      "\n",
      "Guess: je pense que les 5sens sont bouge apprendre ? ? ?\n",
      "\n",
      "Seek: quelle et les 5 sens du cerveau.\n",
      "\n",
      "Assess: non\n",
      "\n",
      "assess Cues: {\"1\":\"Le cerveau contr√¥le toutes nos actions, volontaires et involontaires\",\"2\":\"Notre cerveau peut stocker plus d'informations que 100 ordinateurs r√©unis\",\"3\":\"Il y a plusieurs activit√©s qui aident la m√©moire mais, surtout, il faut avoir un sommeil suffisant et bien r√©gl√© \"}\n",
      "\n",
      "Identify Validity: <NA>\n",
      "\n",
      "Guess Validity: <NA>\n",
      "\n",
      "Seek Validity: 1\n",
      "\n",
      "Assess Validity: <NA>\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The cycle validity is determined by evaluating each step according to the criteria provided. The Identify step 'quelle et les 5sens' indicates a topic of interest related to the senses, which is consistent with the reference text about memory and the brain's functions. The Guess step 'je pense que les 5sens sont bouge apprendre ? ? ?' suggests a possible explanation, albeit unclear, but it does attempt to relate to the senses. The Seek step 'quelle et les 5 sens du cerveau.' is formulated as a question, and the Seek Validity is already marked as valid (1), so we accept it without re-checking. The Assess step 'non' indicates that no answer was found, which is acceptable. The consistency between Identify, Guess, and Seek steps is maintained as they all relate to the senses. The reference link is valid as the steps relate to the topic of the reference text, which discusses how the brain processes information through the senses. The Seek question originality is valid because the specific question about the '5 senses of the brain' is not answered in the reference text. The Resolving Answer criterion is met because the Assess step states 'non', indicating no answer was found, which is consistent with the assess_cues provided. The Valid No criterion is satisfied because the answer to the Seek question is not present in the assess_cues. Since the Seek Validity is marked as valid, and all other criteria are met, the cycle is valid.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1339\n",
      "Cost => 0.0057\n",
      "\n",
      "=== Processing Verbatim 7/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc5_3\n",
      "\n",
      "Text: Toutankhamon etait un pharaon, un roi de l'Egypte antique. Il est tres connu aujourdhui parce que des archeologues ont retrouve son cercueil intacte avec tous ses tresors, en 1922. Pour les Egyptiens, il y avait une vie apres la mort, une vie eternelle. Cest pour cela que le corps devait etre conserve dans le meilleur etat possible, cest ce quon appelle la momification. Cest aussi pour cela que lon retrouve aujourdhui de la nourriture, des armes ou des tresors dans les tombeaux. Ces objets accompagnaient le pharaon dans sa vie apres la mort.\n",
      "\n",
      "Identify: L'Egypte antique\n",
      "\n",
      "Guess: Les archeologues sont des chercheurs scientifiques\n",
      "\n",
      "Seek: Qu'est-ce que l'Egypte antique\n",
      "\n",
      "Assess: L'Egypte antique est une ancienne civilisation de l'Afrique du Nord\n",
      "\n",
      "assess Cues: {\"1\":\"L'Egypte antique est une ancienne civilisation de l'Afrique du Nord\",\"2\":\"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\"3\":\"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"}\n",
      "\n",
      "Identify Validity: 1\n",
      "\n",
      "Guess Validity: <NA>\n",
      "\n",
      "Seek Validity: 1\n",
      "\n",
      "Assess Validity: 1\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The entry meets the criteria for a valid cycle. The Identify step 'L'Egypte antique' indicates a topic of interest related to ancient Egypt. The Guess step 'Les archeologues sont des chercheurs scientifiques' suggests a possible explanation related to the topic of archaeology, which is relevant to the discovery of Tutankhamun's tomb. The Seek step 'Qu'est-ce que l'Egypte antique' is formulated as a question, asking for information about ancient Egypt. The Assess step 'L'Egypte antique est une ancienne civilisation de l'Afrique du Nord' provides a possible answer to the Seek question, and this answer is found in the assess_cues, confirming its validity. The Identify, Guess, and Seek steps are consistent and related to the topic of the reference text, which discusses ancient Egypt and Tutankhamun. The Seek question's answer is not found in the reference text, ensuring originality. The Assess step resolves the Seek question with a valid answer from the assess_cues. All steps have numeric validity, and there is no mechanical rating to override the decision.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1292\n",
      "Cost => 0.0050\n",
      "\n",
      "=== Processing Verbatim 8/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc6_7\n",
      "\n",
      "Text: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite. Selon la legende, Rome aurait ete fondee par Romulus qui aurai donne son nom a la ville. Au depart, ce n'etait qu'un groupe de quelques villages, puis l'Empire romain va couvrir une grande partie de l'Europe et entourer toute la mer Mediterranee. La religion romaine comptait de nombreux dieux, inspires en partie des dieux grecs.\n",
      "\n",
      "Identify: la mer Mediterranee\n",
      "\n",
      "Guess: La mer Mediterranee entoure l'Europe\n",
      "\n",
      "Seek: Ou se trouve la mer Mediterranee?\n",
      "\n",
      "Assess: La Mediterranee est une mer internationale\n",
      "\n",
      "assess Cues: {\"1\":\"Dans la l√©gende, Romulus est le premier roi de Rome\",\"2\":\"L'Antiquit√© s'est √©tendue entre 3200 avant J.C, jusqu'√† l'ann√©e 476\",\"4\":\"La M√©diterran√©e touche plusieurs continents\"}\n",
      "\n",
      "Identify Validity: 3\n",
      "\n",
      "Guess Validity: 3\n",
      "\n",
      "Seek Validity: 3\n",
      "\n",
      "Assess Validity: <NA>\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The mechanical_rating is empty, so we need to evaluate the cycle validity based on the criteria. The Identify step is valid as it indicates a topic of interest related to the Mediterranean Sea. The Guess step is valid as it suggests a possible explanation about the Mediterranean Sea surrounding Europe. The Seek step is valid as it is formulated as a question asking about the location of the Mediterranean Sea. The Assess step provides an answer, stating that the Mediterranean is an international sea, which is related to the Seek question. The consistency is maintained as all steps are related to the Mediterranean Sea. The Reference Link is valid as the steps are related to the topic of the reference text, which mentions the Mediterranean Sea. The Seek Question Originality is valid because the answer to the Seek question is not found in the reference text. The Resolving Answer is valid as the Assess step provides an answer to the Seek question. The Valid Answer criterion is not met because the Assess step states an answer that is not present in the assess_cues, indicating that no valid answer was found. Therefore, the cycle is invalid.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1231\n",
      "Cost => 0.0049\n",
      "\n",
      "=== Processing Verbatim 9/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc6_8\n",
      "\n",
      "Text: La foret tropicale humide a un climat qui fournit beaucoup d'eau et une temperature elevee toute l'annee. Cela favorise la densite et la croissance permanente des plantes de ces forets.  En Asie, on appelle ce genre de foret 'la jungle'. Elle est parfois appelee foret dense, bien que cette expression signifie seulement que les arbres sont tres proches les uns des autres. Dans les forets tropicales, les arbres formant trois etages de hauteur differentes, sont tres serres et perdent leurs feuilles irregulierement.\n",
      "\n",
      "Identify: Lieux des forets tropicales\n",
      "\n",
      "Guess: Les forest tropicales se trouvent dans differents endroits du monde\n",
      "\n",
      "Seek: Ou se trouvent les forets tropicales du monde?\n",
      "\n",
      "Assess: les forets tropicales se trouvent en Asie,Australie en amerique du sud et en afrique\n",
      "\n",
      "assess Cues: {\"4\":\"Les for√™ts tropicales sont toujours vertes car il pleut tout le temps\",\"2\":\"La temp√©rature dans les for√™ts tropicales est entre 30¬∞ et 40¬∞C\",\"3\":\"La for√™t tropicale se trouve en Asie, en Am√©rique du Sud et en Afrique\"}\n",
      "\n",
      "Identify Validity: 3\n",
      "\n",
      "Guess Validity: <NA>\n",
      "\n",
      "Seek Validity: 3\n",
      "\n",
      "Assess Validity: 3\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The mechanical_rating is empty, so we need to evaluate the cycle validity based on the criteria. The Identify step indicates a topic of interest: 'Lieux des forets tropicales', which is valid. The Guess step suggests a possible explanation: 'Les forest tropicales se trouvent dans differents endroits du monde', which is valid. The Seek step is formulated as a question: 'Ou se trouvent les forets tropicales du monde?', which is valid. The Assess step provides a possible answer: 'les forets tropicales se trouvent en Asie, Australie en amerique du sud et en afrique', which is valid. The Identify, Guess, and Seek steps are consistent and related to the same question about the location of tropical forests. They are also related to the topic of the reference text, which discusses tropical forests. The Seek question originality is valid because the answer to the Seek question is not found in the reference text. The Assess step provides an answer that resolves the Seek question. The answer in the Assess step is indeed found in the assess_cues ('La for√™t tropicale se trouve en Asie, en Am√©rique du Sud et en Afrique'), confirming a valid answer was found. All criteria are met, so the cycle is valid.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1314\n",
      "Cost => 0.0053\n",
      "\n",
      "=== Processing Verbatim 10/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: mc10_3\n",
      "\n",
      "Text: Toutankhamon etait un pharaon, un roi de l'Egypte antique. Il est tres connu aujourdhui parce que des archeologues ont retrouve son cercueil intacte avec tous ses tresors, en 1922. Pour les Egyptiens, il y avait une vie apres la mort, une vie eternelle. Cest pour cela que le corps devait etre conserve dans le meilleur etat possible, cest ce quon appelle la momification. Cest aussi pour cela que lon retrouve aujourdhui de la nourriture, des armes ou des tresors dans les tombeaux. Ces objets accompagnaient le pharaon dans sa vie apres la mort.\n",
      "\n",
      "Identify: Le tombeau de Toutankhamon\n",
      "\n",
      "Guess: Ia momie etait entiere\n",
      "\n",
      "Seek: Les objets du tombeau\n",
      "\n",
      "Assess: Je ne trouve pas ma reponse\n",
      "\n",
      "assess Cues: {\"1\":\"L'Egypte antique est une ancienne civilisation de l'Afrique du Nord\",\"2\":\"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\"3\":\"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"}\n",
      "\n",
      "Identify Validity: <NA>\n",
      "\n",
      "Guess Validity: <NA>\n",
      "\n",
      "Seek Validity: <NA>\n",
      "\n",
      "Assess Validity: <NA>\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The cycle needs to be evaluated based on the criteria provided. Let's go through each step: \\n\\n1. Identify Step: 'Le tombeau de Toutankhamon' indicates a topic of interest related to the reference text, which discusses Toutankhamon and his tomb. \\n\\n2. Guess Step: 'Ia momie etait entiere' suggests a possible explanation related to the condition of the mummy, which is relevant to the topic of the tomb. \\n\\n3. Seek Step: 'Les objets du tombeau' is not formulated as a question, which is a requirement for validity. \\n\\n4. Assess Step: 'Je ne trouve pas ma reponse' states that no answer was found, which is acceptable. \\n\\n5. Consistency: The Identify, Guess, and Seek steps are related to the same topic, which is Toutankhamon's tomb and its contents. \\n\\n6. Reference Link: The steps are related to the topic of the reference text, which is about Toutankhamon and his tomb. \\n\\n7. Seek Question Originality: The answer to the Seek step is not found in the reference text, as the text does not specifically list the objects in the tomb. \\n\\n8. Resolving Answer: The Assess step states no answer was found, which is consistent with the Seek step. \\n\\n9. Valid No: The Assess step indicates no answer was found, and the answer to the Seek question is not present in the assess_cues. \\n\\nThe cycle is invalid because the Seek step is not formulated as a question, which is a critical requirement for validity.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1405\n",
      "Cost => 0.0062\n",
      "\n",
      "=== Processing Verbatim 11/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: mc10_4\n",
      "\n",
      "Text: La tres grande majorite de lor disponible dans le monde est utilisee en bijouterie et pour sa fonction decorative : vetements, monuments ou uvres dart... Mais lor a dautres usages plus inattendus : les dentistes lutilisent pour fabriquer des couronnes dentaires par exemple. Presque tous les produits electroniques (telephone, ordinateur...) contiennent de lor en tres petite quantite car il permet de bien connecter les composantes. L'or est egalement utilise dans la fabrication des equipements spatiaux.\n",
      "\n",
      "Identify: L'or dans les equipements patiaux\n",
      "\n",
      "Guess: L'or permet de bien connecter les composants internes des equipements spatiaux\n",
      "\n",
      "Seek: Ou trouver de l 'or\n",
      "\n",
      "Assess: Je n'ai pas pu trouver ma reponse\n",
      "\n",
      "assess Cues: {\"4\":\"Les couronnes en or sont plus ch√®res\",\"2\":\"L'or permet de bien connecter les composants internes des √©quipement spatiaux\",\"3\":\"Le cuivre est un tr√®s bon conducteur pas cher\"}\n",
      "\n",
      "Identify Validity: 2\n",
      "\n",
      "Guess Validity: 2\n",
      "\n",
      "Seek Validity: <NA>\n",
      "\n",
      "Assess Validity: <NA>\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The entry has numeric values for Identify_validity and Guess_validity, so these steps are automatically considered valid. The Seek step is formulated as a question, 'Ou trouver de l 'or', which is valid. The Assess step states 'Je n'ai pas pu trouver ma reponse', indicating no answer was found, which is valid. The Identify, Guess, and Seek steps are related to the topic of the reference text, which is about the uses of gold, including in space equipment. The Seek question 'Ou trouver de l 'or' is not answered in the reference text, which discusses the uses of gold but not where to find it. The Assess step correctly states no answer was found, and the answer to the Seek question is not present in the assess_cues. Therefore, all criteria for a valid cycle are met.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1187\n",
      "Cost => 0.0044\n",
      "\n",
      "=== Processing Verbatim 12/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: mc12_8\n",
      "\n",
      "Text: La foret tropicale humide a un climat qui fournit beaucoup d'eau et une temperature elevee toute l'annee. Cela favorise la densite et la croissance permanente des plantes de ces forets.  En Asie, on appelle ce genre de foret 'la jungle'. Elle est parfois appelee foret dense, bien que cette expression signifie seulement que les arbres sont tres proches les uns des autres. Dans les forets tropicales, les arbres formant trois etages de hauteur differentes, sont tres serres et perdent leurs feuilles irregulierement.\n",
      "\n",
      "Identify: lieux des forets tropicales\n",
      "\n",
      "Guess: les forets tropical se trouvent dans different endroits du monde\n",
      "\n",
      "Seek: ou se trouvent les forets tropical du monde ?\n",
      "\n",
      "Assess: les foets se trouvent en asie australie enamerique central amerique du sud et en afrique\n",
      "\n",
      "assess Cues: {\"4\":\"Les for√™ts tropicales sont toujours vertes car il pleut tout le temps\",\"2\":\"La temp√©rature dans les for√™ts tropicales est entre 30¬∞ et 40¬∞C\",\"3\":\"La for√™t tropicale se trouve en Asie, en Am√©rique du Sud et en Afrique\"}\n",
      "\n",
      "Identify Validity: 3\n",
      "\n",
      "Guess Validity: <NA>\n",
      "\n",
      "Seek Validity: 3\n",
      "\n",
      "Assess Validity: 3\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The mechanical_rating is empty, so we need to evaluate the cycle validity based on the provided criteria. The Identify step indicates a topic of interest: 'lieux des forets tropicales'. The Guess step suggests a possible explanation: 'les forets tropical se trouvent dans different endroits du monde'. The Seek step is formulated as a question: 'ou se trouvent les forets tropical du monde ?'. The Assess step provides a possible answer: 'les foets se trouvent en asie australie enamerique central amerique du sud et en afrique'. The Identify, Guess, and Seek steps are consistent and related to the same question about the locations of tropical forests. They are also related to the topic of the reference text, which discusses tropical forests. The Seek question originality is satisfied because the specific locations of tropical forests are not mentioned in the reference text. The Assess step provides an answer that resolves the Seek question. However, the answer in the Assess step is not found in the assess_cues, which means the cycle is not valid according to the 'Valid Answer' criterion. Therefore, the cycle is invalid.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1283\n",
      "Cost => 0.0050\n",
      "Using all labeled data: 12 samples\n",
      "Scenario 'zero_shot' - Train size (all data): 12, No validation set\n",
      "\n",
      "=== Processing Verbatim 1/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc4_1\n",
      "\n",
      "Text: A des dizaines de kilometres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une eruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune eruption dans les 10 000 dernieres annees. Au dela de 10 000 ans, on peut dire que le volcan est eteint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temperature du magma depasse les 500c\n",
      "\n",
      "Seek: Quelle est la temperature du magma?\n",
      "\n",
      "Assess: La temperature du magma atteint les 1000c\n",
      "\n",
      "assess Cues: {\"1\":\"La terre contient 7 couches\",\"2\":\"La temp√©rature du magma atteint les 1000¬∞C\",\"3\":\"La temp√©rature √† l'√©ruption d'un volcan est √† 700¬∞C\"}\n",
      "\n",
      "Identify Validity: <NA>\n",
      "\n",
      "Guess Validity: 2\n",
      "\n",
      "Seek Validity: 2\n",
      "\n",
      "Assess Validity: 2\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Examples\n",
      "Example 1:\n",
      "\n",
      "Id: bc4_1\n",
      "\n",
      "Text:\n",
      "A des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu'on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma.\n",
      "Le magma est responsable de l'explosion des volcans : c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan.\n",
      "On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es.\n",
      "Au-del√† de 10 000 ans, on peut dire que le volcan est √©teint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temp√©rature du magma d√©passe les 500‚ÄØ¬∞C\n",
      "\n",
      "Seek: Quelle est la temp√©rature du magma ?\n",
      "\n",
      "Assess: La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\n",
      "\n",
      "Assess Cues:\n",
      "‚ÄÉ\"1\" : \"La terre contient 7 couches\",\n",
      "‚ÄÉ\"2\" : \"La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\",\n",
      "‚ÄÉ\"3\" : \"La temp√©rature √† l'√©ruption d'un volcan est √† 700‚ÄØ¬∞C\"\n",
      "\n",
      "Identify Validity: <NA>\n",
      "Guess Validity: 2\n",
      "Seek Validity: 2\n",
      "Assess Validity: 2\n",
      "Mechanical Rating: <NA>\n",
      "\n",
      "Reasoning:\n",
      "Identify nomme ¬´ le magma ¬ª, indiquant clairement le sujet.\n",
      "Guess propose une affirmation plausible sur la temp√©rature du magma (>500‚ÄØ¬∞C).\n",
      "Seek est une question bien formul√©e qui interroge sur la temp√©rature du magma.\n",
      "Assess fournit une r√©ponse explicite (‚âà1000‚ÄØ¬∞C) qui r√©pond directement √† la question pos√©e dans Seek et correspond exactement √† l‚Äôoption 2 des assess_cues, donc la r√©ponse est consid√©r√©e comme ¬´ trouv√©e ¬ª.\n",
      "\n",
      "Les √©tapes Identify/Guess/Seek/Assess sont toutes centr√©es sur le m√™me th√®me (la temp√©rature du magma) et restent li√©es au sujet du texte de r√©f√©rence (magma/volcans), bien que la temp√©rature sp√©cifique ne soit pas donn√©e dans le texte, ce qui respecte le crit√®re d‚Äôoriginalit√©.\n",
      "\n",
      "Les drapeaux num√©riques marquent d√©j√† Guess, Seek et Assess comme valides ; Identify est √©galement valide de mani√®re ind√©pendante.\n",
      "Ainsi, chaque crit√®re de validit√© du codebook est respect√©.\n",
      "\n",
      "Classification: 1\n",
      "\n",
      "Example 2:\n",
      "Id : bc5_3\n",
      "\n",
      "Text :\n",
      "Toutankhamon √©tait un pharaon, un roi de l'√âgypte antique.\n",
      "Il est tr√®s connu aujourd‚Äôhui parce que des arch√©ologues ont retrouv√© son cercueil intact avec tous ses tr√©sors, en 1922.\n",
      "Pour les √âgyptiens, il y avait une vie apr√®s la mort, une vie √©ternelle.\n",
      "C‚Äôest pour cela que le corps devait √™tre conserv√© dans le meilleur √©tat possible : c‚Äôest ce qu‚Äôon appelle la momification.\n",
      "C‚Äôest aussi pour cela que l‚Äôon retrouve aujourd‚Äôhui de la nourriture, des armes ou des tr√©sors dans les tombeaux.\n",
      "Ces objets accompagnaient le pharaon dans sa vie apr√®s la mort.\n",
      "\n",
      "Identify : L'√âgypte antique\n",
      "\n",
      "Guess : Les arch√©ologues sont des chercheurs scientifiques\n",
      "\n",
      "Seek : Qu‚Äôest-ce que l‚Äô√âgypte antique ?\n",
      "\n",
      "Assess : L‚Äô√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord\n",
      "\n",
      "Assess Cues :\n",
      "‚ÄÉ\"1\" : \"L'√âgypte antique est une ancienne civilisation de l'Afrique du Nord\",\n",
      "‚ÄÉ\"2\" : \"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\n",
      "‚ÄÉ\"3\" : \"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"\n",
      "\n",
      "Identify Validity : 1\n",
      "Guess Validity : <NA>\n",
      "Seek Validity : 1\n",
      "Assess Validity : 1\n",
      "Mechanical Rating : <NA>\n",
      "\n",
      "Reasoning :\n",
      "L‚Äô√©tape Identify (¬´ L‚Äô√âgypte antique ¬ª) est d√©j√† marqu√©e comme valide et identifie clairement le sujet.\n",
      "L‚Äô√©tape Seek est une question bien formul√©e (¬´ Qu‚Äôest-ce que l‚Äô√âgypte antique ? ¬ª), et elle est originale car le texte de r√©f√©rence ne d√©finit pas directement ce terme.\n",
      "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 1 des assess_cues, donc cette r√©ponse est correcte.\n",
      "\n",
      "Cependant, l‚Äô√©tape Guess (¬´ Les arch√©ologues sont des chercheurs scientifiques ¬ª) ne tente pas de r√©pondre √† la question pos√©e en Seek et n‚Äôest que vaguement li√©e au texte (elle parle des arch√©ologues, pas de l‚Äô√âgypte antique en soi).\n",
      "Comme l‚Äô√©tape Guess ne propose pas une explication pertinente en lien avec la m√™me question, la cha√Æne Identify‚ÄìGuess‚ÄìSeek n‚Äôest pas coh√©rente.\n",
      "\n",
      "L‚Äô√©chec √† l‚Äô√©tape Guess et le manque de coh√©rence invalident tout le cycle.\n",
      "\n",
      "Classification : 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The Identify step ('le magma') clearly indicates the topic of interest, which is the magma. The Guess step ('La temp√©rature du magma d√©passe les 500¬∞C') suggests a plausible explanation related to the temperature of magma, which is consistent with the topic. The Seek step ('Quelle est la temp√©rature du magma?') is formulated as a question and is directly related to the topic of magma. The Assess step ('La temp√©rature du magma atteint les 1000¬∞C') provides a specific answer to the Seek question, and this answer is found in the assess_cues as option 2, indicating that an answer was indeed found. All steps are consistent and related to the topic of the reference text, which discusses magma and volcanic eruptions, but does not provide the specific temperature of magma, ensuring originality. Therefore, all criteria for a valid cycle are met.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 2000\n",
      "Cost => 0.0064\n",
      "\n",
      "=== Processing Verbatim 2/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc4_6\n",
      "\n",
      "Text: La religion de la Grece antique comprend plusieurs dieux. Les aventures de ces dieux forment la mythologie grecque, l'une des mythologies les plus developpees de l'histoire antique. D'apres les Grecs, les 12 principaux dieux vivent sur l'Olympe et ont une apparence et un comportement comparables a celui des humains. Les dieux les plus connus sont : Athena, deesse de la guerre, Zeus, roi des dieux et dieu de la foudre. Hera, deesse du mariage. Poseidon, dieu des oceans.\n",
      "\n",
      "Identify: Une mythologie\n",
      "\n",
      "Guess: Une mythologie est un ensemble de contes imaginaires\n",
      "\n",
      "Seek: Qu'est-ce qu'une mythologie\n",
      "\n",
      "Assess: Une mythologie est un ensemble de legendes lies a une civilisation bien precise\n",
      "\n",
      "assess Cues: {\"1\":\"Une mythologie est un ensemble de l√©gendes li√©es √† une civilisation\",\"2\":\"L'histoire comprend beaucoup de mythologies\",\"3\":\"L'Olympe est la plus haute montagne de Gr√®ce\"}\n",
      "\n",
      "Identify Validity: 1\n",
      "\n",
      "Guess Validity: 1\n",
      "\n",
      "Seek Validity: 1\n",
      "\n",
      "Assess Validity: 1\n",
      "\n",
      "mechanical Rating: 1\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Examples\n",
      "Example 1:\n",
      "\n",
      "Id: bc4_1\n",
      "\n",
      "Text:\n",
      "A des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu'on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma.\n",
      "Le magma est responsable de l'explosion des volcans : c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan.\n",
      "On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es.\n",
      "Au-del√† de 10 000 ans, on peut dire que le volcan est √©teint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temp√©rature du magma d√©passe les 500‚ÄØ¬∞C\n",
      "\n",
      "Seek: Quelle est la temp√©rature du magma ?\n",
      "\n",
      "Assess: La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\n",
      "\n",
      "Assess Cues:\n",
      "‚ÄÉ\"1\" : \"La terre contient 7 couches\",\n",
      "‚ÄÉ\"2\" : \"La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\",\n",
      "‚ÄÉ\"3\" : \"La temp√©rature √† l'√©ruption d'un volcan est √† 700‚ÄØ¬∞C\"\n",
      "\n",
      "Identify Validity: <NA>\n",
      "Guess Validity: 2\n",
      "Seek Validity: 2\n",
      "Assess Validity: 2\n",
      "Mechanical Rating: <NA>\n",
      "\n",
      "Reasoning:\n",
      "Identify nomme ¬´ le magma ¬ª, indiquant clairement le sujet.\n",
      "Guess propose une affirmation plausible sur la temp√©rature du magma (>500‚ÄØ¬∞C).\n",
      "Seek est une question bien formul√©e qui interroge sur la temp√©rature du magma.\n",
      "Assess fournit une r√©ponse explicite (‚âà1000‚ÄØ¬∞C) qui r√©pond directement √† la question pos√©e dans Seek et correspond exactement √† l‚Äôoption 2 des assess_cues, donc la r√©ponse est consid√©r√©e comme ¬´ trouv√©e ¬ª.\n",
      "\n",
      "Les √©tapes Identify/Guess/Seek/Assess sont toutes centr√©es sur le m√™me th√®me (la temp√©rature du magma) et restent li√©es au sujet du texte de r√©f√©rence (magma/volcans), bien que la temp√©rature sp√©cifique ne soit pas donn√©e dans le texte, ce qui respecte le crit√®re d‚Äôoriginalit√©.\n",
      "\n",
      "Les drapeaux num√©riques marquent d√©j√† Guess, Seek et Assess comme valides ; Identify est √©galement valide de mani√®re ind√©pendante.\n",
      "Ainsi, chaque crit√®re de validit√© du codebook est respect√©.\n",
      "\n",
      "Classification: 1\n",
      "\n",
      "Example 2:\n",
      "Id : bc5_3\n",
      "\n",
      "Text :\n",
      "Toutankhamon √©tait un pharaon, un roi de l'√âgypte antique.\n",
      "Il est tr√®s connu aujourd‚Äôhui parce que des arch√©ologues ont retrouv√© son cercueil intact avec tous ses tr√©sors, en 1922.\n",
      "Pour les √âgyptiens, il y avait une vie apr√®s la mort, une vie √©ternelle.\n",
      "C‚Äôest pour cela que le corps devait √™tre conserv√© dans le meilleur √©tat possible : c‚Äôest ce qu‚Äôon appelle la momification.\n",
      "C‚Äôest aussi pour cela que l‚Äôon retrouve aujourd‚Äôhui de la nourriture, des armes ou des tr√©sors dans les tombeaux.\n",
      "Ces objets accompagnaient le pharaon dans sa vie apr√®s la mort.\n",
      "\n",
      "Identify : L'√âgypte antique\n",
      "\n",
      "Guess : Les arch√©ologues sont des chercheurs scientifiques\n",
      "\n",
      "Seek : Qu‚Äôest-ce que l‚Äô√âgypte antique ?\n",
      "\n",
      "Assess : L‚Äô√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord\n",
      "\n",
      "Assess Cues :\n",
      "‚ÄÉ\"1\" : \"L'√âgypte antique est une ancienne civilisation de l'Afrique du Nord\",\n",
      "‚ÄÉ\"2\" : \"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\n",
      "‚ÄÉ\"3\" : \"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"\n",
      "\n",
      "Identify Validity : 1\n",
      "Guess Validity : <NA>\n",
      "Seek Validity : 1\n",
      "Assess Validity : 1\n",
      "Mechanical Rating : <NA>\n",
      "\n",
      "Reasoning :\n",
      "L‚Äô√©tape Identify (¬´ L‚Äô√âgypte antique ¬ª) est d√©j√† marqu√©e comme valide et identifie clairement le sujet.\n",
      "L‚Äô√©tape Seek est une question bien formul√©e (¬´ Qu‚Äôest-ce que l‚Äô√âgypte antique ? ¬ª), et elle est originale car le texte de r√©f√©rence ne d√©finit pas directement ce terme.\n",
      "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 1 des assess_cues, donc cette r√©ponse est correcte.\n",
      "\n",
      "Cependant, l‚Äô√©tape Guess (¬´ Les arch√©ologues sont des chercheurs scientifiques ¬ª) ne tente pas de r√©pondre √† la question pos√©e en Seek et n‚Äôest que vaguement li√©e au texte (elle parle des arch√©ologues, pas de l‚Äô√âgypte antique en soi).\n",
      "Comme l‚Äô√©tape Guess ne propose pas une explication pertinente en lien avec la m√™me question, la cha√Æne Identify‚ÄìGuess‚ÄìSeek n‚Äôest pas coh√©rente.\n",
      "\n",
      "L‚Äô√©chec √† l‚Äô√©tape Guess et le manque de coh√©rence invalident tout le cycle.\n",
      "\n",
      "Classification : 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The mechanical_rating is already provided as 1, which overrides any other logic in the codebook. Therefore, the cycle is automatically considered valid without further evaluation of the individual steps.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1879\n",
      "Cost => 0.0051\n",
      "\n",
      "=== Processing Verbatim 3/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc4_7\n",
      "\n",
      "Text: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite. Selon la legende, Rome aurait ete fondee par Romulus qui aurai donne son nom a la ville. Au depart, ce n'etait qu'un groupe de quelques villages, puis l'Empire romain va couvrir une grande partie de l'Europe et entourer toute la mer Mediterranee. La religion romaine comptait de nombreux dieux, inspires en partie des dieux grecs.\n",
      "\n",
      "Identify: Romulus\n",
      "\n",
      "Guess: Romulus est un roi de Rome\n",
      "\n",
      "Seek: Qui et Romulus mythologie a\n",
      "\n",
      "Assess: Dans la legende Romulus et le premier roi de Rome\n",
      "\n",
      "assess Cues: {\"1\":\"Dans la l√©gende, Romulus est le premier roi de Rome\",\"2\":\"L'Antiquit√© s'est √©tendue entre 3200 avant J.C, jusqu'√† l'ann√©e 476\",\"4\":\"La M√©diterran√©e touche plusieurs continents\"}\n",
      "\n",
      "Identify Validity: 1\n",
      "\n",
      "Guess Validity: 1\n",
      "\n",
      "Seek Validity: 1\n",
      "\n",
      "Assess Validity: 1\n",
      "\n",
      "mechanical Rating: 1\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Examples\n",
      "Example 1:\n",
      "\n",
      "Id: bc4_1\n",
      "\n",
      "Text:\n",
      "A des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu'on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma.\n",
      "Le magma est responsable de l'explosion des volcans : c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan.\n",
      "On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es.\n",
      "Au-del√† de 10 000 ans, on peut dire que le volcan est √©teint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temp√©rature du magma d√©passe les 500‚ÄØ¬∞C\n",
      "\n",
      "Seek: Quelle est la temp√©rature du magma ?\n",
      "\n",
      "Assess: La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\n",
      "\n",
      "Assess Cues:\n",
      "‚ÄÉ\"1\" : \"La terre contient 7 couches\",\n",
      "‚ÄÉ\"2\" : \"La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\",\n",
      "‚ÄÉ\"3\" : \"La temp√©rature √† l'√©ruption d'un volcan est √† 700‚ÄØ¬∞C\"\n",
      "\n",
      "Identify Validity: <NA>\n",
      "Guess Validity: 2\n",
      "Seek Validity: 2\n",
      "Assess Validity: 2\n",
      "Mechanical Rating: <NA>\n",
      "\n",
      "Reasoning:\n",
      "Identify nomme ¬´ le magma ¬ª, indiquant clairement le sujet.\n",
      "Guess propose une affirmation plausible sur la temp√©rature du magma (>500‚ÄØ¬∞C).\n",
      "Seek est une question bien formul√©e qui interroge sur la temp√©rature du magma.\n",
      "Assess fournit une r√©ponse explicite (‚âà1000‚ÄØ¬∞C) qui r√©pond directement √† la question pos√©e dans Seek et correspond exactement √† l‚Äôoption 2 des assess_cues, donc la r√©ponse est consid√©r√©e comme ¬´ trouv√©e ¬ª.\n",
      "\n",
      "Les √©tapes Identify/Guess/Seek/Assess sont toutes centr√©es sur le m√™me th√®me (la temp√©rature du magma) et restent li√©es au sujet du texte de r√©f√©rence (magma/volcans), bien que la temp√©rature sp√©cifique ne soit pas donn√©e dans le texte, ce qui respecte le crit√®re d‚Äôoriginalit√©.\n",
      "\n",
      "Les drapeaux num√©riques marquent d√©j√† Guess, Seek et Assess comme valides ; Identify est √©galement valide de mani√®re ind√©pendante.\n",
      "Ainsi, chaque crit√®re de validit√© du codebook est respect√©.\n",
      "\n",
      "Classification: 1\n",
      "\n",
      "Example 2:\n",
      "Id : bc5_3\n",
      "\n",
      "Text :\n",
      "Toutankhamon √©tait un pharaon, un roi de l'√âgypte antique.\n",
      "Il est tr√®s connu aujourd‚Äôhui parce que des arch√©ologues ont retrouv√© son cercueil intact avec tous ses tr√©sors, en 1922.\n",
      "Pour les √âgyptiens, il y avait une vie apr√®s la mort, une vie √©ternelle.\n",
      "C‚Äôest pour cela que le corps devait √™tre conserv√© dans le meilleur √©tat possible : c‚Äôest ce qu‚Äôon appelle la momification.\n",
      "C‚Äôest aussi pour cela que l‚Äôon retrouve aujourd‚Äôhui de la nourriture, des armes ou des tr√©sors dans les tombeaux.\n",
      "Ces objets accompagnaient le pharaon dans sa vie apr√®s la mort.\n",
      "\n",
      "Identify : L'√âgypte antique\n",
      "\n",
      "Guess : Les arch√©ologues sont des chercheurs scientifiques\n",
      "\n",
      "Seek : Qu‚Äôest-ce que l‚Äô√âgypte antique ?\n",
      "\n",
      "Assess : L‚Äô√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord\n",
      "\n",
      "Assess Cues :\n",
      "‚ÄÉ\"1\" : \"L'√âgypte antique est une ancienne civilisation de l'Afrique du Nord\",\n",
      "‚ÄÉ\"2\" : \"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\n",
      "‚ÄÉ\"3\" : \"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"\n",
      "\n",
      "Identify Validity : 1\n",
      "Guess Validity : <NA>\n",
      "Seek Validity : 1\n",
      "Assess Validity : 1\n",
      "Mechanical Rating : <NA>\n",
      "\n",
      "Reasoning :\n",
      "L‚Äô√©tape Identify (¬´ L‚Äô√âgypte antique ¬ª) est d√©j√† marqu√©e comme valide et identifie clairement le sujet.\n",
      "L‚Äô√©tape Seek est une question bien formul√©e (¬´ Qu‚Äôest-ce que l‚Äô√âgypte antique ? ¬ª), et elle est originale car le texte de r√©f√©rence ne d√©finit pas directement ce terme.\n",
      "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 1 des assess_cues, donc cette r√©ponse est correcte.\n",
      "\n",
      "Cependant, l‚Äô√©tape Guess (¬´ Les arch√©ologues sont des chercheurs scientifiques ¬ª) ne tente pas de r√©pondre √† la question pos√©e en Seek et n‚Äôest que vaguement li√©e au texte (elle parle des arch√©ologues, pas de l‚Äô√âgypte antique en soi).\n",
      "Comme l‚Äô√©tape Guess ne propose pas une explication pertinente en lien avec la m√™me question, la cha√Æne Identify‚ÄìGuess‚ÄìSeek n‚Äôest pas coh√©rente.\n",
      "\n",
      "L‚Äô√©chec √† l‚Äô√©tape Guess et le manque de coh√©rence invalident tout le cycle.\n",
      "\n",
      "Classification : 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The mechanical_rating is already provided as 1, which overrides any other logic in the codebook. Therefore, the cycle is automatically considered valid without further evaluation of the individual steps.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1863\n",
      "Cost => 0.0051\n",
      "\n",
      "=== Processing Verbatim 4/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc4_8\n",
      "\n",
      "Text: La foret tropicale humide a un climat qui fournit beaucoup d'eau et une temperature elevee toute l'annee. Cela favorise la densite et la croissance permanente des plantes de ces forets.  En Asie, on appelle ce genre de foret 'la jungle'. Elle est parfois appelee foret dense, bien que cette expression signifie seulement que les arbres sont tres proches les uns des autres. Dans les forets tropicales, les arbres formant trois etages de hauteur differentes, sont tres serres et perdent leurs feuilles irregulierement.\n",
      "\n",
      "Identify: Les plantes de foret tropicales\n",
      "\n",
      "Guess: Les foret tropicale contiennent tout type de plantes\n",
      "\n",
      "Seek: Quelle plante peut-on voir dans les forets\n",
      "\n",
      "Assess: Les foret tropicale sont toujours vertes car il pleut tout le temps\n",
      "\n",
      "assess Cues: {\"4\":\"Les for√™ts tropicales sont toujours vertes car il pleut tout le temps\",\"2\":\"La temp√©rature dans les for√™ts tropicales est entre 30¬∞ et 40¬∞C\",\"3\":\"La for√™t tropicale se trouve en Asie, en Am√©rique du Sud et en Afrique\"}\n",
      "\n",
      "Identify Validity: 1\n",
      "\n",
      "Guess Validity: 1\n",
      "\n",
      "Seek Validity: 1\n",
      "\n",
      "Assess Validity: 4\n",
      "\n",
      "mechanical Rating: 0\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Examples\n",
      "Example 1:\n",
      "\n",
      "Id: bc4_1\n",
      "\n",
      "Text:\n",
      "A des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu'on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma.\n",
      "Le magma est responsable de l'explosion des volcans : c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan.\n",
      "On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es.\n",
      "Au-del√† de 10 000 ans, on peut dire que le volcan est √©teint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temp√©rature du magma d√©passe les 500‚ÄØ¬∞C\n",
      "\n",
      "Seek: Quelle est la temp√©rature du magma ?\n",
      "\n",
      "Assess: La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\n",
      "\n",
      "Assess Cues:\n",
      "‚ÄÉ\"1\" : \"La terre contient 7 couches\",\n",
      "‚ÄÉ\"2\" : \"La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\",\n",
      "‚ÄÉ\"3\" : \"La temp√©rature √† l'√©ruption d'un volcan est √† 700‚ÄØ¬∞C\"\n",
      "\n",
      "Identify Validity: <NA>\n",
      "Guess Validity: 2\n",
      "Seek Validity: 2\n",
      "Assess Validity: 2\n",
      "Mechanical Rating: <NA>\n",
      "\n",
      "Reasoning:\n",
      "Identify nomme ¬´ le magma ¬ª, indiquant clairement le sujet.\n",
      "Guess propose une affirmation plausible sur la temp√©rature du magma (>500‚ÄØ¬∞C).\n",
      "Seek est une question bien formul√©e qui interroge sur la temp√©rature du magma.\n",
      "Assess fournit une r√©ponse explicite (‚âà1000‚ÄØ¬∞C) qui r√©pond directement √† la question pos√©e dans Seek et correspond exactement √† l‚Äôoption 2 des assess_cues, donc la r√©ponse est consid√©r√©e comme ¬´ trouv√©e ¬ª.\n",
      "\n",
      "Les √©tapes Identify/Guess/Seek/Assess sont toutes centr√©es sur le m√™me th√®me (la temp√©rature du magma) et restent li√©es au sujet du texte de r√©f√©rence (magma/volcans), bien que la temp√©rature sp√©cifique ne soit pas donn√©e dans le texte, ce qui respecte le crit√®re d‚Äôoriginalit√©.\n",
      "\n",
      "Les drapeaux num√©riques marquent d√©j√† Guess, Seek et Assess comme valides ; Identify est √©galement valide de mani√®re ind√©pendante.\n",
      "Ainsi, chaque crit√®re de validit√© du codebook est respect√©.\n",
      "\n",
      "Classification: 1\n",
      "\n",
      "Example 2:\n",
      "Id : bc5_3\n",
      "\n",
      "Text :\n",
      "Toutankhamon √©tait un pharaon, un roi de l'√âgypte antique.\n",
      "Il est tr√®s connu aujourd‚Äôhui parce que des arch√©ologues ont retrouv√© son cercueil intact avec tous ses tr√©sors, en 1922.\n",
      "Pour les √âgyptiens, il y avait une vie apr√®s la mort, une vie √©ternelle.\n",
      "C‚Äôest pour cela que le corps devait √™tre conserv√© dans le meilleur √©tat possible : c‚Äôest ce qu‚Äôon appelle la momification.\n",
      "C‚Äôest aussi pour cela que l‚Äôon retrouve aujourd‚Äôhui de la nourriture, des armes ou des tr√©sors dans les tombeaux.\n",
      "Ces objets accompagnaient le pharaon dans sa vie apr√®s la mort.\n",
      "\n",
      "Identify : L'√âgypte antique\n",
      "\n",
      "Guess : Les arch√©ologues sont des chercheurs scientifiques\n",
      "\n",
      "Seek : Qu‚Äôest-ce que l‚Äô√âgypte antique ?\n",
      "\n",
      "Assess : L‚Äô√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord\n",
      "\n",
      "Assess Cues :\n",
      "‚ÄÉ\"1\" : \"L'√âgypte antique est une ancienne civilisation de l'Afrique du Nord\",\n",
      "‚ÄÉ\"2\" : \"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\n",
      "‚ÄÉ\"3\" : \"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"\n",
      "\n",
      "Identify Validity : 1\n",
      "Guess Validity : <NA>\n",
      "Seek Validity : 1\n",
      "Assess Validity : 1\n",
      "Mechanical Rating : <NA>\n",
      "\n",
      "Reasoning :\n",
      "L‚Äô√©tape Identify (¬´ L‚Äô√âgypte antique ¬ª) est d√©j√† marqu√©e comme valide et identifie clairement le sujet.\n",
      "L‚Äô√©tape Seek est une question bien formul√©e (¬´ Qu‚Äôest-ce que l‚Äô√âgypte antique ? ¬ª), et elle est originale car le texte de r√©f√©rence ne d√©finit pas directement ce terme.\n",
      "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 1 des assess_cues, donc cette r√©ponse est correcte.\n",
      "\n",
      "Cependant, l‚Äô√©tape Guess (¬´ Les arch√©ologues sont des chercheurs scientifiques ¬ª) ne tente pas de r√©pondre √† la question pos√©e en Seek et n‚Äôest que vaguement li√©e au texte (elle parle des arch√©ologues, pas de l‚Äô√âgypte antique en soi).\n",
      "Comme l‚Äô√©tape Guess ne propose pas une explication pertinente en lien avec la m√™me question, la cha√Æne Identify‚ÄìGuess‚ÄìSeek n‚Äôest pas coh√©rente.\n",
      "\n",
      "L‚Äô√©chec √† l‚Äô√©tape Guess et le manque de coh√©rence invalident tout le cycle.\n",
      "\n",
      "Classification : 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The mechanical_rating is already provided as 0, which overrides any other logic in the codebook. Therefore, the cycle is classified as invalid without further evaluation.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1893\n",
      "Cost => 0.0051\n",
      "\n",
      "=== Processing Verbatim 5/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc5_1\n",
      "\n",
      "Text: A des dizaines de kilometres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une eruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune eruption dans les 10 000 dernieres annees. Au dela de 10 000 ans, on peut dire que le volcan est eteint.\n",
      "\n",
      "Identify: Pourqoi on l'appelle le manteau.\n",
      "\n",
      "Guess: le magma et une pierre.\n",
      "\n",
      "Seek: quel et la temperature magma\n",
      "\n",
      "Assess: 1 000c\n",
      "\n",
      "assess Cues: {\"1\":\"La terre contient 7 couches\",\"2\":\"La temp√©rature du magma atteint les 1000¬∞C\",\"3\":\"La temp√©rature √† l'√©ruption d'un volcan est √† 700¬∞C\"}\n",
      "\n",
      "Identify Validity: <NA>\n",
      "\n",
      "Guess Validity: <NA>\n",
      "\n",
      "Seek Validity: 2\n",
      "\n",
      "Assess Validity: <NA>\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Examples\n",
      "Example 1:\n",
      "\n",
      "Id: bc4_1\n",
      "\n",
      "Text:\n",
      "A des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu'on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma.\n",
      "Le magma est responsable de l'explosion des volcans : c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan.\n",
      "On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es.\n",
      "Au-del√† de 10 000 ans, on peut dire que le volcan est √©teint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temp√©rature du magma d√©passe les 500‚ÄØ¬∞C\n",
      "\n",
      "Seek: Quelle est la temp√©rature du magma ?\n",
      "\n",
      "Assess: La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\n",
      "\n",
      "Assess Cues:\n",
      "‚ÄÉ\"1\" : \"La terre contient 7 couches\",\n",
      "‚ÄÉ\"2\" : \"La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\",\n",
      "‚ÄÉ\"3\" : \"La temp√©rature √† l'√©ruption d'un volcan est √† 700‚ÄØ¬∞C\"\n",
      "\n",
      "Identify Validity: <NA>\n",
      "Guess Validity: 2\n",
      "Seek Validity: 2\n",
      "Assess Validity: 2\n",
      "Mechanical Rating: <NA>\n",
      "\n",
      "Reasoning:\n",
      "Identify nomme ¬´ le magma ¬ª, indiquant clairement le sujet.\n",
      "Guess propose une affirmation plausible sur la temp√©rature du magma (>500‚ÄØ¬∞C).\n",
      "Seek est une question bien formul√©e qui interroge sur la temp√©rature du magma.\n",
      "Assess fournit une r√©ponse explicite (‚âà1000‚ÄØ¬∞C) qui r√©pond directement √† la question pos√©e dans Seek et correspond exactement √† l‚Äôoption 2 des assess_cues, donc la r√©ponse est consid√©r√©e comme ¬´ trouv√©e ¬ª.\n",
      "\n",
      "Les √©tapes Identify/Guess/Seek/Assess sont toutes centr√©es sur le m√™me th√®me (la temp√©rature du magma) et restent li√©es au sujet du texte de r√©f√©rence (magma/volcans), bien que la temp√©rature sp√©cifique ne soit pas donn√©e dans le texte, ce qui respecte le crit√®re d‚Äôoriginalit√©.\n",
      "\n",
      "Les drapeaux num√©riques marquent d√©j√† Guess, Seek et Assess comme valides ; Identify est √©galement valide de mani√®re ind√©pendante.\n",
      "Ainsi, chaque crit√®re de validit√© du codebook est respect√©.\n",
      "\n",
      "Classification: 1\n",
      "\n",
      "Example 2:\n",
      "Id : bc5_3\n",
      "\n",
      "Text :\n",
      "Toutankhamon √©tait un pharaon, un roi de l'√âgypte antique.\n",
      "Il est tr√®s connu aujourd‚Äôhui parce que des arch√©ologues ont retrouv√© son cercueil intact avec tous ses tr√©sors, en 1922.\n",
      "Pour les √âgyptiens, il y avait une vie apr√®s la mort, une vie √©ternelle.\n",
      "C‚Äôest pour cela que le corps devait √™tre conserv√© dans le meilleur √©tat possible : c‚Äôest ce qu‚Äôon appelle la momification.\n",
      "C‚Äôest aussi pour cela que l‚Äôon retrouve aujourd‚Äôhui de la nourriture, des armes ou des tr√©sors dans les tombeaux.\n",
      "Ces objets accompagnaient le pharaon dans sa vie apr√®s la mort.\n",
      "\n",
      "Identify : L'√âgypte antique\n",
      "\n",
      "Guess : Les arch√©ologues sont des chercheurs scientifiques\n",
      "\n",
      "Seek : Qu‚Äôest-ce que l‚Äô√âgypte antique ?\n",
      "\n",
      "Assess : L‚Äô√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord\n",
      "\n",
      "Assess Cues :\n",
      "‚ÄÉ\"1\" : \"L'√âgypte antique est une ancienne civilisation de l'Afrique du Nord\",\n",
      "‚ÄÉ\"2\" : \"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\n",
      "‚ÄÉ\"3\" : \"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"\n",
      "\n",
      "Identify Validity : 1\n",
      "Guess Validity : <NA>\n",
      "Seek Validity : 1\n",
      "Assess Validity : 1\n",
      "Mechanical Rating : <NA>\n",
      "\n",
      "Reasoning :\n",
      "L‚Äô√©tape Identify (¬´ L‚Äô√âgypte antique ¬ª) est d√©j√† marqu√©e comme valide et identifie clairement le sujet.\n",
      "L‚Äô√©tape Seek est une question bien formul√©e (¬´ Qu‚Äôest-ce que l‚Äô√âgypte antique ? ¬ª), et elle est originale car le texte de r√©f√©rence ne d√©finit pas directement ce terme.\n",
      "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 1 des assess_cues, donc cette r√©ponse est correcte.\n",
      "\n",
      "Cependant, l‚Äô√©tape Guess (¬´ Les arch√©ologues sont des chercheurs scientifiques ¬ª) ne tente pas de r√©pondre √† la question pos√©e en Seek et n‚Äôest que vaguement li√©e au texte (elle parle des arch√©ologues, pas de l‚Äô√âgypte antique en soi).\n",
      "Comme l‚Äô√©tape Guess ne propose pas une explication pertinente en lien avec la m√™me question, la cha√Æne Identify‚ÄìGuess‚ÄìSeek n‚Äôest pas coh√©rente.\n",
      "\n",
      "L‚Äô√©chec √† l‚Äô√©tape Guess et le manque de coh√©rence invalident tout le cycle.\n",
      "\n",
      "Classification : 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The Identify step ('Pourqoi on l'appelle le manteau') indicates a topic of interest related to the reference text, which mentions the mantle. The Guess step ('le magma et une pierre') does not suggest a plausible explanation related to the Identify step or the reference text. The Seek step ('quel et la temperature magma') is formulated as a question and is related to the reference text, but it is not consistent with the Identify and Guess steps. The Assess step ('1 000c') provides an answer that matches option 2 in the assess_cues ('La temp√©rature du magma atteint les 1000¬∞C'), which answers the Seek question. However, the cycle lacks consistency between Identify, Guess, and Seek steps, and the Guess step does not provide a valid explanation. Therefore, the cycle is invalid.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1989\n",
      "Cost => 0.0064\n",
      "\n",
      "=== Processing Verbatim 6/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc5_2\n",
      "\n",
      "Text: La memoire, cest une fonction de notre cerveau qui nous permet de garder les informations pour ensuite les recuperer et les utiliser. Tables de multiplication, poesies, ... notre cerveau est capable denregistrer et d'organiser enormement dinformations. Il recolte ces informations grace a nos 5 sens. Les informations sont ensuite envoyees dans differentes zones du cerveau.  Il y a donc une zone pour les sons, une zone pour les images ... Et la memoire, cest comme un muscle : en sentrainant, on peut retenir plus de choses et plus facilement !\n",
      "\n",
      "Identify: quelle et les 5sens\n",
      "\n",
      "Guess: je pense que les 5sens sont bouge apprendre ? ? ?\n",
      "\n",
      "Seek: quelle et les 5 sens du cerveau.\n",
      "\n",
      "Assess: non\n",
      "\n",
      "assess Cues: {\"1\":\"Le cerveau contr√¥le toutes nos actions, volontaires et involontaires\",\"2\":\"Notre cerveau peut stocker plus d'informations que 100 ordinateurs r√©unis\",\"3\":\"Il y a plusieurs activit√©s qui aident la m√©moire mais, surtout, il faut avoir un sommeil suffisant et bien r√©gl√© \"}\n",
      "\n",
      "Identify Validity: <NA>\n",
      "\n",
      "Guess Validity: <NA>\n",
      "\n",
      "Seek Validity: 1\n",
      "\n",
      "Assess Validity: <NA>\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Examples\n",
      "Example 1:\n",
      "\n",
      "Id: bc4_1\n",
      "\n",
      "Text:\n",
      "A des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu'on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma.\n",
      "Le magma est responsable de l'explosion des volcans : c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan.\n",
      "On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es.\n",
      "Au-del√† de 10 000 ans, on peut dire que le volcan est √©teint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temp√©rature du magma d√©passe les 500‚ÄØ¬∞C\n",
      "\n",
      "Seek: Quelle est la temp√©rature du magma ?\n",
      "\n",
      "Assess: La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\n",
      "\n",
      "Assess Cues:\n",
      "‚ÄÉ\"1\" : \"La terre contient 7 couches\",\n",
      "‚ÄÉ\"2\" : \"La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\",\n",
      "‚ÄÉ\"3\" : \"La temp√©rature √† l'√©ruption d'un volcan est √† 700‚ÄØ¬∞C\"\n",
      "\n",
      "Identify Validity: <NA>\n",
      "Guess Validity: 2\n",
      "Seek Validity: 2\n",
      "Assess Validity: 2\n",
      "Mechanical Rating: <NA>\n",
      "\n",
      "Reasoning:\n",
      "Identify nomme ¬´ le magma ¬ª, indiquant clairement le sujet.\n",
      "Guess propose une affirmation plausible sur la temp√©rature du magma (>500‚ÄØ¬∞C).\n",
      "Seek est une question bien formul√©e qui interroge sur la temp√©rature du magma.\n",
      "Assess fournit une r√©ponse explicite (‚âà1000‚ÄØ¬∞C) qui r√©pond directement √† la question pos√©e dans Seek et correspond exactement √† l‚Äôoption 2 des assess_cues, donc la r√©ponse est consid√©r√©e comme ¬´ trouv√©e ¬ª.\n",
      "\n",
      "Les √©tapes Identify/Guess/Seek/Assess sont toutes centr√©es sur le m√™me th√®me (la temp√©rature du magma) et restent li√©es au sujet du texte de r√©f√©rence (magma/volcans), bien que la temp√©rature sp√©cifique ne soit pas donn√©e dans le texte, ce qui respecte le crit√®re d‚Äôoriginalit√©.\n",
      "\n",
      "Les drapeaux num√©riques marquent d√©j√† Guess, Seek et Assess comme valides ; Identify est √©galement valide de mani√®re ind√©pendante.\n",
      "Ainsi, chaque crit√®re de validit√© du codebook est respect√©.\n",
      "\n",
      "Classification: 1\n",
      "\n",
      "Example 2:\n",
      "Id : bc5_3\n",
      "\n",
      "Text :\n",
      "Toutankhamon √©tait un pharaon, un roi de l'√âgypte antique.\n",
      "Il est tr√®s connu aujourd‚Äôhui parce que des arch√©ologues ont retrouv√© son cercueil intact avec tous ses tr√©sors, en 1922.\n",
      "Pour les √âgyptiens, il y avait une vie apr√®s la mort, une vie √©ternelle.\n",
      "C‚Äôest pour cela que le corps devait √™tre conserv√© dans le meilleur √©tat possible : c‚Äôest ce qu‚Äôon appelle la momification.\n",
      "C‚Äôest aussi pour cela que l‚Äôon retrouve aujourd‚Äôhui de la nourriture, des armes ou des tr√©sors dans les tombeaux.\n",
      "Ces objets accompagnaient le pharaon dans sa vie apr√®s la mort.\n",
      "\n",
      "Identify : L'√âgypte antique\n",
      "\n",
      "Guess : Les arch√©ologues sont des chercheurs scientifiques\n",
      "\n",
      "Seek : Qu‚Äôest-ce que l‚Äô√âgypte antique ?\n",
      "\n",
      "Assess : L‚Äô√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord\n",
      "\n",
      "Assess Cues :\n",
      "‚ÄÉ\"1\" : \"L'√âgypte antique est une ancienne civilisation de l'Afrique du Nord\",\n",
      "‚ÄÉ\"2\" : \"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\n",
      "‚ÄÉ\"3\" : \"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"\n",
      "\n",
      "Identify Validity : 1\n",
      "Guess Validity : <NA>\n",
      "Seek Validity : 1\n",
      "Assess Validity : 1\n",
      "Mechanical Rating : <NA>\n",
      "\n",
      "Reasoning :\n",
      "L‚Äô√©tape Identify (¬´ L‚Äô√âgypte antique ¬ª) est d√©j√† marqu√©e comme valide et identifie clairement le sujet.\n",
      "L‚Äô√©tape Seek est une question bien formul√©e (¬´ Qu‚Äôest-ce que l‚Äô√âgypte antique ? ¬ª), et elle est originale car le texte de r√©f√©rence ne d√©finit pas directement ce terme.\n",
      "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 1 des assess_cues, donc cette r√©ponse est correcte.\n",
      "\n",
      "Cependant, l‚Äô√©tape Guess (¬´ Les arch√©ologues sont des chercheurs scientifiques ¬ª) ne tente pas de r√©pondre √† la question pos√©e en Seek et n‚Äôest que vaguement li√©e au texte (elle parle des arch√©ologues, pas de l‚Äô√âgypte antique en soi).\n",
      "Comme l‚Äô√©tape Guess ne propose pas une explication pertinente en lien avec la m√™me question, la cha√Æne Identify‚ÄìGuess‚ÄìSeek n‚Äôest pas coh√©rente.\n",
      "\n",
      "L‚Äô√©chec √† l‚Äô√©tape Guess et le manque de coh√©rence invalident tout le cycle.\n",
      "\n",
      "Classification : 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The Identify step ('quelle et les 5sens') indicates a topic of interest related to the text, which discusses how the brain collects information through the five senses. The Guess step ('je pense que les 5sens sont bouge apprendre ? ? ?') attempts to suggest a possible explanation, although it is not clearly articulated. The Seek step ('quelle et les 5 sens du cerveau.') is formulated as a question, asking about the five senses, which is not directly answered in the reference text. The Assess step states 'non', indicating no answer was found, which is valid as the answer to the Seek question is not present in the assess_cues. The Identify, Guess, and Seek steps are related to the same question about the five senses and are consistent with the topic of the reference text. The Seek question is original as the specific details about the five senses are not found in the reference text. Since all criteria are met, the cycle is valid.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 2048\n",
      "Cost => 0.0067\n",
      "\n",
      "=== Processing Verbatim 7/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc5_3\n",
      "\n",
      "Text: Toutankhamon etait un pharaon, un roi de l'Egypte antique. Il est tres connu aujourdhui parce que des archeologues ont retrouve son cercueil intacte avec tous ses tresors, en 1922. Pour les Egyptiens, il y avait une vie apres la mort, une vie eternelle. Cest pour cela que le corps devait etre conserve dans le meilleur etat possible, cest ce quon appelle la momification. Cest aussi pour cela que lon retrouve aujourdhui de la nourriture, des armes ou des tresors dans les tombeaux. Ces objets accompagnaient le pharaon dans sa vie apres la mort.\n",
      "\n",
      "Identify: L'Egypte antique\n",
      "\n",
      "Guess: Les archeologues sont des chercheurs scientifiques\n",
      "\n",
      "Seek: Qu'est-ce que l'Egypte antique\n",
      "\n",
      "Assess: L'Egypte antique est une ancienne civilisation de l'Afrique du Nord\n",
      "\n",
      "assess Cues: {\"1\":\"L'Egypte antique est une ancienne civilisation de l'Afrique du Nord\",\"2\":\"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\"3\":\"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"}\n",
      "\n",
      "Identify Validity: 1\n",
      "\n",
      "Guess Validity: <NA>\n",
      "\n",
      "Seek Validity: 1\n",
      "\n",
      "Assess Validity: 1\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Examples\n",
      "Example 1:\n",
      "\n",
      "Id: bc4_1\n",
      "\n",
      "Text:\n",
      "A des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu'on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma.\n",
      "Le magma est responsable de l'explosion des volcans : c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan.\n",
      "On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es.\n",
      "Au-del√† de 10 000 ans, on peut dire que le volcan est √©teint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temp√©rature du magma d√©passe les 500‚ÄØ¬∞C\n",
      "\n",
      "Seek: Quelle est la temp√©rature du magma ?\n",
      "\n",
      "Assess: La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\n",
      "\n",
      "Assess Cues:\n",
      "‚ÄÉ\"1\" : \"La terre contient 7 couches\",\n",
      "‚ÄÉ\"2\" : \"La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\",\n",
      "‚ÄÉ\"3\" : \"La temp√©rature √† l'√©ruption d'un volcan est √† 700‚ÄØ¬∞C\"\n",
      "\n",
      "Identify Validity: <NA>\n",
      "Guess Validity: 2\n",
      "Seek Validity: 2\n",
      "Assess Validity: 2\n",
      "Mechanical Rating: <NA>\n",
      "\n",
      "Reasoning:\n",
      "Identify nomme ¬´ le magma ¬ª, indiquant clairement le sujet.\n",
      "Guess propose une affirmation plausible sur la temp√©rature du magma (>500‚ÄØ¬∞C).\n",
      "Seek est une question bien formul√©e qui interroge sur la temp√©rature du magma.\n",
      "Assess fournit une r√©ponse explicite (‚âà1000‚ÄØ¬∞C) qui r√©pond directement √† la question pos√©e dans Seek et correspond exactement √† l‚Äôoption 2 des assess_cues, donc la r√©ponse est consid√©r√©e comme ¬´ trouv√©e ¬ª.\n",
      "\n",
      "Les √©tapes Identify/Guess/Seek/Assess sont toutes centr√©es sur le m√™me th√®me (la temp√©rature du magma) et restent li√©es au sujet du texte de r√©f√©rence (magma/volcans), bien que la temp√©rature sp√©cifique ne soit pas donn√©e dans le texte, ce qui respecte le crit√®re d‚Äôoriginalit√©.\n",
      "\n",
      "Les drapeaux num√©riques marquent d√©j√† Guess, Seek et Assess comme valides ; Identify est √©galement valide de mani√®re ind√©pendante.\n",
      "Ainsi, chaque crit√®re de validit√© du codebook est respect√©.\n",
      "\n",
      "Classification: 1\n",
      "\n",
      "Example 2:\n",
      "Id : bc5_3\n",
      "\n",
      "Text :\n",
      "Toutankhamon √©tait un pharaon, un roi de l'√âgypte antique.\n",
      "Il est tr√®s connu aujourd‚Äôhui parce que des arch√©ologues ont retrouv√© son cercueil intact avec tous ses tr√©sors, en 1922.\n",
      "Pour les √âgyptiens, il y avait une vie apr√®s la mort, une vie √©ternelle.\n",
      "C‚Äôest pour cela que le corps devait √™tre conserv√© dans le meilleur √©tat possible : c‚Äôest ce qu‚Äôon appelle la momification.\n",
      "C‚Äôest aussi pour cela que l‚Äôon retrouve aujourd‚Äôhui de la nourriture, des armes ou des tr√©sors dans les tombeaux.\n",
      "Ces objets accompagnaient le pharaon dans sa vie apr√®s la mort.\n",
      "\n",
      "Identify : L'√âgypte antique\n",
      "\n",
      "Guess : Les arch√©ologues sont des chercheurs scientifiques\n",
      "\n",
      "Seek : Qu‚Äôest-ce que l‚Äô√âgypte antique ?\n",
      "\n",
      "Assess : L‚Äô√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord\n",
      "\n",
      "Assess Cues :\n",
      "‚ÄÉ\"1\" : \"L'√âgypte antique est une ancienne civilisation de l'Afrique du Nord\",\n",
      "‚ÄÉ\"2\" : \"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\n",
      "‚ÄÉ\"3\" : \"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"\n",
      "\n",
      "Identify Validity : 1\n",
      "Guess Validity : <NA>\n",
      "Seek Validity : 1\n",
      "Assess Validity : 1\n",
      "Mechanical Rating : <NA>\n",
      "\n",
      "Reasoning :\n",
      "L‚Äô√©tape Identify (¬´ L‚Äô√âgypte antique ¬ª) est d√©j√† marqu√©e comme valide et identifie clairement le sujet.\n",
      "L‚Äô√©tape Seek est une question bien formul√©e (¬´ Qu‚Äôest-ce que l‚Äô√âgypte antique ? ¬ª), et elle est originale car le texte de r√©f√©rence ne d√©finit pas directement ce terme.\n",
      "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 1 des assess_cues, donc cette r√©ponse est correcte.\n",
      "\n",
      "Cependant, l‚Äô√©tape Guess (¬´ Les arch√©ologues sont des chercheurs scientifiques ¬ª) ne tente pas de r√©pondre √† la question pos√©e en Seek et n‚Äôest que vaguement li√©e au texte (elle parle des arch√©ologues, pas de l‚Äô√âgypte antique en soi).\n",
      "Comme l‚Äô√©tape Guess ne propose pas une explication pertinente en lien avec la m√™me question, la cha√Æne Identify‚ÄìGuess‚ÄìSeek n‚Äôest pas coh√©rente.\n",
      "\n",
      "L‚Äô√©chec √† l‚Äô√©tape Guess et le manque de coh√©rence invalident tout le cycle.\n",
      "\n",
      "Classification : 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The Identify step ('L'√âgypte antique') is marked as valid and correctly identifies the topic of interest. The Seek step ('Qu‚Äôest-ce que l‚Äô√âgypte antique ?') is a well-formulated question and is original, as the reference text does not directly define 'L'√âgypte antique'. The Assess step provides a valid answer ('L'√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord') that matches option 1 in the assess_cues, confirming that an answer was found. However, the Guess step ('Les arch√©ologues sont des chercheurs scientifiques') does not suggest a possible explanation related to the Seek question and is only loosely related to the reference text, focusing on archaeologists rather than the topic of 'L'√âgypte antique'. This lack of coherence between the Identify, Guess, and Seek steps means the cycle does not meet the consistency criterion. As a result, the cycle is invalid.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 2072\n",
      "Cost => 0.0067\n",
      "\n",
      "=== Processing Verbatim 8/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc6_7\n",
      "\n",
      "Text: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite. Selon la legende, Rome aurait ete fondee par Romulus qui aurai donne son nom a la ville. Au depart, ce n'etait qu'un groupe de quelques villages, puis l'Empire romain va couvrir une grande partie de l'Europe et entourer toute la mer Mediterranee. La religion romaine comptait de nombreux dieux, inspires en partie des dieux grecs.\n",
      "\n",
      "Identify: la mer Mediterranee\n",
      "\n",
      "Guess: La mer Mediterranee entoure l'Europe\n",
      "\n",
      "Seek: Ou se trouve la mer Mediterranee?\n",
      "\n",
      "Assess: La Mediterranee est une mer internationale\n",
      "\n",
      "assess Cues: {\"1\":\"Dans la l√©gende, Romulus est le premier roi de Rome\",\"2\":\"L'Antiquit√© s'est √©tendue entre 3200 avant J.C, jusqu'√† l'ann√©e 476\",\"4\":\"La M√©diterran√©e touche plusieurs continents\"}\n",
      "\n",
      "Identify Validity: 3\n",
      "\n",
      "Guess Validity: 3\n",
      "\n",
      "Seek Validity: 3\n",
      "\n",
      "Assess Validity: <NA>\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Examples\n",
      "Example 1:\n",
      "\n",
      "Id: bc4_1\n",
      "\n",
      "Text:\n",
      "A des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu'on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma.\n",
      "Le magma est responsable de l'explosion des volcans : c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan.\n",
      "On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es.\n",
      "Au-del√† de 10 000 ans, on peut dire que le volcan est √©teint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temp√©rature du magma d√©passe les 500‚ÄØ¬∞C\n",
      "\n",
      "Seek: Quelle est la temp√©rature du magma ?\n",
      "\n",
      "Assess: La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\n",
      "\n",
      "Assess Cues:\n",
      "‚ÄÉ\"1\" : \"La terre contient 7 couches\",\n",
      "‚ÄÉ\"2\" : \"La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\",\n",
      "‚ÄÉ\"3\" : \"La temp√©rature √† l'√©ruption d'un volcan est √† 700‚ÄØ¬∞C\"\n",
      "\n",
      "Identify Validity: <NA>\n",
      "Guess Validity: 2\n",
      "Seek Validity: 2\n",
      "Assess Validity: 2\n",
      "Mechanical Rating: <NA>\n",
      "\n",
      "Reasoning:\n",
      "Identify nomme ¬´ le magma ¬ª, indiquant clairement le sujet.\n",
      "Guess propose une affirmation plausible sur la temp√©rature du magma (>500‚ÄØ¬∞C).\n",
      "Seek est une question bien formul√©e qui interroge sur la temp√©rature du magma.\n",
      "Assess fournit une r√©ponse explicite (‚âà1000‚ÄØ¬∞C) qui r√©pond directement √† la question pos√©e dans Seek et correspond exactement √† l‚Äôoption 2 des assess_cues, donc la r√©ponse est consid√©r√©e comme ¬´ trouv√©e ¬ª.\n",
      "\n",
      "Les √©tapes Identify/Guess/Seek/Assess sont toutes centr√©es sur le m√™me th√®me (la temp√©rature du magma) et restent li√©es au sujet du texte de r√©f√©rence (magma/volcans), bien que la temp√©rature sp√©cifique ne soit pas donn√©e dans le texte, ce qui respecte le crit√®re d‚Äôoriginalit√©.\n",
      "\n",
      "Les drapeaux num√©riques marquent d√©j√† Guess, Seek et Assess comme valides ; Identify est √©galement valide de mani√®re ind√©pendante.\n",
      "Ainsi, chaque crit√®re de validit√© du codebook est respect√©.\n",
      "\n",
      "Classification: 1\n",
      "\n",
      "Example 2:\n",
      "Id : bc5_3\n",
      "\n",
      "Text :\n",
      "Toutankhamon √©tait un pharaon, un roi de l'√âgypte antique.\n",
      "Il est tr√®s connu aujourd‚Äôhui parce que des arch√©ologues ont retrouv√© son cercueil intact avec tous ses tr√©sors, en 1922.\n",
      "Pour les √âgyptiens, il y avait une vie apr√®s la mort, une vie √©ternelle.\n",
      "C‚Äôest pour cela que le corps devait √™tre conserv√© dans le meilleur √©tat possible : c‚Äôest ce qu‚Äôon appelle la momification.\n",
      "C‚Äôest aussi pour cela que l‚Äôon retrouve aujourd‚Äôhui de la nourriture, des armes ou des tr√©sors dans les tombeaux.\n",
      "Ces objets accompagnaient le pharaon dans sa vie apr√®s la mort.\n",
      "\n",
      "Identify : L'√âgypte antique\n",
      "\n",
      "Guess : Les arch√©ologues sont des chercheurs scientifiques\n",
      "\n",
      "Seek : Qu‚Äôest-ce que l‚Äô√âgypte antique ?\n",
      "\n",
      "Assess : L‚Äô√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord\n",
      "\n",
      "Assess Cues :\n",
      "‚ÄÉ\"1\" : \"L'√âgypte antique est une ancienne civilisation de l'Afrique du Nord\",\n",
      "‚ÄÉ\"2\" : \"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\n",
      "‚ÄÉ\"3\" : \"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"\n",
      "\n",
      "Identify Validity : 1\n",
      "Guess Validity : <NA>\n",
      "Seek Validity : 1\n",
      "Assess Validity : 1\n",
      "Mechanical Rating : <NA>\n",
      "\n",
      "Reasoning :\n",
      "L‚Äô√©tape Identify (¬´ L‚Äô√âgypte antique ¬ª) est d√©j√† marqu√©e comme valide et identifie clairement le sujet.\n",
      "L‚Äô√©tape Seek est une question bien formul√©e (¬´ Qu‚Äôest-ce que l‚Äô√âgypte antique ? ¬ª), et elle est originale car le texte de r√©f√©rence ne d√©finit pas directement ce terme.\n",
      "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 1 des assess_cues, donc cette r√©ponse est correcte.\n",
      "\n",
      "Cependant, l‚Äô√©tape Guess (¬´ Les arch√©ologues sont des chercheurs scientifiques ¬ª) ne tente pas de r√©pondre √† la question pos√©e en Seek et n‚Äôest que vaguement li√©e au texte (elle parle des arch√©ologues, pas de l‚Äô√âgypte antique en soi).\n",
      "Comme l‚Äô√©tape Guess ne propose pas une explication pertinente en lien avec la m√™me question, la cha√Æne Identify‚ÄìGuess‚ÄìSeek n‚Äôest pas coh√©rente.\n",
      "\n",
      "L‚Äô√©chec √† l‚Äô√©tape Guess et le manque de coh√©rence invalident tout le cycle.\n",
      "\n",
      "Classification : 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The Identify step ('la mer Mediterranee') indicates a topic of interest related to the reference text, which mentions the Mediterranean Sea. The Guess step ('La mer Mediterranee entoure l'Europe') suggests a possible explanation related to the Mediterranean Sea, although it is not entirely accurate. The Seek step ('Ou se trouve la mer Mediterranee?') is formulated as a question, asking for the location of the Mediterranean Sea. The Assess step ('La Mediterranee est une mer internationale') provides an answer, but it does not directly answer the Seek question about the location of the Mediterranean Sea. The Assess step does not match any of the assess_cues, which means the answer was not found in the provided options. The Identify, Guess, and Seek steps are consistent and related to the topic of the reference text. The Seek question is original as the specific location of the Mediterranean Sea is not found in the reference text. However, the Assess step fails to provide a valid answer to the Seek question, and the answer is not found in the assess_cues, which invalidates the cycle.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 2042\n",
      "Cost => 0.0069\n",
      "\n",
      "=== Processing Verbatim 9/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc6_8\n",
      "\n",
      "Text: La foret tropicale humide a un climat qui fournit beaucoup d'eau et une temperature elevee toute l'annee. Cela favorise la densite et la croissance permanente des plantes de ces forets.  En Asie, on appelle ce genre de foret 'la jungle'. Elle est parfois appelee foret dense, bien que cette expression signifie seulement que les arbres sont tres proches les uns des autres. Dans les forets tropicales, les arbres formant trois etages de hauteur differentes, sont tres serres et perdent leurs feuilles irregulierement.\n",
      "\n",
      "Identify: Lieux des forets tropicales\n",
      "\n",
      "Guess: Les forest tropicales se trouvent dans differents endroits du monde\n",
      "\n",
      "Seek: Ou se trouvent les forets tropicales du monde?\n",
      "\n",
      "Assess: les forets tropicales se trouvent en Asie,Australie en amerique du sud et en afrique\n",
      "\n",
      "assess Cues: {\"4\":\"Les for√™ts tropicales sont toujours vertes car il pleut tout le temps\",\"2\":\"La temp√©rature dans les for√™ts tropicales est entre 30¬∞ et 40¬∞C\",\"3\":\"La for√™t tropicale se trouve en Asie, en Am√©rique du Sud et en Afrique\"}\n",
      "\n",
      "Identify Validity: 3\n",
      "\n",
      "Guess Validity: <NA>\n",
      "\n",
      "Seek Validity: 3\n",
      "\n",
      "Assess Validity: 3\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Examples\n",
      "Example 1:\n",
      "\n",
      "Id: bc4_1\n",
      "\n",
      "Text:\n",
      "A des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu'on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma.\n",
      "Le magma est responsable de l'explosion des volcans : c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan.\n",
      "On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es.\n",
      "Au-del√† de 10 000 ans, on peut dire que le volcan est √©teint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temp√©rature du magma d√©passe les 500‚ÄØ¬∞C\n",
      "\n",
      "Seek: Quelle est la temp√©rature du magma ?\n",
      "\n",
      "Assess: La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\n",
      "\n",
      "Assess Cues:\n",
      "‚ÄÉ\"1\" : \"La terre contient 7 couches\",\n",
      "‚ÄÉ\"2\" : \"La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\",\n",
      "‚ÄÉ\"3\" : \"La temp√©rature √† l'√©ruption d'un volcan est √† 700‚ÄØ¬∞C\"\n",
      "\n",
      "Identify Validity: <NA>\n",
      "Guess Validity: 2\n",
      "Seek Validity: 2\n",
      "Assess Validity: 2\n",
      "Mechanical Rating: <NA>\n",
      "\n",
      "Reasoning:\n",
      "Identify nomme ¬´ le magma ¬ª, indiquant clairement le sujet.\n",
      "Guess propose une affirmation plausible sur la temp√©rature du magma (>500‚ÄØ¬∞C).\n",
      "Seek est une question bien formul√©e qui interroge sur la temp√©rature du magma.\n",
      "Assess fournit une r√©ponse explicite (‚âà1000‚ÄØ¬∞C) qui r√©pond directement √† la question pos√©e dans Seek et correspond exactement √† l‚Äôoption 2 des assess_cues, donc la r√©ponse est consid√©r√©e comme ¬´ trouv√©e ¬ª.\n",
      "\n",
      "Les √©tapes Identify/Guess/Seek/Assess sont toutes centr√©es sur le m√™me th√®me (la temp√©rature du magma) et restent li√©es au sujet du texte de r√©f√©rence (magma/volcans), bien que la temp√©rature sp√©cifique ne soit pas donn√©e dans le texte, ce qui respecte le crit√®re d‚Äôoriginalit√©.\n",
      "\n",
      "Les drapeaux num√©riques marquent d√©j√† Guess, Seek et Assess comme valides ; Identify est √©galement valide de mani√®re ind√©pendante.\n",
      "Ainsi, chaque crit√®re de validit√© du codebook est respect√©.\n",
      "\n",
      "Classification: 1\n",
      "\n",
      "Example 2:\n",
      "Id : bc5_3\n",
      "\n",
      "Text :\n",
      "Toutankhamon √©tait un pharaon, un roi de l'√âgypte antique.\n",
      "Il est tr√®s connu aujourd‚Äôhui parce que des arch√©ologues ont retrouv√© son cercueil intact avec tous ses tr√©sors, en 1922.\n",
      "Pour les √âgyptiens, il y avait une vie apr√®s la mort, une vie √©ternelle.\n",
      "C‚Äôest pour cela que le corps devait √™tre conserv√© dans le meilleur √©tat possible : c‚Äôest ce qu‚Äôon appelle la momification.\n",
      "C‚Äôest aussi pour cela que l‚Äôon retrouve aujourd‚Äôhui de la nourriture, des armes ou des tr√©sors dans les tombeaux.\n",
      "Ces objets accompagnaient le pharaon dans sa vie apr√®s la mort.\n",
      "\n",
      "Identify : L'√âgypte antique\n",
      "\n",
      "Guess : Les arch√©ologues sont des chercheurs scientifiques\n",
      "\n",
      "Seek : Qu‚Äôest-ce que l‚Äô√âgypte antique ?\n",
      "\n",
      "Assess : L‚Äô√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord\n",
      "\n",
      "Assess Cues :\n",
      "‚ÄÉ\"1\" : \"L'√âgypte antique est une ancienne civilisation de l'Afrique du Nord\",\n",
      "‚ÄÉ\"2\" : \"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\n",
      "‚ÄÉ\"3\" : \"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"\n",
      "\n",
      "Identify Validity : 1\n",
      "Guess Validity : <NA>\n",
      "Seek Validity : 1\n",
      "Assess Validity : 1\n",
      "Mechanical Rating : <NA>\n",
      "\n",
      "Reasoning :\n",
      "L‚Äô√©tape Identify (¬´ L‚Äô√âgypte antique ¬ª) est d√©j√† marqu√©e comme valide et identifie clairement le sujet.\n",
      "L‚Äô√©tape Seek est une question bien formul√©e (¬´ Qu‚Äôest-ce que l‚Äô√âgypte antique ? ¬ª), et elle est originale car le texte de r√©f√©rence ne d√©finit pas directement ce terme.\n",
      "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 1 des assess_cues, donc cette r√©ponse est correcte.\n",
      "\n",
      "Cependant, l‚Äô√©tape Guess (¬´ Les arch√©ologues sont des chercheurs scientifiques ¬ª) ne tente pas de r√©pondre √† la question pos√©e en Seek et n‚Äôest que vaguement li√©e au texte (elle parle des arch√©ologues, pas de l‚Äô√âgypte antique en soi).\n",
      "Comme l‚Äô√©tape Guess ne propose pas une explication pertinente en lien avec la m√™me question, la cha√Æne Identify‚ÄìGuess‚ÄìSeek n‚Äôest pas coh√©rente.\n",
      "\n",
      "L‚Äô√©chec √† l‚Äô√©tape Guess et le manque de coh√©rence invalident tout le cycle.\n",
      "\n",
      "Classification : 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The Identify step ('Lieux des forets tropicales') indicates a topic of interest related to the location of tropical forests. The Guess step ('Les forest tropicales se trouvent dans differents endroits du monde') suggests a possible explanation, indicating that tropical forests are found in various locations globally. The Seek step ('Ou se trouvent les forets tropicales du monde?') is formulated as a question asking for specific locations of tropical forests. The Assess step ('les forets tropicales se trouvent en Asie, Australie en amerique du sud et en afrique') provides a possible answer to the Seek question, stating specific continents where tropical forests are found. The answer in the Assess step matches option 3 in the assess_cues, confirming that an answer was found. The Identify, Guess, and Seek steps are consistent and related to the same question about the location of tropical forests. They are also related to the topic of the reference text, which discusses tropical forests. The Seek question is original as the specific locations are not mentioned in the reference text. The Assess step resolves the Seek question with an answer found in the assess_cues. All criteria for a valid cycle are met.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 2110\n",
      "Cost => 0.0072\n",
      "\n",
      "=== Processing Verbatim 10/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: mc10_3\n",
      "\n",
      "Text: Toutankhamon etait un pharaon, un roi de l'Egypte antique. Il est tres connu aujourdhui parce que des archeologues ont retrouve son cercueil intacte avec tous ses tresors, en 1922. Pour les Egyptiens, il y avait une vie apres la mort, une vie eternelle. Cest pour cela que le corps devait etre conserve dans le meilleur etat possible, cest ce quon appelle la momification. Cest aussi pour cela que lon retrouve aujourdhui de la nourriture, des armes ou des tresors dans les tombeaux. Ces objets accompagnaient le pharaon dans sa vie apres la mort.\n",
      "\n",
      "Identify: Le tombeau de Toutankhamon\n",
      "\n",
      "Guess: Ia momie etait entiere\n",
      "\n",
      "Seek: Les objets du tombeau\n",
      "\n",
      "Assess: Je ne trouve pas ma reponse\n",
      "\n",
      "assess Cues: {\"1\":\"L'Egypte antique est une ancienne civilisation de l'Afrique du Nord\",\"2\":\"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\"3\":\"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"}\n",
      "\n",
      "Identify Validity: <NA>\n",
      "\n",
      "Guess Validity: <NA>\n",
      "\n",
      "Seek Validity: <NA>\n",
      "\n",
      "Assess Validity: <NA>\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Examples\n",
      "Example 1:\n",
      "\n",
      "Id: bc4_1\n",
      "\n",
      "Text:\n",
      "A des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu'on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma.\n",
      "Le magma est responsable de l'explosion des volcans : c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan.\n",
      "On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es.\n",
      "Au-del√† de 10 000 ans, on peut dire que le volcan est √©teint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temp√©rature du magma d√©passe les 500‚ÄØ¬∞C\n",
      "\n",
      "Seek: Quelle est la temp√©rature du magma ?\n",
      "\n",
      "Assess: La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\n",
      "\n",
      "Assess Cues:\n",
      "‚ÄÉ\"1\" : \"La terre contient 7 couches\",\n",
      "‚ÄÉ\"2\" : \"La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\",\n",
      "‚ÄÉ\"3\" : \"La temp√©rature √† l'√©ruption d'un volcan est √† 700‚ÄØ¬∞C\"\n",
      "\n",
      "Identify Validity: <NA>\n",
      "Guess Validity: 2\n",
      "Seek Validity: 2\n",
      "Assess Validity: 2\n",
      "Mechanical Rating: <NA>\n",
      "\n",
      "Reasoning:\n",
      "Identify nomme ¬´ le magma ¬ª, indiquant clairement le sujet.\n",
      "Guess propose une affirmation plausible sur la temp√©rature du magma (>500‚ÄØ¬∞C).\n",
      "Seek est une question bien formul√©e qui interroge sur la temp√©rature du magma.\n",
      "Assess fournit une r√©ponse explicite (‚âà1000‚ÄØ¬∞C) qui r√©pond directement √† la question pos√©e dans Seek et correspond exactement √† l‚Äôoption 2 des assess_cues, donc la r√©ponse est consid√©r√©e comme ¬´ trouv√©e ¬ª.\n",
      "\n",
      "Les √©tapes Identify/Guess/Seek/Assess sont toutes centr√©es sur le m√™me th√®me (la temp√©rature du magma) et restent li√©es au sujet du texte de r√©f√©rence (magma/volcans), bien que la temp√©rature sp√©cifique ne soit pas donn√©e dans le texte, ce qui respecte le crit√®re d‚Äôoriginalit√©.\n",
      "\n",
      "Les drapeaux num√©riques marquent d√©j√† Guess, Seek et Assess comme valides ; Identify est √©galement valide de mani√®re ind√©pendante.\n",
      "Ainsi, chaque crit√®re de validit√© du codebook est respect√©.\n",
      "\n",
      "Classification: 1\n",
      "\n",
      "Example 2:\n",
      "Id : bc5_3\n",
      "\n",
      "Text :\n",
      "Toutankhamon √©tait un pharaon, un roi de l'√âgypte antique.\n",
      "Il est tr√®s connu aujourd‚Äôhui parce que des arch√©ologues ont retrouv√© son cercueil intact avec tous ses tr√©sors, en 1922.\n",
      "Pour les √âgyptiens, il y avait une vie apr√®s la mort, une vie √©ternelle.\n",
      "C‚Äôest pour cela que le corps devait √™tre conserv√© dans le meilleur √©tat possible : c‚Äôest ce qu‚Äôon appelle la momification.\n",
      "C‚Äôest aussi pour cela que l‚Äôon retrouve aujourd‚Äôhui de la nourriture, des armes ou des tr√©sors dans les tombeaux.\n",
      "Ces objets accompagnaient le pharaon dans sa vie apr√®s la mort.\n",
      "\n",
      "Identify : L'√âgypte antique\n",
      "\n",
      "Guess : Les arch√©ologues sont des chercheurs scientifiques\n",
      "\n",
      "Seek : Qu‚Äôest-ce que l‚Äô√âgypte antique ?\n",
      "\n",
      "Assess : L‚Äô√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord\n",
      "\n",
      "Assess Cues :\n",
      "‚ÄÉ\"1\" : \"L'√âgypte antique est une ancienne civilisation de l'Afrique du Nord\",\n",
      "‚ÄÉ\"2\" : \"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\n",
      "‚ÄÉ\"3\" : \"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"\n",
      "\n",
      "Identify Validity : 1\n",
      "Guess Validity : <NA>\n",
      "Seek Validity : 1\n",
      "Assess Validity : 1\n",
      "Mechanical Rating : <NA>\n",
      "\n",
      "Reasoning :\n",
      "L‚Äô√©tape Identify (¬´ L‚Äô√âgypte antique ¬ª) est d√©j√† marqu√©e comme valide et identifie clairement le sujet.\n",
      "L‚Äô√©tape Seek est une question bien formul√©e (¬´ Qu‚Äôest-ce que l‚Äô√âgypte antique ? ¬ª), et elle est originale car le texte de r√©f√©rence ne d√©finit pas directement ce terme.\n",
      "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 1 des assess_cues, donc cette r√©ponse est correcte.\n",
      "\n",
      "Cependant, l‚Äô√©tape Guess (¬´ Les arch√©ologues sont des chercheurs scientifiques ¬ª) ne tente pas de r√©pondre √† la question pos√©e en Seek et n‚Äôest que vaguement li√©e au texte (elle parle des arch√©ologues, pas de l‚Äô√âgypte antique en soi).\n",
      "Comme l‚Äô√©tape Guess ne propose pas une explication pertinente en lien avec la m√™me question, la cha√Æne Identify‚ÄìGuess‚ÄìSeek n‚Äôest pas coh√©rente.\n",
      "\n",
      "L‚Äô√©chec √† l‚Äô√©tape Guess et le manque de coh√©rence invalident tout le cycle.\n",
      "\n",
      "Classification : 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The Identify step ('Le tombeau de Toutankhamon') indicates a topic of interest related to the reference text, which discusses Toutankhamon and his tomb. The Guess step ('La momie etait entiere') suggests a possible explanation related to the tomb's contents, which is consistent with the topic. The Seek step ('Les objets du tombeau') is not formulated as a question, which is a requirement for validity. The Assess step ('Je ne trouve pas ma reponse') states that no answer was found, which is acceptable if the answer to the Seek question is not present in the assess_cues. However, the Seek step should have been a question, and the cycle lacks consistency because the Seek step does not directly relate to the Guess step. Additionally, the Seek question originality criterion is not met because the reference text discusses the objects found in the tomb. Therefore, the cycle is invalid due to the lack of a question in the Seek step and inconsistency between the steps.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 2077\n",
      "Cost => 0.0069\n",
      "\n",
      "=== Processing Verbatim 11/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: mc10_4\n",
      "\n",
      "Text: La tres grande majorite de lor disponible dans le monde est utilisee en bijouterie et pour sa fonction decorative : vetements, monuments ou uvres dart... Mais lor a dautres usages plus inattendus : les dentistes lutilisent pour fabriquer des couronnes dentaires par exemple. Presque tous les produits electroniques (telephone, ordinateur...) contiennent de lor en tres petite quantite car il permet de bien connecter les composantes. L'or est egalement utilise dans la fabrication des equipements spatiaux.\n",
      "\n",
      "Identify: L'or dans les equipements patiaux\n",
      "\n",
      "Guess: L'or permet de bien connecter les composants internes des equipements spatiaux\n",
      "\n",
      "Seek: Ou trouver de l 'or\n",
      "\n",
      "Assess: Je n'ai pas pu trouver ma reponse\n",
      "\n",
      "assess Cues: {\"4\":\"Les couronnes en or sont plus ch√®res\",\"2\":\"L'or permet de bien connecter les composants internes des √©quipement spatiaux\",\"3\":\"Le cuivre est un tr√®s bon conducteur pas cher\"}\n",
      "\n",
      "Identify Validity: 2\n",
      "\n",
      "Guess Validity: 2\n",
      "\n",
      "Seek Validity: <NA>\n",
      "\n",
      "Assess Validity: <NA>\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Examples\n",
      "Example 1:\n",
      "\n",
      "Id: bc4_1\n",
      "\n",
      "Text:\n",
      "A des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu'on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma.\n",
      "Le magma est responsable de l'explosion des volcans : c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan.\n",
      "On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es.\n",
      "Au-del√† de 10 000 ans, on peut dire que le volcan est √©teint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temp√©rature du magma d√©passe les 500‚ÄØ¬∞C\n",
      "\n",
      "Seek: Quelle est la temp√©rature du magma ?\n",
      "\n",
      "Assess: La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\n",
      "\n",
      "Assess Cues:\n",
      "‚ÄÉ\"1\" : \"La terre contient 7 couches\",\n",
      "‚ÄÉ\"2\" : \"La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\",\n",
      "‚ÄÉ\"3\" : \"La temp√©rature √† l'√©ruption d'un volcan est √† 700‚ÄØ¬∞C\"\n",
      "\n",
      "Identify Validity: <NA>\n",
      "Guess Validity: 2\n",
      "Seek Validity: 2\n",
      "Assess Validity: 2\n",
      "Mechanical Rating: <NA>\n",
      "\n",
      "Reasoning:\n",
      "Identify nomme ¬´ le magma ¬ª, indiquant clairement le sujet.\n",
      "Guess propose une affirmation plausible sur la temp√©rature du magma (>500‚ÄØ¬∞C).\n",
      "Seek est une question bien formul√©e qui interroge sur la temp√©rature du magma.\n",
      "Assess fournit une r√©ponse explicite (‚âà1000‚ÄØ¬∞C) qui r√©pond directement √† la question pos√©e dans Seek et correspond exactement √† l‚Äôoption 2 des assess_cues, donc la r√©ponse est consid√©r√©e comme ¬´ trouv√©e ¬ª.\n",
      "\n",
      "Les √©tapes Identify/Guess/Seek/Assess sont toutes centr√©es sur le m√™me th√®me (la temp√©rature du magma) et restent li√©es au sujet du texte de r√©f√©rence (magma/volcans), bien que la temp√©rature sp√©cifique ne soit pas donn√©e dans le texte, ce qui respecte le crit√®re d‚Äôoriginalit√©.\n",
      "\n",
      "Les drapeaux num√©riques marquent d√©j√† Guess, Seek et Assess comme valides ; Identify est √©galement valide de mani√®re ind√©pendante.\n",
      "Ainsi, chaque crit√®re de validit√© du codebook est respect√©.\n",
      "\n",
      "Classification: 1\n",
      "\n",
      "Example 2:\n",
      "Id : bc5_3\n",
      "\n",
      "Text :\n",
      "Toutankhamon √©tait un pharaon, un roi de l'√âgypte antique.\n",
      "Il est tr√®s connu aujourd‚Äôhui parce que des arch√©ologues ont retrouv√© son cercueil intact avec tous ses tr√©sors, en 1922.\n",
      "Pour les √âgyptiens, il y avait une vie apr√®s la mort, une vie √©ternelle.\n",
      "C‚Äôest pour cela que le corps devait √™tre conserv√© dans le meilleur √©tat possible : c‚Äôest ce qu‚Äôon appelle la momification.\n",
      "C‚Äôest aussi pour cela que l‚Äôon retrouve aujourd‚Äôhui de la nourriture, des armes ou des tr√©sors dans les tombeaux.\n",
      "Ces objets accompagnaient le pharaon dans sa vie apr√®s la mort.\n",
      "\n",
      "Identify : L'√âgypte antique\n",
      "\n",
      "Guess : Les arch√©ologues sont des chercheurs scientifiques\n",
      "\n",
      "Seek : Qu‚Äôest-ce que l‚Äô√âgypte antique ?\n",
      "\n",
      "Assess : L‚Äô√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord\n",
      "\n",
      "Assess Cues :\n",
      "‚ÄÉ\"1\" : \"L'√âgypte antique est une ancienne civilisation de l'Afrique du Nord\",\n",
      "‚ÄÉ\"2\" : \"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\n",
      "‚ÄÉ\"3\" : \"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"\n",
      "\n",
      "Identify Validity : 1\n",
      "Guess Validity : <NA>\n",
      "Seek Validity : 1\n",
      "Assess Validity : 1\n",
      "Mechanical Rating : <NA>\n",
      "\n",
      "Reasoning :\n",
      "L‚Äô√©tape Identify (¬´ L‚Äô√âgypte antique ¬ª) est d√©j√† marqu√©e comme valide et identifie clairement le sujet.\n",
      "L‚Äô√©tape Seek est une question bien formul√©e (¬´ Qu‚Äôest-ce que l‚Äô√âgypte antique ? ¬ª), et elle est originale car le texte de r√©f√©rence ne d√©finit pas directement ce terme.\n",
      "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 1 des assess_cues, donc cette r√©ponse est correcte.\n",
      "\n",
      "Cependant, l‚Äô√©tape Guess (¬´ Les arch√©ologues sont des chercheurs scientifiques ¬ª) ne tente pas de r√©pondre √† la question pos√©e en Seek et n‚Äôest que vaguement li√©e au texte (elle parle des arch√©ologues, pas de l‚Äô√âgypte antique en soi).\n",
      "Comme l‚Äô√©tape Guess ne propose pas une explication pertinente en lien avec la m√™me question, la cha√Æne Identify‚ÄìGuess‚ÄìSeek n‚Äôest pas coh√©rente.\n",
      "\n",
      "L‚Äô√©chec √† l‚Äô√©tape Guess et le manque de coh√©rence invalident tout le cycle.\n",
      "\n",
      "Classification : 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The Identify step ('L'or dans les equipements spatiaux') indicates a topic of interest related to the reference text, which mentions the use of gold in space equipment. The Guess step ('L'or permet de bien connecter les composants internes des equipements spatiaux') suggests a possible explanation for why gold is used in space equipment, which is consistent with the topic. The Seek step ('Ou trouver de l'or') is formulated as a question, but it is not directly related to the topic of gold in space equipment; it is more about the general location of gold, which is not consistent with the Identify and Guess steps. The Assess step states that no answer was found, which is valid if the answer to the Seek question is not present in the assess_cues. The assess_cues do not contain an answer to the Seek question ('Ou trouver de l'or'), so the Assess step is valid. However, the cycle lacks consistency because the Seek step is not related to the same question as the Identify and Guess steps. Additionally, the Seek question's originality is compromised because the reference text discusses where gold is used, which could be interpreted as where it is found. Therefore, the cycle is invalid due to inconsistency and lack of originality in the Seek question.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 2083\n",
      "Cost => 0.0073\n",
      "\n",
      "=== Processing Verbatim 12/12 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: mc12_8\n",
      "\n",
      "Text: La foret tropicale humide a un climat qui fournit beaucoup d'eau et une temperature elevee toute l'annee. Cela favorise la densite et la croissance permanente des plantes de ces forets.  En Asie, on appelle ce genre de foret 'la jungle'. Elle est parfois appelee foret dense, bien que cette expression signifie seulement que les arbres sont tres proches les uns des autres. Dans les forets tropicales, les arbres formant trois etages de hauteur differentes, sont tres serres et perdent leurs feuilles irregulierement.\n",
      "\n",
      "Identify: lieux des forets tropicales\n",
      "\n",
      "Guess: les forets tropical se trouvent dans different endroits du monde\n",
      "\n",
      "Seek: ou se trouvent les forets tropical du monde ?\n",
      "\n",
      "Assess: les foets se trouvent en asie australie enamerique central amerique du sud et en afrique\n",
      "\n",
      "assess Cues: {\"4\":\"Les for√™ts tropicales sont toujours vertes car il pleut tout le temps\",\"2\":\"La temp√©rature dans les for√™ts tropicales est entre 30¬∞ et 40¬∞C\",\"3\":\"La for√™t tropicale se trouve en Asie, en Am√©rique du Sud et en Afrique\"}\n",
      "\n",
      "Identify Validity: 3\n",
      "\n",
      "Guess Validity: <NA>\n",
      "\n",
      "Seek Validity: 3\n",
      "\n",
      "Assess Validity: 3\n",
      "\n",
      "mechanical Rating: <NA>\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Examples\n",
      "Example 1:\n",
      "\n",
      "Id: bc4_1\n",
      "\n",
      "Text:\n",
      "A des dizaines de kilom√®tres sous nos pieds, la terre contient une couche qu'on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c‚Äôest ce qu'on appelle le magma.\n",
      "Le magma est responsable de l'explosion des volcans : c'est ce que les scientifiques appellent aussi une √©ruption d'un volcan.\n",
      "On dit qu‚Äôun volcan est endormi s‚Äôil n‚Äôy a eu aucune √©ruption dans les 10 000 derni√®res ann√©es.\n",
      "Au-del√† de 10 000 ans, on peut dire que le volcan est √©teint.\n",
      "\n",
      "Identify: le magma\n",
      "\n",
      "Guess: La temp√©rature du magma d√©passe les 500‚ÄØ¬∞C\n",
      "\n",
      "Seek: Quelle est la temp√©rature du magma ?\n",
      "\n",
      "Assess: La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\n",
      "\n",
      "Assess Cues:\n",
      "‚ÄÉ\"1\" : \"La terre contient 7 couches\",\n",
      "‚ÄÉ\"2\" : \"La temp√©rature du magma atteint les 1000‚ÄØ¬∞C\",\n",
      "‚ÄÉ\"3\" : \"La temp√©rature √† l'√©ruption d'un volcan est √† 700‚ÄØ¬∞C\"\n",
      "\n",
      "Identify Validity: <NA>\n",
      "Guess Validity: 2\n",
      "Seek Validity: 2\n",
      "Assess Validity: 2\n",
      "Mechanical Rating: <NA>\n",
      "\n",
      "Reasoning:\n",
      "Identify nomme ¬´ le magma ¬ª, indiquant clairement le sujet.\n",
      "Guess propose une affirmation plausible sur la temp√©rature du magma (>500‚ÄØ¬∞C).\n",
      "Seek est une question bien formul√©e qui interroge sur la temp√©rature du magma.\n",
      "Assess fournit une r√©ponse explicite (‚âà1000‚ÄØ¬∞C) qui r√©pond directement √† la question pos√©e dans Seek et correspond exactement √† l‚Äôoption 2 des assess_cues, donc la r√©ponse est consid√©r√©e comme ¬´ trouv√©e ¬ª.\n",
      "\n",
      "Les √©tapes Identify/Guess/Seek/Assess sont toutes centr√©es sur le m√™me th√®me (la temp√©rature du magma) et restent li√©es au sujet du texte de r√©f√©rence (magma/volcans), bien que la temp√©rature sp√©cifique ne soit pas donn√©e dans le texte, ce qui respecte le crit√®re d‚Äôoriginalit√©.\n",
      "\n",
      "Les drapeaux num√©riques marquent d√©j√† Guess, Seek et Assess comme valides ; Identify est √©galement valide de mani√®re ind√©pendante.\n",
      "Ainsi, chaque crit√®re de validit√© du codebook est respect√©.\n",
      "\n",
      "Classification: 1\n",
      "\n",
      "Example 2:\n",
      "Id : bc5_3\n",
      "\n",
      "Text :\n",
      "Toutankhamon √©tait un pharaon, un roi de l'√âgypte antique.\n",
      "Il est tr√®s connu aujourd‚Äôhui parce que des arch√©ologues ont retrouv√© son cercueil intact avec tous ses tr√©sors, en 1922.\n",
      "Pour les √âgyptiens, il y avait une vie apr√®s la mort, une vie √©ternelle.\n",
      "C‚Äôest pour cela que le corps devait √™tre conserv√© dans le meilleur √©tat possible : c‚Äôest ce qu‚Äôon appelle la momification.\n",
      "C‚Äôest aussi pour cela que l‚Äôon retrouve aujourd‚Äôhui de la nourriture, des armes ou des tr√©sors dans les tombeaux.\n",
      "Ces objets accompagnaient le pharaon dans sa vie apr√®s la mort.\n",
      "\n",
      "Identify : L'√âgypte antique\n",
      "\n",
      "Guess : Les arch√©ologues sont des chercheurs scientifiques\n",
      "\n",
      "Seek : Qu‚Äôest-ce que l‚Äô√âgypte antique ?\n",
      "\n",
      "Assess : L‚Äô√âgypte antique est une ancienne civilisation de l‚ÄôAfrique du Nord\n",
      "\n",
      "Assess Cues :\n",
      "‚ÄÉ\"1\" : \"L'√âgypte antique est une ancienne civilisation de l'Afrique du Nord\",\n",
      "‚ÄÉ\"2\" : \"La momification revient √† conserver le corps dans une bo√Æte et le mettre dans une pi√®ce sans lumi√®re et sans air\",\n",
      "‚ÄÉ\"3\" : \"Le premier Pharaon a v√©cu √† 3000 av. J.-C.\"\n",
      "\n",
      "Identify Validity : 1\n",
      "Guess Validity : <NA>\n",
      "Seek Validity : 1\n",
      "Assess Validity : 1\n",
      "Mechanical Rating : <NA>\n",
      "\n",
      "Reasoning :\n",
      "L‚Äô√©tape Identify (¬´ L‚Äô√âgypte antique ¬ª) est d√©j√† marqu√©e comme valide et identifie clairement le sujet.\n",
      "L‚Äô√©tape Seek est une question bien formul√©e (¬´ Qu‚Äôest-ce que l‚Äô√âgypte antique ? ¬ª), et elle est originale car le texte de r√©f√©rence ne d√©finit pas directement ce terme.\n",
      "L‚Äô√©tape Assess fournit une r√©ponse explicite qui correspond √† l‚Äôoption 1 des assess_cues, donc cette r√©ponse est correcte.\n",
      "\n",
      "Cependant, l‚Äô√©tape Guess (¬´ Les arch√©ologues sont des chercheurs scientifiques ¬ª) ne tente pas de r√©pondre √† la question pos√©e en Seek et n‚Äôest que vaguement li√©e au texte (elle parle des arch√©ologues, pas de l‚Äô√âgypte antique en soi).\n",
      "Comme l‚Äô√©tape Guess ne propose pas une explication pertinente en lien avec la m√™me question, la cha√Æne Identify‚ÄìGuess‚ÄìSeek n‚Äôest pas coh√©rente.\n",
      "\n",
      "L‚Äô√©chec √† l‚Äô√©tape Guess et le manque de coh√©rence invalident tout le cycle.\n",
      "\n",
      "Classification : 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The Identify step ('lieux des forets tropicales') indicates a topic of interest related to the location of tropical forests. The Guess step ('les forets tropical se trouvent dans different endroits du monde') suggests a possible explanation, indicating that tropical forests are found in various locations globally. The Seek step ('ou se trouvent les forets tropical du monde ?') is formulated as a question asking about the locations of tropical forests worldwide. The Assess step ('les foets se trouvent en asie australie enamerique central amerique du sud et en afrique') provides a possible answer, listing specific regions where tropical forests are found. The answer in the Assess step corresponds to the information in assess_cues option 3 ('La for√™t tropicale se trouve en Asie, en Am√©rique du Sud et en Afrique'), indicating that an answer was found. The Identify, Guess, and Seek steps are consistent and related to the same question about the locations of tropical forests, and they are linked to the topic of the reference text, which discusses tropical forests. The Seek question's answer is not found in the reference text, ensuring originality. The Assess step resolves the Seek question with a valid answer found in the assess_cues. All criteria for a valid cycle are met.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 2123\n",
      "Cost => 0.0074\n",
      "\n",
      "Columns in detailed_results_df:\n",
      "['sample_id', 'split', 'verbatim', 'iteration', 'Rater_Oli', 'Rater_Gaia', 'Rater_Chloe', 'ModelPrediction', 'Reasoning', 'run', 'prompt_name', 'use_validation_set']\n"
     ]
    }
   ],
   "source": [
    "# 9) Run scenarios and get results\n",
    "\n",
    "annotation_columns = ['Rater_Oli', 'Rater_Gaia', 'Rater_Chloe']\n",
    "labels = [0,1]\n",
    "\n",
    "# Filter labeled data (drop rows with NaN in any annotation column)\n",
    "labeled_data = data.dropna(subset=annotation_columns)\n",
    "unlabeled_data = data[~data.index.isin(labeled_data.index)]\n",
    "\n",
    "n_runs = 1  # Number of runs per scenario\n",
    "verbose = True  # Whether to print verbose output\n",
    "\n",
    "# Run the scenarios - this only runs the LLM and saves all the generated labels\n",
    "complex_case_for_metrics = run_scenarios(\n",
    "    scenarios=scenarios,\n",
    "    data=labeled_data,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    n_runs=n_runs,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Saving / Re-Loading the Results\n",
    "\n",
    "This step provides an option to save the classification results to a file for future reference or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possibility to save the results\n",
    "\n",
    "# Save the annotated results to a CSV file\n",
    "complex_case_for_metrics.to_csv(\"data/complex_user_case/outputs/complex_case_for_metrics.csv\", sep=\";\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, load the annotated results from the CSV file if needed\n",
    "\n",
    "complex_case_for_metrics = pd.read_csv(\n",
    "    \"data/complex_user_case/outputs/complex_case_for_metrics.csv\",\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8-sig\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance Against Human Annotations\n",
    "\n",
    "To determine whether the model's classification is reliable and can be used to annotate the rest of the unlabeled dataset,  \n",
    "it is recommended to evaluate its alignment with human annotations.  \n",
    "If the alignment is sufficiently high, you may choose to rely on the model-generated labels for the remaining data.\n",
    "\n",
    "We propose **four types of analysis**, depending on your goals:\n",
    "\n",
    "- **If you want to measure agreement between annotators**:  \n",
    "  Use **Cohen's Kappa**, a simple and widely used metric for inter-rater agreement.\n",
    "\n",
    "- **If you need detailed per-class performance metrics** (e.g., recall, true positives, false positives):  \n",
    "  Use **Classification Metrics**. This method gives a descriptive breakdown of model performance by class.\n",
    "\n",
    "- **If you have multiple manual annotations and want a more robust estimate**:  \n",
    "  Use **Krippendorff's Alpha**. This method provides:\n",
    "  - A confidence interval for the agreement, computed via bootstrapping\n",
    "  - An estimate of the risk that the true alpha value lies outside this interval\n",
    "\n",
    "- **If you have multiple annotation columns (‚â• 3)** and want to assess whether the model can \"replace\" or **outperform individual annotators**,  \n",
    "  and you can afford to annotate 50‚Äì100 entries:  \n",
    "  Use the **Alt-Test**. This stricter test compares the model to each annotator using a **leave-one-out** approach.\n",
    "\n",
    "Among the available methods, **Krippendorff‚Äôs Alpha** and the **Alt-Test** are the ones we consider more **rigorous and robust**.\n",
    "\n",
    "> **Note 1**: The final decision on whether the model's performance is ‚Äúgood enough‚Äù depends on your research domain,  \n",
    "> acceptable error tolerance, and practical factors such as annotation cost and time. It can be totally valid to accept the model based solely on its Cohen‚Äôs kappa score,\n",
    " if it is approximately equivalent to human inter-rater agreement.\n",
    "\n",
    "> **Note 2**: If the agreement between human annotators is low, the issue likely lies in the codebook (e.g., unclear guidelines) or the annotation task itself.\n",
    "> In such cases, it‚Äôs unrealistic to expect the LLM to achieve high performance if humans themselves struggle to agree on the correct labels.\n",
    "\n",
    "> **Note 3**: If you're not satisfied with the model‚Äôs performance, you can go back and **adjust the scenario** (this may include updating the codebook, adding examples, using another model...)  \n",
    "> ‚ö†Ô∏è However, if you do this **multiple times**, it is strongly recommended to use a **validation set** to avoid overfitting to your annotated subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's Kappa\n",
    "\n",
    "This analysis provides:\n",
    "\n",
    "- **Mean agreement between the LLM and all human annotators** (when multiple annotators are available)\n",
    "- **Mean agreement among human annotators** (when multiple annotators are available)\n",
    "- **Individual agreement scores** for all pairwise comparisons\n",
    "\n",
    "#### Weighting Options\n",
    "\n",
    "You can set kappa_weights to different values. Use:\n",
    "\n",
    "- **unweighted (remove the parameter)**:  \n",
    "  Treats all disagreements equally.  \n",
    "  _Example: Disagreeing between `0` and `1` is treated the same as between `0` and `2`._\n",
    "\n",
    "- **linear**:  \n",
    "  Weights disagreements by their distance.  \n",
    "  _Example: A disagreement between `0` and `2` is considered twice as bad as between `0` and `1`._\n",
    "\n",
    "- **quadratic**:  \n",
    "  Weights disagreements by the square of their distance.  \n",
    "  _Example: A disagreement between `0` and `2` is considered four times as bad as between `0` and `1`._\n",
    "\n",
    "> **Note **: If `n_runs` > 1, the reported metrics will include **variability across runs**, allowing you to assess the **consistency** of LLM performance.  \n",
    "> Lower variance indicates more stable and reliable model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Columns in detailed_results_df (in compute_kappa_metrics) ===\n",
      "['sample_id', 'split', 'verbatim', 'iteration', 'Rater_Oli', 'Rater_Gaia', 'Rater_Chloe', 'ModelPrediction', 'Reasoning', 'run', 'prompt_name', 'use_validation_set']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>iteration</th>\n",
       "      <th>n_runs</th>\n",
       "      <th>use_validation_set</th>\n",
       "      <th>N_train</th>\n",
       "      <th>N_val</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>kappa_train</th>\n",
       "      <th>mean_llm_human_agreement</th>\n",
       "      <th>mean_human_human_agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>few_shot</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zero_shot</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_name  iteration  n_runs  use_validation_set  N_train  N_val  \\\n",
       "0    few_shot          1       1               False       12      0   \n",
       "1   zero_shot          1       1               False       12      0   \n",
       "\n",
       "   accuracy_train  kappa_train  mean_llm_human_agreement  \\\n",
       "0            0.75          0.5                       0.5   \n",
       "1            1.00          1.0                       1.0   \n",
       "\n",
       "   mean_human_human_agreement  \n",
       "0                         1.0  \n",
       "1                         1.0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10) Compute metrics from the detailed results\n",
    "# First, compute kappa metrics\n",
    "kappa_df, detailed_kappa_metrics = compute_kappa_metrics(\n",
    "    detailed_results_df=complex_case_for_metrics,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "kappa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Detailed Kappa Metrics ===\n",
      "\n",
      "Scenario: few_shot_iteration_1\n",
      "\n",
      "LLM vs Human Annotators:\n",
      "  Human_Annotator  Cohens_Kappa\n",
      "0       Rater_Oli      0.623027\n",
      "1      Rater_Gaia      0.623027\n",
      "2     Rater_Chloe      0.610178\n",
      "\n",
      "Human vs Human Annotators:\n",
      "  Annotator_1  Annotator_2  Cohens_Kappa\n",
      "0   Rater_Oli   Rater_Gaia      0.946429\n",
      "1   Rater_Oli  Rater_Chloe      0.840989\n",
      "2  Rater_Gaia  Rater_Chloe      0.840989\n",
      "\n",
      "Scenario: zero_shot_iteration_1\n",
      "\n",
      "LLM vs Human Annotators:\n",
      "  Human_Annotator  Cohens_Kappa\n",
      "0       Rater_Oli      0.647887\n",
      "1      Rater_Gaia      0.647887\n",
      "2     Rater_Chloe      0.639510\n",
      "\n",
      "Human vs Human Annotators:\n",
      "  Annotator_1  Annotator_2  Cohens_Kappa\n",
      "0   Rater_Oli   Rater_Gaia      0.946429\n",
      "1   Rater_Oli  Rater_Chloe      0.840989\n",
      "2  Rater_Gaia  Rater_Chloe      0.840989\n"
     ]
    }
   ],
   "source": [
    "# Additional details about the kappa metrics\n",
    "\n",
    "print(\"\\n=== Detailed Kappa Metrics ===\")\n",
    "if detailed_kappa_metrics:\n",
    "    for scenario_key, metrics in detailed_kappa_metrics.items():\n",
    "        print(f\"\\nScenario: {scenario_key}\")\n",
    "        \n",
    "        print(\"\\nLLM vs Human Annotators:\")\n",
    "        print(metrics['llm_vs_human_df'])\n",
    "        \n",
    "        print(\"\\nHuman vs Human Annotators:\")\n",
    "        print(metrics['human_vs_human_df'])\n",
    "else:\n",
    "    print(\"No detailed kappa metrics available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics (Per-Class Analysis)\n",
    "\n",
    "Analyze detailed classification metrics for each class, focusing on **recall** and **confusion matrix elements**.\n",
    "\n",
    "This analysis uses the **majority vote from human annotations** as the ground truth and provides:\n",
    "\n",
    "#### Global Metrics (prefix: `global_*`)\n",
    "\n",
    "- `global_accuracy_train`: Overall accuracy on training data\n",
    "- `global_recall_train`: Macro recall on training data\n",
    "- `global_error_rate_train`: 1 - accuracy\n",
    "\n",
    "(And similarly for validation data with suffix `_val`, if `use_validation_set = True`)\n",
    "\n",
    "#### Per-Class Metrics (prefix: `class_<label>_*_train`)\n",
    "\n",
    "For each class label (e.g., `0`, `1`), the following are computed:\n",
    "\n",
    "- `class_<label>_recall_train`: Proportion of actual class instances correctly identified (True Positives)\n",
    "- `class_<label>_error_rate_train`: Proportion of actual class instances incorrectly classified (Miss Rate)\n",
    "- `class_<label>_correct_count_train`: Number of correctly predicted instances\n",
    "- `class_<label>_missed_count_train`: Number of missed instances (False Negatives)\n",
    "- `class_<label>_false_positives_train`: Number of incorrect predictions *as* this class (False Positives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Columns in detailed_results_df (in compute_classification_metrics_from_results) ===\n",
      "['sample_id', 'split', 'verbatim', 'iteration', 'Rater_Oli', 'Rater_Gaia', 'Rater_Chloe', 'ModelPrediction', 'Reasoning', 'run', 'prompt_name', 'use_validation_set']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>iteration</th>\n",
       "      <th>n_runs</th>\n",
       "      <th>use_validation_set</th>\n",
       "      <th>N_train</th>\n",
       "      <th>N_val</th>\n",
       "      <th>global_accuracy_train</th>\n",
       "      <th>global_recall_train</th>\n",
       "      <th>global_error_rate_train</th>\n",
       "      <th>class_0_recall_train</th>\n",
       "      <th>class_0_error_rate_train</th>\n",
       "      <th>class_0_correct_count_train</th>\n",
       "      <th>class_0_missed_count_train</th>\n",
       "      <th>class_0_false_positives_train</th>\n",
       "      <th>class_1_recall_train</th>\n",
       "      <th>class_1_error_rate_train</th>\n",
       "      <th>class_1_correct_count_train</th>\n",
       "      <th>class_1_missed_count_train</th>\n",
       "      <th>class_1_false_positives_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>few_shot</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.709402</td>\n",
       "      <td>0.290598</td>\n",
       "      <td>83</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>102</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zero_shot</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>0.835556</td>\n",
       "      <td>0.839031</td>\n",
       "      <td>0.164444</td>\n",
       "      <td>0.752137</td>\n",
       "      <td>0.247863</td>\n",
       "      <td>88</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_name  iteration  n_runs  use_validation_set  N_train  N_val  \\\n",
       "0    few_shot          1       3               False      225      0   \n",
       "1   zero_shot          1       3               False      225      0   \n",
       "\n",
       "   global_accuracy_train  global_recall_train  global_error_rate_train  \\\n",
       "0               0.822222             0.826923                 0.177778   \n",
       "1               0.835556             0.839031                 0.164444   \n",
       "\n",
       "   class_0_recall_train  class_0_error_rate_train  \\\n",
       "0              0.709402                  0.290598   \n",
       "1              0.752137                  0.247863   \n",
       "\n",
       "   class_0_correct_count_train  class_0_missed_count_train  \\\n",
       "0                           83                          34   \n",
       "1                           88                          29   \n",
       "\n",
       "   class_0_false_positives_train  class_1_recall_train  \\\n",
       "0                              6              0.944444   \n",
       "1                              8              0.925926   \n",
       "\n",
       "   class_1_error_rate_train  class_1_correct_count_train  \\\n",
       "0                  0.055556                          102   \n",
       "1                  0.074074                          100   \n",
       "\n",
       "   class_1_missed_count_train  class_1_false_positives_train  \n",
       "0                           6                             34  \n",
       "1                           8                             29  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute classification metrics\n",
    "classification_df = compute_classification_metrics_from_results(\n",
    "    detailed_results_df=complex_case_for_metrics,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)    # show all columns\n",
    "classification_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krippendorff‚Äôs‚ÄØŒ± Non‚ÄëInferiority Test  \n",
    "*(Requires ‚â•‚ÄØ3 human annotation columns)*\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "This test evaluates whether the model's annotations are **statistically non-inferior** to fully human-annotated data.  \n",
    "If successful, this means the model can probably take over the annotation of the remaining, unlabeled data.\n",
    "\n",
    "#### How the Test Works\n",
    "\n",
    "- **Human reliability (`Œ±_human`)**  \n",
    "  Krippendorff‚Äôs Œ± is computed across all *n* human annotators.\n",
    "\n",
    "- **Model reliability (`Œ±_model`)**  \n",
    "  For each possible panel of (*n‚ÄØ‚àí‚ÄØ1*) humans + the model, compute Krippendorff‚Äôs Œ±.  \n",
    "  The final value is the **mean** Œ± across all such combinations.\n",
    "\n",
    "- **Effect size (Œî)**  \n",
    "  \\[\n",
    "  \\Delta = \\alpha_{\\text{model}} - \\alpha_{\\text{human}}\n",
    "  \\]  \n",
    "  - Positive Œî ‚Üí Model improves reliability  \n",
    "  - Negative Œî ‚Üí Performance drop\n",
    "\n",
    "- **Uncertainty estimation via bootstrapping**  \n",
    "  The dataset is resampled thousands of times (e.g., 2,000) to recompute Œî.  \n",
    "  A **90‚ÄØ% confidence interval (CI)** (configurable) is constructed to show where the true Œî likely lies.\n",
    "\n",
    "\n",
    "- **Non‚ÄëInferiority Margin (`Œ¥`)**\n",
    "    You define `Œ¥` (commonly set to **‚àí0.05**) as the **largest acceptable drop** in Œ± when using the model.\n",
    "\n",
    "- **Decision rule**:  \n",
    "  If the entire confidence interval lies **above `Œ¥`**, the model is declared **non-inferior**.  \n",
    "  With a 90‚ÄØ% CI, this reflects a **5‚ÄØ% one-sided risk** of wrongly approving a model worse than the lower born of the CI.\n",
    "\n",
    "#### Interpretation Cheatsheet\n",
    "\n",
    "| CI Position                 | What It Means for Deployment                                               |\n",
    "|----------------------------|-----------------------------------------------------------------------------|\n",
    "| CI fully above **0**       | ‚úÖ Model is **statistically superior** to humans  |\n",
    "| CI fully above **Œ¥**, but crosses 0 | ‚úÖ Model is **non-inferior** (small, acceptable loss)     |\n",
    "| CI touches or falls below **Œ¥** | ‚ùå Model is possibly worse than the humans by the Œ¥ margin|\n",
    "\n",
    "#### Why ‚Äú5‚ÄØ% Risk‚Äù?\n",
    "\n",
    "- A 90‚ÄØ% CI corresponds to a **one-sided Œ± = 0.05** non-inferiority test.\n",
    "- This 5‚ÄØ% risk applies to the **margin Œ¥**, not to zero.\n",
    "- If the CI just touches Œ¥ ‚Üí ‚âà‚ÄØ5‚ÄØ% chance that the **true Œî ‚â§ Œ¥**\n",
    "- If the CI is well above Œ¥ ‚Üí Risk that **true Œî ‚â§ 0** is even lower than 5‚ÄØ%\n",
    "\n",
    "#### Settings and Their Effects\n",
    "\n",
    "| Setting                        | Increase ‚Üí                          | Decrease ‚Üí                          |\n",
    "|-------------------------------|-------------------------------------|-------------------------------------|\n",
    "| **Confidence level** (e.g. 90‚ÄØ% ‚Üí 95‚ÄØ%) | ‚Äì CI gets **wider**<br>‚Äì Test becomes **stricter**<br>‚Äì Type I error drops (5‚ÄØ% ‚Üí 2.5‚ÄØ%) | ‚Äì CI gets **narrower**<br>‚Äì Easier to declare non-inferiority<br>‚Äì Higher false positive risk |\n",
    "| **Non-inferiority margin `Œ¥`** (e.g. ‚àí0.05 ‚Üí ‚àí0.10) | ‚Äì You tolerate a **larger drop**<br>‚Äì Easier for model to pass<br>‚Äì Lower guaranteed quality | ‚Äì You demand **closer match to humans**<br>‚Äì Harder to pass<br>‚Äì Stronger quality guarantee |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Non-inferiority Test: few_shot_iteration_1 ===\n",
      "Human trios Œ±: 0.8761 ¬± 0.0000\n",
      "Model trios Œ±: 0.7037 ¬± 0.0085\n",
      "Œî = model ‚àí human = -0.1724 ¬± 0.0085\n",
      "90% CI: [-0.2720, -0.0829]\n",
      "Non-inferiority demonstrated in 0/3 runs\n",
      "‚ùå Non-inferiority NOT demonstrated in any run (margin = -0.05)\n",
      "\n",
      "=== Non-inferiority Test: zero_shot_iteration_1 ===\n",
      "Human trios Œ±: 0.8761 ¬± 0.0000\n",
      "Model trios Œ±: 0.7220 ¬± 0.0223\n",
      "Œî = model ‚àí human = -0.1540 ¬± 0.0223\n",
      "90% CI: [-0.2515, -0.0661]\n",
      "Non-inferiority demonstrated in 0/3 runs\n",
      "‚ùå Non-inferiority NOT demonstrated in any run (margin = -0.05)\n"
     ]
    }
   ],
   "source": [
    "# Run the non-inferiority test\n",
    "non_inferiority_results = compute_krippendorff_non_inferiority(\n",
    "    detailed_results_df=complex_case_for_metrics,\n",
    "    annotation_columns=annotation_columns,\n",
    "    model_column=\"ModelPrediction\",\n",
    "    level_of_measurement='ordinal',\n",
    "    non_inferiority_margin=-0.05,\n",
    "    n_bootstrap=2000, \n",
    "    confidence_level=90.0,\n",
    "    random_seed=42, \n",
    "    verbose=False   \n",
    ")\n",
    "\n",
    "# Print results in a formatted way\n",
    "print_non_inferiority_results(non_inferiority_results, show_per_run=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Annotator Test (ALT-Test)\n",
    "\n",
    "The **ALT-Test** evaluates whether an LLM can perform **as well as or better than human annotators**, based on a **leave-one-human-out** approach.\n",
    "\n",
    "This method requires **at least 3 human annotation columns**.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "- The LLM is compared against **each human annotator**, one at a time.\n",
    "- For each comparison:\n",
    "  - One human is **excluded**\n",
    "  - The model‚Äôs predictions are evaluated **against the remaining human annotations**\n",
    "  - This simulates a realistic setting where the LLM replaces a single annotator and is judged by agreement with the rest\n",
    "\n",
    "#### Key Metrics in Output\n",
    "\n",
    "- **`winning_rate_train`**: Proportion of annotators for which the LLM performs as well or better (after adjusting for Œµ)\n",
    "- **`passed_alt_test_train`**: `True` if the LLM passes the test (i.e., `winning_rate ‚â• 0.5`)\n",
    "- **`avg_adv_prob_train`**: Average advantage probability, how likely the model is better across comparisons\n",
    "- **`p_values_train`**: List of p-values for each comparison\n",
    "\n",
    "#### Interpreting `Œµ` (Epsilon)\n",
    "\n",
    "- `Œµ` accounts for the **cost/effort/time trade-off** between using an LLM and a human annotator.\n",
    "- Higher `Œµ` gives the model more leeway, useful when **human annotations are costly**.\n",
    "- Recommendations from the original paper:\n",
    "  - `Œµ = 0.2` ‚Üí when humans are **experts**\n",
    "  - `Œµ = 0.1` ‚Üí when humans are **crowdworkers**\n",
    "\n",
    "> If `winning_rate ‚â• 0.5`, the LLM is considered **statistically competitive with human annotators** for this dataset and scenario (the LLM is \"better\" than half the humans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Columns in detailed_results_df (in run_alt_test_on_results) ===\n",
      "['sample_id', 'split', 'verbatim', 'iteration', 'Rater_Oli', 'Rater_Gaia', 'Rater_Chloe', 'ModelPrediction', 'Reasoning', 'run', 'prompt_name', 'use_validation_set']\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 75 valid labels\n",
      "  Rater_Oli: 75 valid labels\n",
      "  Rater_Gaia: 75 valid labels\n",
      "  Rater_Chloe: 75 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_Gaia: int64\n",
      "  Rater_Chloe: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_Gaia type after conversion: <class 'numpy.int32'>\n",
      "Rater_Chloe type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0663 => rejectH0=False | rho_f=0.853, rho_h=0.987\n",
      "Rater_Gaia: p=0.0663 => rejectH0=False | rho_f=0.853, rho_h=0.987\n",
      "Rater_Chloe: p=0.0295 => rejectH0=False | rho_f=0.853, rho_h=0.960\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.000\n",
      "Average Advantage Probability (rho) = 0.853\n",
      "Passed Alt-Test? => False\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 75 valid labels\n",
      "  Rater_Oli: 75 valid labels\n",
      "  Rater_Gaia: 75 valid labels\n",
      "  Rater_Chloe: 75 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_Gaia: int64\n",
      "  Rater_Chloe: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_Gaia type after conversion: <class 'numpy.int32'>\n",
      "Rater_Chloe type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.1214 => rejectH0=False | rho_f=0.840, rho_h=0.987\n",
      "Rater_Gaia: p=0.1214 => rejectH0=False | rho_f=0.840, rho_h=0.987\n",
      "Rater_Chloe: p=0.0572 => rejectH0=False | rho_f=0.840, rho_h=0.960\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.000\n",
      "Average Advantage Probability (rho) = 0.840\n",
      "Passed Alt-Test? => False\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 75 valid labels\n",
      "  Rater_Oli: 75 valid labels\n",
      "  Rater_Gaia: 75 valid labels\n",
      "  Rater_Chloe: 75 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_Gaia: int64\n",
      "  Rater_Chloe: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_Gaia type after conversion: <class 'numpy.int32'>\n",
      "Rater_Chloe type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0663 => rejectH0=False | rho_f=0.853, rho_h=0.987\n",
      "Rater_Gaia: p=0.0663 => rejectH0=False | rho_f=0.853, rho_h=0.987\n",
      "Rater_Chloe: p=0.0295 => rejectH0=False | rho_f=0.853, rho_h=0.960\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.000\n",
      "Average Advantage Probability (rho) = 0.853\n",
      "Passed Alt-Test? => False\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 75 valid labels\n",
      "  Rater_Oli: 75 valid labels\n",
      "  Rater_Gaia: 75 valid labels\n",
      "  Rater_Chloe: 75 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_Gaia: int64\n",
      "  Rater_Chloe: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_Gaia type after conversion: <class 'numpy.int32'>\n",
      "Rater_Chloe type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.1214 => rejectH0=False | rho_f=0.840, rho_h=0.987\n",
      "Rater_Gaia: p=0.1214 => rejectH0=False | rho_f=0.840, rho_h=0.987\n",
      "Rater_Chloe: p=0.0572 => rejectH0=False | rho_f=0.840, rho_h=0.960\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.000\n",
      "Average Advantage Probability (rho) = 0.840\n",
      "Passed Alt-Test? => False\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 75 valid labels\n",
      "  Rater_Oli: 75 valid labels\n",
      "  Rater_Gaia: 75 valid labels\n",
      "  Rater_Chloe: 75 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_Gaia: int64\n",
      "  Rater_Chloe: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_Gaia type after conversion: <class 'numpy.int32'>\n",
      "Rater_Chloe type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0312 => rejectH0=False | rho_f=0.867, rho_h=0.987\n",
      "Rater_Gaia: p=0.0312 => rejectH0=False | rho_f=0.867, rho_h=0.987\n",
      "Rater_Chloe: p=0.0133 => rejectH0=False | rho_f=0.867, rho_h=0.960\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.000\n",
      "Average Advantage Probability (rho) = 0.867\n",
      "Passed Alt-Test? => False\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 75 valid labels\n",
      "  Rater_Oli: 75 valid labels\n",
      "  Rater_Gaia: 75 valid labels\n",
      "  Rater_Chloe: 75 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_Gaia: int64\n",
      "  Rater_Chloe: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_Gaia type after conversion: <class 'numpy.int32'>\n",
      "Rater_Chloe type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0122 => rejectH0=True | rho_f=0.880, rho_h=0.987\n",
      "Rater_Gaia: p=0.0122 => rejectH0=True | rho_f=0.880, rho_h=0.987\n",
      "Rater_Chloe: p=0.0051 => rejectH0=True | rho_f=0.880, rho_h=0.960\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 1.000\n",
      "Average Advantage Probability (rho) = 0.880\n",
      "Passed Alt-Test? => True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>N_train</th>\n",
       "      <th>winning_rate_train</th>\n",
       "      <th>passed_alt_test_train</th>\n",
       "      <th>avg_adv_prob_train</th>\n",
       "      <th>p_values_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>few_shot</td>\n",
       "      <td>225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.848889</td>\n",
       "      <td>[0.08466375098751755, 0.08466375098751755, 0.03874305049373127]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>zero_shot</td>\n",
       "      <td>225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.862222</td>\n",
       "      <td>[0.05491296600985901, 0.05491296600985901, 0.025228945592110284]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_name  N_train  winning_rate_train  passed_alt_test_train  \\\n",
       "6    few_shot      225                 0.0                  False   \n",
       "7   zero_shot      225                 0.0                  False   \n",
       "\n",
       "   avg_adv_prob_train  \\\n",
       "6            0.848889   \n",
       "7            0.862222   \n",
       "\n",
       "                                                     p_values_train  \n",
       "6   [0.08466375098751755, 0.08466375098751755, 0.03874305049373127]  \n",
       "7  [0.05491296600985901, 0.05491296600985901, 0.025228945592110284]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run ALT test\n",
    "epsilon = 0.2  # Epsilon parameter for ALT test\n",
    "alt_test_df = run_alt_test_on_results(\n",
    "    detailed_results_df=complex_case_for_metrics,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    epsilon=epsilon,\n",
    "    alpha=0.05,\n",
    "    verbose=verbose\n",
    ")\n",
    "alt_test_df = alt_test_df.drop(\n",
    "    columns=[\"iteration\", \"run\", \"use_validation_set\", \"N_val\", \"n_runs\"]\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)   # show full content in each cell\n",
    "alt_test_df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step: Classify the Full Dataset\n",
    "\n",
    "If you are satisfied with the evaluation metrics, you can now use the **best-performing scenario** to classify the **entire unlabeled dataset**.\n",
    "\n",
    "Simply **copy the chosen scenario** and run the classification.\n",
    "\n",
    "> This time, only **one run is needed**, since you're not computing evaluation metrics (there are no human labels to compare against).\n",
    "\n",
    "If you're **not satisfied with the results**, feel free to continue exploring and testing **different scenarios**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = [\n",
    "    {\n",
    "        # LLM settings\n",
    "        \"provider_llm1\": \"azure\",\n",
    "        \"model_name_llm1\": \"gpt-4o\",\n",
    "        \"temperature_llm1\": 0,\n",
    "        \"prompt_name\": \"few_shot\",\n",
    "        \"subsample_size\": -1,  # Size of data subset to use\n",
    "\n",
    "        # Prompt configuration\n",
    "        \"template\": \"\"\"\n",
    "You are an assistant that evaluates data entries.\n",
    "\n",
    "The data has the following columns:\n",
    "- \"ID\": Unique identifiant of the participant\n",
    "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\\n\"\n",
    "- \"Identify\": Response for the IDENTIFY step\n",
    "- \"Guess\": Response for the GUESS step\n",
    "- \"Seek\": Response for the SEEK step\n",
    "- \"Assess\": Response for the ASSESS step\n",
    "\n",
    "Here is an entry to evaluate:\n",
    "{verbatim_text}\n",
    "\n",
    "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
    "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
    "\n",
    "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
    "\n",
    "- Identify Step: Does the Identify step indicate a topic of interest?\n",
    "- Guess Step: Does the Guess step suggest a possible explanation?\n",
    "- Seek Step: Is the Seek step formulated as a question?\n",
    "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
    "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
    "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
    "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
    "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
    "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
    "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
    "\n",
    "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
    "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
    "\n",
    "If all these criteria are met, the cycle is valid.\n",
    "Validity is expressed as:\n",
    "1: Valid cycle\n",
    "0: Invalid cycle\n",
    "\n",
    "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Example 1\n",
    "Key:\n",
    "AA25I4\n",
    "\n",
    "Reference:\n",
    "\"Rain forms when water evaporates into the atmosphere, condenses into droplets, and falls due to gravity.\"\n",
    "\n",
    "Cycle Steps:\n",
    "IDENTIFY: \"I don‚Äôt understand how rain forms.\"\n",
    "GUESS: \"Maybe rain condenses in the sky, forming droplets.\"\n",
    "SEEK: \"How does rain form?\"\n",
    "ASSESS: \"No\"\n",
    "Assess Cues:\n",
    "\n",
    "Validity Columns:\n",
    "Identify_validity: NA\n",
    "Guess_validity: 2\n",
    "Seek_validity: NA\n",
    "Assess_validity: NA\n",
    "Mechanical_rating: NA\n",
    "\n",
    "Reasoning\n",
    "Since the mechanical_rating column is empty, the validity must be determined using the codebook.\n",
    "\n",
    "Reasoning:\n",
    "Identify step: Does the Identify step indicate a topic of interest?\n",
    "Yes: The topic is the formation of rain.\n",
    "\n",
    "Guess step: A numeric value is present in the Guess_validity column, so no further validation is needed.\n",
    "Yes: It proposes condensation as the mechanism for rain formation.\n",
    "\n",
    "Seek step: Is the Seek step formulated as a question?\n",
    "Yes: It is explicitly phrased as a question with an interrogative structure.\n",
    "\n",
    "Assess step: Does it identify a possible answer or state that no answer was found (\"No\" is acceptable)?\n",
    "Yes: It states that the answer to the question was not found, which is a valid response in the Assess step.\n",
    "\n",
    "Consistency: Are the Identify, Guess, and Seek steps related to the same topic?\n",
    "Yes: They all pertain to the process of rain formation.\n",
    "\n",
    "Reference Link: Are the Identify, Guess, and Seek steps related to the reference text?\n",
    "Yes: The text discusses rain and explains its formation.\n",
    "\n",
    "Seek Question Originality: Is the answer to the Seek question absent (even vaguely) from the reference text?\n",
    "No: The answer is explicitly provided in the reference text.\n",
    "\n",
    "Resolving Answer:\n",
    "Not applicable (the answer was not found).\n",
    "\n",
    "Valid Answer:\n",
    "Not applicable (the answer was not found).\n",
    "\n",
    "Valid No: Is the answer to the SEEK question absent from the assess_cues?\n",
    "Yes: The answer to the SEEK question is not in assess_cues, so the \"No\" is valid.\n",
    "\n",
    "Conclusion\n",
    "The cycle is not valid because the answer to the SEEK question is explicitly present in the reference text.\n",
    "\n",
    "Validity:\n",
    "0\n",
    "\"\"\",\n",
    "        # Output\n",
    "        \"selected_fields\": [\"Classification\", \"Reasoning\"],\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\":\n",
    "        \"\"\"\n",
    "Please follow the JSON format below:\n",
    "```json\n",
    "{{\n",
    "  \"Reasoning\": \"Your text here\",\n",
    "  \"Classification\": \"Your integer here\"\n",
    "}}\n",
    "\"\"\",\n",
    "        \"json_output\": True,\n",
    "\n",
    "        # Prompt optimization\n",
    "        \"provider_llm2\": \"azure\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "        \"max_iterations\": 1,\n",
    "        \"use_validation_set\": False,\n",
    "        \"validation_size\": 10,\n",
    "        \"random_state\": 42,\n",
    "\n",
    "        # Majority vote\n",
    "        \"n_completions\": 1,\n",
    "\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run the scenario\n",
    "complex_case_fully_annotated = run_scenarios(\n",
    "    scenarios=scenario,\n",
    "    data=data,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    n_runs=n_runs,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_case_fully_annotated.to_csv(\"data/complex_user_case/outputs/complex_case_fully_annotated.csv\", sep=\";\", index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
