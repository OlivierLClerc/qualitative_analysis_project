{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Import functions from qualitative_analysis\n",
    "from qualitative_analysis.data_processing import load_data, clean_and_normalize, sanitize_dataframe\n",
    "from qualitative_analysis.model_interaction import get_llm_client\n",
    "from qualitative_analysis.evaluation import compute_cohens_kappa\n",
    "from qualitative_analysis.utils import save_results_to_csv, load_results_from_csv\n",
    "import qualitative_analysis.config as config\n",
    "from qualitative_analysis.prompt_construction import construct_prompt\n",
    "from qualitative_analysis.prompt_construction import build_data_format_description\n",
    "from qualitative_analysis.response_parsing import extract_code_from_response\n",
    "from qualitative_analysis.cost_estimation import openai_api_calculate_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = 'data'\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ref</th>\n",
       "      <th>txt1.ctrl1</th>\n",
       "      <th>txt1.det</th>\n",
       "      <th>txt1.exp</th>\n",
       "      <th>txt1.ctrl2</th>\n",
       "      <th>corr_cycle1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BC23</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>Le manteau,le centre de la Terre</td>\n",
       "      <td>Je pense que le manteau et le centre de la Ter...</td>\n",
       "      <td>Le manteau et le centre de la Terre ne font il...</td>\n",
       "      <td>Je n'ai pas pu trouver la réponse à  ma question.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BC14</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>Température du magma</td>\n",
       "      <td>La température du magma s'élève à  plus de 100...</td>\n",
       "      <td>à combien de  °cle magma est?</td>\n",
       "      <td>La température du magma atteint les 1000 °C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BC5</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>Pourqoi on l'appelle le manteau.</td>\n",
       "      <td>le magma et une pierre.</td>\n",
       "      <td>quel et la temperature magma</td>\n",
       "      <td>1 000c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BC22</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>La température du magma</td>\n",
       "      <td>La température du magma dépasse les 500 °C</td>\n",
       "      <td>Quel est la température du magma ?</td>\n",
       "      <td>La température du magma atteint les 1000 °C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BC20</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>Combien de couches y-t-il ?</td>\n",
       "      <td>Il existe d'autre couches dans la terre à  par...</td>\n",
       "      <td>Quelles sont les autres couches et combien son...</td>\n",
       "      <td>La terre contient 7 couches  dont le noyau.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                                                ref  \\\n",
       "0  BC23  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "1  BC14  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "2   BC5  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "3  BC22  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "4  BC20  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "\n",
       "                         txt1.ctrl1  \\\n",
       "0  Le manteau,le centre de la Terre   \n",
       "1              Température du magma   \n",
       "2  Pourqoi on l'appelle le manteau.   \n",
       "3           La température du magma   \n",
       "4       Combien de couches y-t-il ?   \n",
       "\n",
       "                                            txt1.det  \\\n",
       "0  Je pense que le manteau et le centre de la Ter...   \n",
       "1  La température du magma s'élève à  plus de 100...   \n",
       "2                            le magma et une pierre.   \n",
       "3         La température du magma dépasse les 500 °C   \n",
       "4  Il existe d'autre couches dans la terre à  par...   \n",
       "\n",
       "                                            txt1.exp  \\\n",
       "0  Le manteau et le centre de la Terre ne font il...   \n",
       "1                      à combien de  °cle magma est?   \n",
       "2                       quel et la temperature magma   \n",
       "3                 Quel est la température du magma ?   \n",
       "4  Quelles sont les autres couches et combien son...   \n",
       "\n",
       "                                          txt1.ctrl2  corr_cycle1  \n",
       "0  Je n'ai pas pu trouver la réponse à  ma question.            1  \n",
       "1        La température du magma atteint les 1000 °C            1  \n",
       "2                                             1 000c            0  \n",
       "3        La température du magma atteint les 1000 °C            1  \n",
       "4        La terre contient 7 couches  dont le noyau.            1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to your dataset\n",
    "data_file_path = os.path.join(data_dir, 'binary_sample.csv')\n",
    "\n",
    "# Load the data\n",
    "data = load_data(data_file_path, file_type='csv', delimiter=';')\n",
    "\n",
    "# Preview the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ocler\\Documents\\Académique\\Inria\\qualitative_analysis_project\\qualitative_analysis\\data_processing.py:139: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df.applymap(\n"
     ]
    }
   ],
   "source": [
    "# 1) Define a mapping from old column names to new names\n",
    "rename_map = {\n",
    "    \"ref\": \"reference\",\n",
    "    \"txt1.ctrl1\": \"Identify\",\n",
    "    \"txt1.det\": \"Guess\",\n",
    "    \"txt1.exp\": \"Seek\",\n",
    "    \"txt1.ctrl2\": \"Assess\"\n",
    "}\n",
    "\n",
    "# 2) Rename the columns in the DataFrame\n",
    "data = data.rename(columns=rename_map)\n",
    "\n",
    "# 3) Now define the new column names for cleaning\n",
    "text_columns = [\"reference\", \"Identify\", \"Guess\", \"Seek\", \"Assess\"]\n",
    "\n",
    "# 4) Clean and normalize the new columns\n",
    "for col in text_columns:\n",
    "    data[col] = clean_and_normalize(data[col])\n",
    "\n",
    "# 5) Sanitize the DataFrame\n",
    "data = sanitize_dataframe(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of verbatims: 45\n",
      "Verbatim example:\n",
      "Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: Le manteau,le centre de la Terre\n",
      "Guess: Je pense que le manteau et le centre de la Terre sont les màames choses\n",
      "Seek: Le manteau et le centre de la Terre ne font ils qu'un?\n",
      "Assess: Je n'ai pas pu trouver la réponse à  ma question.\n"
     ]
    }
   ],
   "source": [
    "# Combine texts and entries\n",
    "\n",
    "data['verbatim'] = data.apply(\n",
    "    lambda row: (\n",
    "        f\"Reference: {row['reference']}\\n\\n\"\n",
    "        f\"Identify: {row['Identify']}\\n\"\n",
    "        f\"Guess: {row['Guess']}\\n\"\n",
    "        f\"Seek: {row['Seek']}\\n\"\n",
    "        f\"Assess: {row['Assess']}\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Extract the list of verbatims\n",
    "verbatims = data['verbatim'].tolist()\n",
    "\n",
    "print(f\"Total number of verbatims: {len(verbatims)}\")\n",
    "print(f\"Verbatim example:\\n{verbatims[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the provider and model\n",
    "provider = 'azure'\n",
    "model_name = 'gpt-4o-mini'\n",
    "\n",
    "# Initialize the client\n",
    "llm_client = get_llm_client(provider=provider, config=config.MODEL_CONFIG[provider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing Verbatim 1/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 870 (prompt) + 37 (completion) = 907 total\n",
      "Cost for Theme 'Identify Step': $0.0002\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 872 (prompt) + 40 (completion) = 912 total\n",
      "Cost for Theme 'Guess Step': $0.0002\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 864 (prompt) + 32 (completion) = 896 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 875 (prompt) + 24 (completion) = 899 total\n",
      "Cost for Theme 'Assess Step': $0.0001\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 891 (prompt) + 50 (completion) = 941 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 893 (prompt) + 43 (completion) = 936 total\n",
      "Cost for Theme 'Reference Link': $0.0002\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 891 (prompt) + 37 (completion) = 928 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 2/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 863 (prompt) + 71 (completion) = 934 total\n",
      "Cost for Theme 'Identify Step': $0.0002\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 856 (prompt) + 28 (completion) = 884 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 853 (prompt) + 25 (completion) = 878 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 891 (prompt) + 43 (completion) = 934 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 874 (prompt) + 37 (completion) = 911 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 1\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 886 (prompt) + 41 (completion) = 927 total\n",
      "Cost for Theme 'Reference Link': $0.0002\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 901 (prompt) + 51 (completion) = 952 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 3/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 818 (prompt) + 46 (completion) = 864 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 0\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 812 (prompt) + 42 (completion) = 854 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 795 (prompt) + 25 (completion) = 820 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 841 (prompt) + 51 (completion) = 892 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 823 (prompt) + 43 (completion) = 866 total\n",
      "Cost for Theme 'Consistency': $0.0001\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 842 (prompt) + 54 (completion) = 896 total\n",
      "Cost for Theme 'Reference Link': $0.0002\n",
      "Extracted Score for 'Reference Link': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 830 (prompt) + 38 (completion) = 868 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0001\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 4/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 856 (prompt) + 69 (completion) = 925 total\n",
      "Cost for Theme 'Identify Step': $0.0002\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 852 (prompt) + 30 (completion) = 882 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 847 (prompt) + 25 (completion) = 872 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 880 (prompt) + 38 (completion) = 918 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 871 (prompt) + 39 (completion) = 910 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 1\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 873 (prompt) + 34 (completion) = 907 total\n",
      "Cost for Theme 'Reference Link': $0.0002\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 901 (prompt) + 57 (completion) = 958 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 5/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 848 (prompt) + 40 (completion) = 888 total\n",
      "Cost for Theme 'Identify Step': $0.0002\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 841 (prompt) + 35 (completion) = 876 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 834 (prompt) + 28 (completion) = 862 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 878 (prompt) + 52 (completion) = 930 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 862 (prompt) + 46 (completion) = 908 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 879 (prompt) + 55 (completion) = 934 total\n",
      "Cost for Theme 'Reference Link': $0.0002\n",
      "Extracted Score for 'Reference Link': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 863 (prompt) + 35 (completion) = 898 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0001\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 6/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 844 (prompt) + 40 (completion) = 884 total\n",
      "Cost for Theme 'Identify Step': $0.0002\n",
      "Extracted Score for 'Identify Step': 0\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 838 (prompt) + 36 (completion) = 874 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 827 (prompt) + 25 (completion) = 852 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 850 (prompt) + 28 (completion) = 878 total\n",
      "Cost for Theme 'Assess Step': $0.0001\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 869 (prompt) + 57 (completion) = 926 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 859 (prompt) + 40 (completion) = 899 total\n",
      "Cost for Theme 'Reference Link': $0.0002\n",
      "Extracted Score for 'Reference Link': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 864 (prompt) + 40 (completion) = 904 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 7/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 835 (prompt) + 41 (completion) = 876 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 827 (prompt) + 35 (completion) = 862 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 819 (prompt) + 27 (completion) = 846 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 865 (prompt) + 53 (completion) = 918 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 861 (prompt) + 59 (completion) = 920 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 874 (prompt) + 64 (completion) = 938 total\n",
      "Cost for Theme 'Reference Link': $0.0002\n",
      "Extracted Score for 'Reference Link': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 853 (prompt) + 39 (completion) = 892 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 8/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 865 (prompt) + 31 (completion) = 896 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 869 (prompt) + 37 (completion) = 906 total\n",
      "Cost for Theme 'Guess Step': $0.0002\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 859 (prompt) + 27 (completion) = 886 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 884 (prompt) + 33 (completion) = 917 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 905 (prompt) + 63 (completion) = 968 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 893 (prompt) + 43 (completion) = 936 total\n",
      "Cost for Theme 'Reference Link': $0.0002\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 906 (prompt) + 52 (completion) = 958 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 9/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 854 (prompt) + 39 (completion) = 893 total\n",
      "Cost for Theme 'Identify Step': $0.0002\n",
      "Extracted Score for 'Identify Step': 0\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 852 (prompt) + 39 (completion) = 891 total\n",
      "Cost for Theme 'Guess Step': $0.0002\n",
      "Extracted Score for 'Guess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 844 (prompt) + 30 (completion) = 874 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 0\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 870 (prompt) + 36 (completion) = 906 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 859 (prompt) + 36 (completion) = 895 total\n",
      "Cost for Theme 'Consistency': $0.0001\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 873 (prompt) + 41 (completion) = 914 total\n",
      "Cost for Theme 'Reference Link': $0.0002\n",
      "Extracted Score for 'Reference Link': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 885 (prompt) + 49 (completion) = 934 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 10/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 831 (prompt) + 67 (completion) = 898 total\n",
      "Cost for Theme 'Identify Step': $0.0002\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 828 (prompt) + 30 (completion) = 858 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 825 (prompt) + 27 (completion) = 852 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 867 (prompt) + 49 (completion) = 916 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 840 (prompt) + 33 (completion) = 873 total\n",
      "Cost for Theme 'Consistency': $0.0001\n",
      "Extracted Score for 'Consistency': 1\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 863 (prompt) + 47 (completion) = 910 total\n",
      "Cost for Theme 'Reference Link': $0.0002\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 860 (prompt) + 40 (completion) = 900 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 11/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 839 (prompt) + 67 (completion) = 906 total\n",
      "Cost for Theme 'Identify Step': $0.0002\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 838 (prompt) + 32 (completion) = 870 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 831 (prompt) + 25 (completion) = 856 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 864 (prompt) + 38 (completion) = 902 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 865 (prompt) + 49 (completion) = 914 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 857 (prompt) + 34 (completion) = 891 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 870 (prompt) + 42 (completion) = 912 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 12/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 831 (prompt) + 43 (completion) = 874 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 0\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 825 (prompt) + 39 (completion) = 864 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 815 (prompt) + 29 (completion) = 844 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 846 (prompt) + 41 (completion) = 887 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 839 (prompt) + 44 (completion) = 883 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 862 (prompt) + 58 (completion) = 920 total\n",
      "Cost for Theme 'Reference Link': $0.0002\n",
      "Extracted Score for 'Reference Link': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 853 (prompt) + 45 (completion) = 898 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 13/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 786 (prompt) + 31 (completion) = 817 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 0\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 783 (prompt) + 30 (completion) = 813 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 779 (prompt) + 25 (completion) = 804 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 0\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 810 (prompt) + 36 (completion) = 846 total\n",
      "Cost for Theme 'Assess Step': $0.0001\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 809 (prompt) + 45 (completion) = 854 total\n",
      "Cost for Theme 'Consistency': $0.0001\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 813 (prompt) + 42 (completion) = 855 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 811 (prompt) + 36 (completion) = 847 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0001\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 14/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 835 (prompt) + 67 (completion) = 902 total\n",
      "Cost for Theme 'Identify Step': $0.0002\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 839 (prompt) + 37 (completion) = 876 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 828 (prompt) + 26 (completion) = 854 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 860 (prompt) + 38 (completion) = 898 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 870 (prompt) + 58 (completion) = 928 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 854 (prompt) + 35 (completion) = 889 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 870 (prompt) + 46 (completion) = 916 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 15/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 807 (prompt) + 71 (completion) = 878 total\n",
      "Cost for Theme 'Identify Step': $0.0002\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 810 (prompt) + 38 (completion) = 848 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 799 (prompt) + 27 (completion) = 826 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 832 (prompt) + 41 (completion) = 873 total\n",
      "Cost for Theme 'Assess Step': $0.0001\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 834 (prompt) + 52 (completion) = 886 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 826 (prompt) + 36 (completion) = 862 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 839 (prompt) + 45 (completion) = 884 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "\n",
      "=== Processing Complete ===\n",
      "Total Tokens Used: 93594\n",
      "Total Cost for Processing: $0.0160\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Le manteau,le centre de la Terre\\nGuess: Je pense que le manteau et le centre de la Terre sont les màames choses\\nSeek: Le manteau et le centre de la Terre ne font ils qu'un?\\nAssess: Je n'ai pas pu trouver la réponse à  ma question.\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Température du magma\\nGuess: La température du magma s'élève à  plus de 1000 °c\\nSeek: à combien de  °cle magma est?\\nAssess: La température du magma atteint les 1000 °C\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Pourqoi on l'appelle le manteau.\\nGuess: le magma et une pierre.\\nSeek: quel et la temperature magma\\nAssess: 1 000c\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: La température du magma\\nGuess: La température du magma dépasse les 500 °C\\nSeek: Quel est la température du magma ?\\nAssess: La température du magma atteint les 1000 °C\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Combien de couches y-t-il ?\\nGuess: Il existe d'autre couches dans la terre à  part le manteau .\\nSeek: Quelles sont les autres couches et combien sont-elles ?\\nAssess: La terre contient 7 couches  dont le noyau.\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Pourquoi ont apelle le liquide le magma.\\nGuess: Moi je crois que le magma est t' une ville d'Afrique.\\nSeek: Quelle est la température du m agma.\\nAssess: Je n'et pas trouvé.\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Les Couches de terre\\nGuess: Il existe d 'autres couches dans le manteau .\\nSeek: Quelle sont les autres couches de la terre\\nAssess: La terre contient 7 chouches dont le noyau.\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Le magma est le responsable de l'explosion car c'est le coeur du volcan.\\nGuess: IL existe d'autres couches dans la terre à  par le manteau.\\nSeek: Que se passe-t-il pendant l'éruption du volcan\\nAssess: IL y a d'autres couches dans la terre dont le noyau\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Si le<<manteau>>existé pas qu'est qui ce passeré\\nGuess: Le manteau est chaud ou froid?\\nSeek: Si on y va on peut mourir\\nAssess: Je n'est pas la raiponce de la question!!!!!!!!!!!!!!!!!!!?!!!!!\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: température du magma\\nGuess: la temperature du magma depasse les 500 C\\nSeek: quelle est la temperature du magma\\nAssess: La température du magma atteint led 1000 degres\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: le magma\\nGuess: La température du magma dépasse les 500c\\nSeek: Quelle est la température du magma?\\nAssess: La température du magma atteint les 1000c\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Les roches sont liquide\\nGuess: Durant l'éruption le volcan fait sortir du magma\\nSeek: Pourquoi le liquide s' appelle le magma\\nAssess: Combien de couche contient le noyau\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: j'ai pas trouver\\nGuess: ...\\nSeek: J'AI AUCUNNE IDEà\\x89\\nAssess: ...\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: comment le magma explosent si c'est une substance liquide?\\nGuess: La température du magma dépasse les 500 \\x84\\x83\\nSeek: comment le magma explosent ?\\nAssess: je n'est pas trouver\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: C'est quoi le magma\\nGuess: Je pense que le magma est de la lave\\nSeek: Pourquoi appel-ton le magma\\nAssess: Je n'ai pas trouvé\", 'Overall_Validity': 0}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Define column descriptions\n",
    "column_descriptions = {\n",
    "    \"ID\": \"Unique identifier for each entry\",\n",
    "    \"reference\": \"The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\",\n",
    "    \"Identify\": \"Response for the IDENTIFY step\",\n",
    "    \"Guess\": \"Response for the GUESS step\",\n",
    "    \"Seek\": \"Response for the SEEK step\",\n",
    "    \"Assess\": \"Response for the ASSESS step\"\n",
    "}\n",
    "\n",
    "# Define criteria as themes\n",
    "codebook = {\n",
    "    \"Identify Step\": \"Does the Identify step indicate a topic of interest?\",\n",
    "    \"Guess Step\": \"Does the Guess step suggest a possible explanation?\",\n",
    "    \"Seek Step\": \"Is the Seek step formulated as a question?\",\n",
    "    \"Assess Step\": \"Does it identify a possible answer or state that no answer was found ('no' is ok)?\",\n",
    "    \"Consistency\": \"Are the Identify, Guess, and Seek steps related to the same question?\",\n",
    "    \"Reference Link\": \"Are the Identify, Guess, and Seek steps related to the topic of the reference text?\",\n",
    "    \"Seek Question Originality\": \"Is the answer to the Seek question not found (even vaguely) in the reference text?\",\n",
    "}\n",
    "\n",
    "# Build data format description\n",
    "data_format_description = build_data_format_description(column_descriptions)\n",
    "\n",
    "# Classification settings\n",
    "post_reasoning = False\n",
    "\n",
    "# Define queries for binary classification\n",
    "binary_query = \"Reply with '1' if the entry meets the criterion or '0' otherwise.\"\n",
    "post_reasoning_query = \"First, generate a one-sentence reasoning about the classification.\"\n",
    "\n",
    "# Select subset of verbatims for testing\n",
    "verbatims_subset = verbatims[:15]\n",
    "\n",
    "def construct_prompt(verbatim, theme, theme_description, post_reasoning=False):\n",
    "    \"\"\"\n",
    "    Constructs a prompt for evaluating a specific binary criterion (theme) for a given verbatim.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant tasked with evaluating the following entry.\n",
    "\n",
    "You are provided with data entries in the following format:\n",
    "\n",
    "{data_format_description}\n",
    "\n",
    "**Entry:**\n",
    "{verbatim}\n",
    "\n",
    "**Criterion:**\n",
    "{theme} - {theme_description}\n",
    "{binary_query}\n",
    "\"\"\"\n",
    "    if post_reasoning:\n",
    "        prompt += f\"\\n\\n{post_reasoning_query}\"\n",
    "    return prompt\n",
    "\n",
    "def generate_with_reasoning(llm_client, model_name, prompt, reasoning_query=None, temperature=0.0001, verbose=False):\n",
    "    \"\"\"\n",
    "    Handles two-step LLM calls for binary classification with reasoning.\n",
    "    Returns the final classification response and the usage object.\n",
    "    \"\"\"\n",
    "    if reasoning_query:\n",
    "        # First call for reasoning\n",
    "        reasoning_prompt = f\"{prompt}\\n\\n{reasoning_query}\"\n",
    "        response_text_1, usage_1 = llm_client.get_response(\n",
    "            prompt=reasoning_prompt,\n",
    "            model=model_name,\n",
    "            max_tokens=500,\n",
    "            temperature=temperature,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        # Second call for classification based on reasoning\n",
    "        classification_prompt = f\"{prompt}\\n\\nReasoning:\\n{response_text_1}\\n\\nProvide the final classification.\"\n",
    "        response_text_2, usage_2 = llm_client.get_response(\n",
    "            prompt=classification_prompt,\n",
    "            model=model_name,\n",
    "            max_tokens=500,\n",
    "            temperature=temperature,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        # Combine usage statistics\n",
    "        usage_1.prompt_tokens += usage_2.prompt_tokens\n",
    "        usage_1.completion_tokens += usage_2.completion_tokens\n",
    "        usage_1.total_tokens += usage_2.total_tokens\n",
    "        return response_text_2, usage_1\n",
    "\n",
    "    # Single-step classification (if post_reasoning is False)\n",
    "    response_text, usage = llm_client.get_response(\n",
    "        prompt=prompt,\n",
    "        model=model_name,\n",
    "        max_tokens=500,\n",
    "        temperature=temperature,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    return response_text, usage\n",
    "\n",
    "# Initialize results and cost tracking\n",
    "results = []\n",
    "verbatim_costs = []  # Track costs per verbatim\n",
    "total_tokens_used = 0\n",
    "total_cost = 0\n",
    "\n",
    "for idx, verbatim in enumerate(verbatims_subset):\n",
    "    print(f\"=== Processing Verbatim {idx + 1}/{len(verbatims_subset)} ===\")\n",
    "    verbatim_tokens_used = 0  # Track tokens for this verbatim\n",
    "    verbatim_cost = 0  # Track cost for this verbatim\n",
    "\n",
    "    # Binary classification processing\n",
    "    for theme, theme_description in codebook.items():\n",
    "        print(f\"\\n--- Evaluating Theme: {theme} ---\")\n",
    "\n",
    "        # Build the prompt\n",
    "        prompt = construct_prompt(\n",
    "            verbatim=verbatim, \n",
    "            theme=theme, \n",
    "            theme_description=theme_description, \n",
    "            post_reasoning=post_reasoning\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response_content, usage = generate_with_reasoning(\n",
    "                llm_client=llm_client,\n",
    "                model_name=model_name,\n",
    "                prompt=prompt,\n",
    "                reasoning_query=post_reasoning_query if post_reasoning else None,\n",
    "                temperature=0.0001,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Track token usage\n",
    "            if usage:\n",
    "                prompt_tokens = usage.prompt_tokens\n",
    "                completion_tokens = usage.completion_tokens\n",
    "                total_tokens = usage.total_tokens\n",
    "\n",
    "                # Calculate the cost for this request\n",
    "                cost = openai_api_calculate_cost(usage, model=model_name)\n",
    "                total_tokens_used += total_tokens\n",
    "                total_cost += cost\n",
    "                verbatim_tokens_used += total_tokens\n",
    "                verbatim_cost += cost\n",
    "\n",
    "                # Print detailed token usage and cost\n",
    "                print(f\"Tokens Used: {prompt_tokens} (prompt) + {completion_tokens} (completion) = {total_tokens} total\")\n",
    "                print(f\"Cost for Theme '{theme}': ${cost:.4f}\")\n",
    "\n",
    "            # Parse response\n",
    "            score = extract_code_from_response(response_content)\n",
    "            if score in [0, 1]:\n",
    "                results.append({\n",
    "                    'Verbatim': verbatim,\n",
    "                    'Theme': theme,\n",
    "                    'Score': score\n",
    "                })\n",
    "                print(f\"Extracted Score for '{theme}': {score}\")\n",
    "            else:\n",
    "                print(f\"Failed to parse a valid score for '{theme}'\")\n",
    "                results.append({\n",
    "                    'Verbatim': verbatim,\n",
    "                    'Theme': theme,\n",
    "                    'Score': None\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Verbatim {idx + 1} for Theme '{theme}': {e}\")\n",
    "            results.append({\n",
    "                'Verbatim': verbatim,\n",
    "                'Theme': theme,\n",
    "                'Score': None\n",
    "            })\n",
    "\n",
    "    # Store verbatim-level cost\n",
    "    verbatim_costs.append({'Verbatim': verbatim, 'Tokens Used': verbatim_tokens_used, 'Cost': verbatim_cost})\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n=== Processing Complete ===\")\n",
    "print(f\"Total Tokens Used: {total_tokens_used}\")\n",
    "print(f\"Total Cost for Processing: ${total_cost:.4f}\")\n",
    "\n",
    "# Optional: Detailed Token and Cost Breakdown\n",
    "# for cost_entry in verbatim_costs:\n",
    "#     print(f\"Verbatim: {cost_entry['Verbatim']}, Tokens Used: {cost_entry['Tokens Used']}, Cost: ${cost_entry['Cost']:.4f}\")\n",
    "\n",
    "# Organize scores by verbatim for binary classification\n",
    "verbatim_scores = defaultdict(dict)\n",
    "for entry in results:\n",
    "    verbatim = entry['Verbatim']\n",
    "    theme = entry['Theme']\n",
    "    score = entry['Score']\n",
    "    verbatim_scores[verbatim][theme] = score\n",
    "\n",
    "# Determine overall validity\n",
    "final_results = []\n",
    "for verbatim, scores in verbatim_scores.items():\n",
    "    overall_validity = 1  # Assume valid\n",
    "    for theme, score in scores.items():\n",
    "        if score != 1:\n",
    "            overall_validity = 0\n",
    "            break\n",
    "    final_results.append({\n",
    "        'Verbatim': verbatim,\n",
    "        'Overall_Validity': overall_validity\n",
    "    })\n",
    "\n",
    "# Optionally, print the final results\n",
    "for result in final_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: data\\outputs\\experiment_gpt-4o-mini_20241218_194022.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# Define the save path\n",
    "outputs_dir = os.path.join(data_dir, 'outputs')\n",
    "os.makedirs(outputs_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "save_path = os.path.join(outputs_dir, f\"experiment_{model_name.replace('/', '_')}_{timestamp}.csv\")\n",
    "\n",
    "# Save results\n",
    "save_results_to_csv(\n",
    "    coding=results_df.to_dict('records'),\n",
    "    save_path=save_path,\n",
    "    fieldnames=['Verbatim', 'Overall_Validity'],\n",
    "    verbatims=None  # Verbatims are included in the results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results loaded from: data\\outputs\\experiment_gpt-4o-mini_20241218_194022.csv\n"
     ]
    }
   ],
   "source": [
    "# Load results\n",
    "loaded_results = load_results_from_csv(save_path)\n",
    "# The function returns (verbatims, coding)\n",
    "verbatims_loaded, coding_loaded = loaded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Score between human annotations and model: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have human annotations in the data\n",
    "human_annotations = data['corr_cycle1'].tolist()  # Replace with actual column name\n",
    "model_coding = results_df['Overall_Validity'].tolist()\n",
    "\n",
    "human_annotations_short = human_annotations[:15]\n",
    "\n",
    "# Compute Cohen's Kappa\n",
    "kappa = compute_cohens_kappa(\n",
    "    human_annotations_short,\n",
    "    model_coding,\n",
    "    labels=[0, 1],\n",
    "    weights='linear'\n",
    ")\n",
    "\n",
    "print(f\"Cohen's Kappa Score between human annotations and model: {kappa:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(human_annotations_short, model_coding, labels=[0, 1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
