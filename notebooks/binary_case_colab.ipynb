{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will perform **automatic classification of textual data** using **Large Language Models (LLMs)**.\n",
    "\n",
    "The dataset we'll be working with requires binary labels (`0` or `1`), meaning this is a **binary classification task**, where each data entry is assigned to one of the two predefined categories.\n",
    "\n",
    "To help you navigate this notebook, here is a step-by-step outline of what we will do:\n",
    "\n",
    "1. **Getting started**  \n",
    "   - Download and install the project and its dependencies, load import and your API key.\n",
    "\n",
    "2. **Load and preprocess the dataset**  \n",
    "   - Upload, explore and pre-process the dataset, with the sample dataset (recommended for a first use) or your own data.\n",
    "\n",
    "3. **Prompt construction and classification on manually annotated data**  \n",
    "\n",
    "4. **Evaluating Model Performance Against Human Annotations**  \n",
    "   - Compute metrics (e.g., **Cohen's Kappa**, **Alt-Test**, ...)\n",
    "\n",
    "5. **Final Step: Classify the Full Dataset**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Before we begin, let's set up the environment by cloning the project and installing the necessary dependencies.\n",
    "\n",
    "### Step 1: Clone the Project\n",
    "Run the following cell to download the project files.\n",
    "This will download the project folder into Colab and switch the working directory to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'qualitative_analysis_project'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/OlivierLClerc/qualitative_analysis_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Install Required Libraries\n",
    "Now, install the project and its dependencies.\n",
    "\n",
    "‚ö†Ô∏è Note:\n",
    "\n",
    "- This will install all required libraries for the notebook to run.\n",
    "- If Colab suggests restarting the runtime, click \"Restart Runtime\" and re-run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd qualitative_analysis_project\n",
    "%pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load Your API Key\n",
    "\n",
    "To use an LLM for analysis, you need to provide your **API key**. This key allows secure access to the API.\n",
    "\n",
    "You can use this pipeline with an **OpenAI**, **Gemini**, or **Anthropic** key.  \n",
    "The code in the cell below is currently configured for **OpenAI**.\n",
    "\n",
    "If you're using another provider, simply replace all occurrences of `OPENAI_API_KEY` with the corresponding variable name:  \n",
    "- For **Gemini** ‚Üí `GEMINI_API_KEY`  \n",
    "- For **Anthropic** ‚Üí `ANTHROPIC_API_KEY`\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "1. Click on the **üîë \"Key\" icon** on the left sidebar in Colab (**‚öôÔ∏è Settings** > **Secrets**).  \n",
    "2. Click **\"Add a new secret\"**.  \n",
    "3. Enter the following:  \n",
    "   - **Name** ‚Üí `OPENAI_API_KEY`\n",
    "   - **Value** ‚Üí *Your API Key* (Get it from [OpenAI](https://platform.openai.com/account/api-keys))  \n",
    "4. Click **\"Save\"**.  \n",
    "\n",
    "#### Troubleshooting\n",
    "\n",
    "- **API Key not found?**  \n",
    "  - Double-check that the secret name is exactly **`OPENAI_API_KEY`**.  \n",
    "  - If the issue persists, **refresh the page** and rerun the cell.  \n",
    "\n",
    "- **Is My Key Secure?**  \n",
    "  - Yes! Colab's **Secrets Manager** encrypts your key and keeps it safe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Retrieve API keys securely from Colab Secrets\n",
    "API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Check if the API key was loaded\n",
    "if API_KEY:\n",
    "    print(\"‚úÖ API Key loaded successfully!\")\n",
    "    os.environ['OPENAI_API_KEY'] = API_KEY\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è API Key not found. Please check the Secrets panel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import Project Modules\n",
    "\n",
    "Now that the project is installed, let's import the necessary modules and functions from the `qualitative_analysis` package. These tools will help us load data, process text, and perform binary classification analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: vLLM is not available. VLLMLLMClient will not be usable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qualitative_analysis import (\n",
    "    load_data,\n",
    "    clean_and_normalize,\n",
    "    sanitize_dataframe,\n",
    ")\n",
    "import os\n",
    "from qualitative_analysis.scenario_runner import run_scenarios\n",
    "from qualitative_analysis.metrics.krippendorff import compute_krippendorff_non_inferiority, print_non_inferiority_results\n",
    "from qualitative_analysis.metrics.kappa import compute_kappa_metrics\n",
    "from qualitative_analysis.metrics.alt_test import run_alt_test_on_results\n",
    "from qualitative_analysis.metrics.classification import compute_classification_metrics_from_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the data (can be a CSV file or an xlsx file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>model</th>\n",
       "      <th>prompt</th>\n",
       "      <th>but</th>\n",
       "      <th>r√©ponse_attendue</th>\n",
       "      <th>r√©ponse_llm</th>\n",
       "      <th>iteration</th>\n",
       "      <th>Rater_Oli</th>\n",
       "      <th>Invalid_Oli</th>\n",
       "      <th>Rater_chloe</th>\n",
       "      <th>Invalid_chloe</th>\n",
       "      <th>Rater_RA</th>\n",
       "      <th>Invalid_RA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>GPT-4o mini</td>\n",
       "      <td>correct</td>\n",
       "      <td>Ton but est de trouver la pi√®ce qui aide ces d...</td>\n",
       "      <td>Le composant reliant les capteurs avec les act...</td>\n",
       "      <td>La partie d'un robot qui relie les capteurs et...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>GPT-4o mini</td>\n",
       "      <td>correct</td>\n",
       "      <td>Ton but est de trouver la pi√®ce qui aide ces d...</td>\n",
       "      <td>Le composant reliant les capteurs avec les act...</td>\n",
       "      <td>La partie d'un robot qui relie les capteurs et...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>GPT-4o mini</td>\n",
       "      <td>correct</td>\n",
       "      <td>Ton but est de trouver la pi√®ce qui aide ces d...</td>\n",
       "      <td>Le composant reliant les capteurs avec les act...</td>\n",
       "      <td>La partie d'un robot qui relie les capteurs et...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>GPT-4o mini</td>\n",
       "      <td>correct</td>\n",
       "      <td>Ton but est de trouver la pi√®ce qui aide ces d...</td>\n",
       "      <td>Le composant reliant les capteurs avec les act...</td>\n",
       "      <td>La partie d'un robot qui relie les capteurs et...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>GPT-4o mini</td>\n",
       "      <td>correct</td>\n",
       "      <td>Ton but est de trouver la pi√®ce qui aide ces d...</td>\n",
       "      <td>Le composant reliant les capteurs avec les act...</td>\n",
       "      <td>La partie d'un robot qui relie les capteurs et...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Code        model   prompt  \\\n",
       "0     1  GPT-4o mini  correct   \n",
       "1     1  GPT-4o mini  correct   \n",
       "2     1  GPT-4o mini  correct   \n",
       "3     1  GPT-4o mini  correct   \n",
       "4     1  GPT-4o mini  correct   \n",
       "\n",
       "                                                 but  \\\n",
       "0  Ton but est de trouver la pi√®ce qui aide ces d...   \n",
       "1  Ton but est de trouver la pi√®ce qui aide ces d...   \n",
       "2  Ton but est de trouver la pi√®ce qui aide ces d...   \n",
       "3  Ton but est de trouver la pi√®ce qui aide ces d...   \n",
       "4  Ton but est de trouver la pi√®ce qui aide ces d...   \n",
       "\n",
       "                                    r√©ponse_attendue  \\\n",
       "0  Le composant reliant les capteurs avec les act...   \n",
       "1  Le composant reliant les capteurs avec les act...   \n",
       "2  Le composant reliant les capteurs avec les act...   \n",
       "3  Le composant reliant les capteurs avec les act...   \n",
       "4  Le composant reliant les capteurs avec les act...   \n",
       "\n",
       "                                         r√©ponse_llm  iteration  Rater_Oli  \\\n",
       "0  La partie d'un robot qui relie les capteurs et...          1        1.0   \n",
       "1  La partie d'un robot qui relie les capteurs et...          2        NaN   \n",
       "2  La partie d'un robot qui relie les capteurs et...          3        NaN   \n",
       "3  La partie d'un robot qui relie les capteurs et...          4        NaN   \n",
       "4  La partie d'un robot qui relie les capteurs et...          5        NaN   \n",
       "\n",
       "   Invalid_Oli  Rater_chloe  Invalid_chloe  Rater_RA  Invalid_RA  \n",
       "0        False          1.0          False       0.0       False  \n",
       "1        False          NaN          False       NaN       False  \n",
       "2        False          NaN          False       NaN       False  \n",
       "3        False          NaN          False       NaN       False  \n",
       "4        False          NaN          False       NaN       False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define data directory\n",
    "data_dir = 'data/binary_user_case'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Define the path to your dataset\n",
    "data_file_path = os.path.join(data_dir, 'binary_data.xlsx')\n",
    "\n",
    "# Load the data\n",
    "data = load_data(data_file_path, file_type='xlsx', delimiter=';')\n",
    "\n",
    "# Preview the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    "The dataset used in this notebook contains **responses generated by a Large Language Model (LLM)**.  \n",
    "We designed 12 STEM exercises, for which we created two categories of prompts:\n",
    "\n",
    "- **Good prompts**: Intended to help the LLM generate a correct and relevant response to the exercise  \n",
    "- **Bad prompts**: Expected to produce responses that are **insufficient** or irrelevant for correctly answering the task\n",
    "\n",
    "The objective is to **evaluate the quality of LLM-generated responses** based on these varying prompt conditions.\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "The dataset contains the following key columns:\n",
    "\n",
    "- **`r√©ponse_attendue`**: A reference response considered correct (the expected answer)  \n",
    "- **`r√©ponse_llm`**: The actual response generated by the LLM ‚Äî this is what needs to be evaluated  \n",
    "- **`iteration`**: An identifier for the exercise-prompt combination\n",
    "\n",
    "To evaluate a given response, we compare the **LLM‚Äôs answer** to the **expected answer**.\n",
    "\n",
    "The dataset also includes ratings from **three independent human annotators**.  \n",
    "These annotations allow us to compute **inter-annotator agreement metrics**, helping assess the reliability of the labels and compare them with model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Preprocessing  (Optional, improve clarity and consistency of text data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Rename key columns**  \n",
    "   Give important columns more descriptive names (commented here)\n",
    "\n",
    "2. **Clean textual data**  \n",
    "   For each text column, run `clean_and_normalize(series)` to  \n",
    "   - trim leading/trailing spaces  \n",
    "   - convert accented characters to plain ASCII (e.g. `'√©'` ‚Üí `'e'`).\n",
    "\n",
    "3. **Convert to integers**  \n",
    "   Convert selected columns to integers using `pd.to_numeric(...).astype(\"Int64\")` to preserve missing values.\n",
    "\n",
    "4. **Sanitize line breaks**  \n",
    "   Run `sanitize_dataframe(df)` to replace newline (`\\n`) and carriage‚Äëreturn (`\\r`) characters with a single space in every string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a) Define a mapping from old column names to new names\n",
    "# rename_map = {\n",
    "#     \"r√©ponse_attendue\": \"expected_answer\",\n",
    "#     \"r√©ponse_llm\": \"llm_answer\"\n",
    "# }\n",
    "\n",
    "\n",
    "# # 1b) Rename the columns in the DataFrame\n",
    "# data = data.rename(columns=rename_map)\n",
    "\n",
    "# 1) Now define the new column names for cleaning\n",
    "text_columns = [\"r√©ponse_attendue\", \"r√©ponse_llm\"]\n",
    "integer_columns = [\"Code\", \"iteration\", \"Rater_Oli\", \"Rater_chloe\", \"Rater_RA\"]\n",
    "\n",
    "# 2) Clean and normalize the new columns\n",
    "for col in text_columns:\n",
    "    data[col] = clean_and_normalize(data[col])\n",
    "\n",
    "# 4) Convert selected columns to integers, preserving NaNs\n",
    "for col in integer_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# 5) Sanitize the DataFrame\n",
    "data = sanitize_dataframe(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Combine Texts and Questions\n",
    "\n",
    "To prepare the data for the LLM, we gather exactly the information a human annotator would need.\n",
    "The concatenated block of fields is called a **verbatim**.\n",
    "\n",
    "#### Create the `verbatim` field\n",
    "\n",
    "1. **Build verbatims**  \n",
    "   For every row we create a multi‚Äëline string containing:  \n",
    "    - **r√©ponse_attendue**: A reference response considered correct (the expected answer)   \n",
    "    - **r√©ponse_llm**: The actual response generated by the LLM ‚Äî this is what needs to be evaluated  \n",
    "   Each section is separated by a blank line for readability, and the result is written to a new column named `verbatim`.\n",
    "\n",
    "2. **Sanity‚Äëcheck**  \n",
    "   - Print the total number of verbatims to ensure every row was processed.  \n",
    "   - Display the first verbatim as a spot‚Äëcheck of the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of verbatims: 720\n",
      "Verbatim example:\n",
      "r√©ponse_attendue: Le composant reliant les capteurs avec les actionneurs est la carte de controle ou le microcontroleur.\n",
      "\n",
      "r√©ponse_llm: La partie d'un robot qui relie les capteurs et les moteurs pour aider a eviter les obstacles est generalement appelee le **systeme de controle** ou **unite de controle**. Ce systeme peut inclure un microcontroleur ou un ordinateur embarque qui recoit les donnees des capteurs (comme des capteurs de distance, des cameras, ou des capteurs infrarouges) et utilise ces informations pour prendre des decisions. Ensuite, il envoie des commandes aux moteurs pour ajuster la direction ou la vitesse du robot afin d'eviter les obstacles. Ce processus fait souvent partie d'un algorithme de navigation ou de traitement de signal qui permet au robot de reagir en temps reel a son environnement.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine texts and entries\n",
    "\n",
    "data['verbatim'] = data.apply(\n",
    "    lambda row: (\n",
    "        f\"r√©ponse_attendue: {row['r√©ponse_attendue']}\\n\\n\"\n",
    "        f\"r√©ponse_llm: {row['r√©ponse_llm']}\\n\\n\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Extract the list of verbatims\n",
    "verbatims = data['verbatim'].tolist()\n",
    "\n",
    "print(f\"Total number of verbatims: {len(verbatims)}\")\n",
    "print(f\"Verbatim example:\\n{verbatims[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt construction and classification on manually annotated data\n",
    "\n",
    "This framework allows you to evaluate different configurations to determine which prompt, model, and parameters yield the most accurate classification. These configurations are stored in the scenarios list.\n",
    "\n",
    "The snippet defines two **classification scenarios** for evaluating participants‚Äô ‚ÄúIdentify‚ÄØ‚Üí‚ÄØGuess‚ÄØ‚Üí‚ÄØSeek‚ÄØ‚Üí‚ÄØAssess‚Äù reasoning cycles with a Large Language Model (LLM).\n",
    "\n",
    "Each scenario is a dictionary inside the `scenarios` list and can be seen as a self‚Äëcontained _experiment_: it specifies\n",
    "\n",
    "* which LLM to call (`provider_llm1`, `model_name_llm1`, `temperature_llm1`);\n",
    "* the **prompt template** that tells the LLM how to judge a single data row;\n",
    "* the expected JSON output (fields listed in `selected_fields`);\n",
    "* optional settings for **prompt‚Äërefinement** by a second LLM (`provider_llm2`, ‚Ä¶).\n",
    "\n",
    "Running the pipeline iterates over every scenario and evaluates every (or a subsample of) data rows, then writes the chosen output fields back to your dataframe or file.\n",
    "\n",
    "### LLM Settings\n",
    "\n",
    "- `provider_llm1`: The LLM provider used for classification (`azure`, `openai`, `anthropic`, `gemini`)\n",
    "- `model_name_llm1`: The model used for classification. This depends on the provider.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "- **For** `azure` ‚Üí `\"gpt-4o\"` or `\"gpt-4o-mini\"`\n",
    "- **For** `openai` ‚Üí `\"gpt-4o\"` or `\"gpt-4o-mini\"`\n",
    "- **For** `anthropic` ‚Üí `\"claude-3-7-sonnet-20250219\"`, `\"claude-3-5-haiku-20241022\"`\n",
    "- **For** `gemini` ‚Üí `\"gemini-2.0-flash-001\"`, `\"gemini-2.5-pro-preview-03-25\"`\n",
    "\n",
    "- `temperature_llm1`: Controls output variability. Set to `0` for deterministic responses. Higher values add randomness (not recommended for evaluation tasks).\n",
    "- `subsample_size`: Number of entries to evaluate. Set to `-1` to use the entire dataset.\n",
    "\n",
    "### Prompt Configuration\n",
    "\n",
    "- `prompt_name`: A short name identifying the scenario, used in performance tracking.\n",
    "- `template`: The full prompt used to guide the LLM. It could include:\n",
    "  - The **role** of the assistant\n",
    "  - A **description** of the input columns\n",
    "  - The **evaluation codebook** (la mani√®re dont les donn√©es doivent etre classifi√©es)\n",
    "  - Optionally, **examples**\n",
    "  - ‚ö†Ô∏è **Must contain** the `{verbatim_text}` placeholder for the entry being evaluated\n",
    "\n",
    "### Output\n",
    "\n",
    "- `selected_fields`: The fields to extract from the LLM‚Äôs output (e.g., `\"Classification\"`, `\"Reasoning\"`).  \n",
    "  You can modify this to include or exclude elements (like adding confidence scores, removing reasonning).\n",
    "- `prefix`: The key to look for in the LLM output that contains the classification label (e.g., `\"Classification\"`).\n",
    "Nous sp√©cifions donc cela pour que le parsing du verdict soit plus facile, pour r√©cuperer les labels de classification.\n",
    "- `label_type`: Data type of the classification label. Typically `\"int\"` for binary classification (`0` or `1`),  \n",
    "  but can be changed to `\"float\"` or `\"str\"` as needed.\n",
    "- `response_template`: The required format of the LLM output (e.g., JSON). This ensures correct parsing. It is recommended not to change this format request.\n",
    "- `json_output`: If `True`, the LLM must respond in JSON. Disabling this is not recommended. If you do, you will have to  \n",
    "  change the `response_template` accordingly.\n",
    "\n",
    "\n",
    "### Prompt Optimization (In developpement - better not to change anything)\n",
    "\n",
    "This section enables **automatic prompt refinement** using a second LLM. It attempts to generate an improved version of the prompt to reduce classification errors.\n",
    "\n",
    "- A second model (`llm2`) is used to review the prompt given to the first model (`llm1`) and suggest changes based on classification failures.\n",
    "- If the new prompt performs better (fewer classification errors), it replaces the original.\n",
    "\n",
    "**Warning**: This can lead to overfitting ‚Äî the new prompt may work well on the training data but generalize poorly.  \n",
    "It's highly recommended to **use a validation set** when using this feature.\n",
    "\n",
    "### Prompt Optimization\n",
    "\n",
    "- `provider_llm2`: LLM provider used for prompt improvement\n",
    "- `model_name_llm2`: Name of the refinement model\n",
    "- `temperature_llm2`: Temperature for the prompt-refiner LLM\n",
    "- `max_iterations`: How many times the prompt should be revised.\n",
    "For example, if you choose 3, each data entry will be classified three times: once with the original prompt, and twice with newly generated prompts.\n",
    "- `use_validation_set`: Whether to use a separate validation set to monitor prompt overfitting (Boolean)\n",
    "- `validation_size`: Number of samples in the validation set\n",
    "- `random_state`: Random seed for reproducible train/validation split\n",
    "\n",
    "### Majority vote\n",
    "\n",
    "- `n_completions`: Number of completions per entry. \n",
    "It is possible to generate multiple responses for each entry using the same LLM. This will produce several classification labels for the same data point.\n",
    "The final label is determined by majority vote. Generating multiple completions can improve robustness but also increases cost.\n",
    "\n",
    "### Example\n",
    "\n",
    "In the current example, we define two scenarios:\n",
    "\n",
    "**Scenario 1**: One prompt in french.\n",
    "\n",
    "**Scenario 2**: One prompt in english (but data still in french)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "    {\n",
    "        # LLM settings\n",
    "        \"provider_llm1\": \"openrouter\",\n",
    "        \"model_name_llm1\": \"anthropic/claude-3.7-sonnet\",\n",
    "        \"temperature_llm1\": 0,\n",
    "        \"prompt_name\": \"french_prompt\",\n",
    "        \"subsample_size\": 20,  # Size of data subset to use\n",
    "\n",
    "        # Our initial prompt\n",
    "        \"template\": \"\"\"\n",
    "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
    "\n",
    "Les donn√©es comprennent les colonnes suivantes :\n",
    "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
    "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
    "- \"iteration\": Identifiant\n",
    "\n",
    "Voici une entr√©e √† √©valuer :\n",
    "{verbatim_text}\n",
    "\n",
    "T√¢che d‚Äô√©valuation :\n",
    "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
    "\n",
    "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
    "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
    "\"\"\",\n",
    "        # Output\n",
    "        \"selected_fields\": [\"Classification\", \"Reasoning\"],\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\":\n",
    "        \"\"\"\n",
    "S'il te plait, suis le format JSON ci-dessous :\n",
    "```json\n",
    "{{\n",
    "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
    "  \"Classification\": \"Ton integer ici\"\n",
    "}}\n",
    "\"\"\",\n",
    "        \"json_output\": True,\n",
    "\n",
    "        # Prompt optimization\n",
    "        \"provider_llm2\": \"azure\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "        \"max_iterations\": 1,\n",
    "        \"use_validation_set\": False,\n",
    "        \"validation_size\": 10,\n",
    "        \"random_state\": 42,\n",
    "\n",
    "        # Majority vote\n",
    "        \"n_completions\": 1,\n",
    "\n",
    "    },\n",
    "#         {\n",
    "#         # LLM settings\n",
    "#         \"provider_llm1\": \"azure\",\n",
    "#         \"model_name_llm1\": \"gpt-4o\",\n",
    "#         \"temperature_llm1\": 0,\n",
    "#         \"prompt_name\": \"english_prompt\",\n",
    "#         \"subsample_size\": 10,  # Size of data subset to use\n",
    "\n",
    "#         # Our initial prompt\n",
    "#         \"template\": \"\"\"\n",
    "# You are an assistant that evaluates data entries.\n",
    "\n",
    "# The data has the following columns:\n",
    "# - \"r√©ponse_attendue\":  An excerpt of a reference answer considered satisfactory\n",
    "# - \"r√©ponse_llm\": The answer provided by the LLM, to be evaluated\n",
    "# - \"iteration\": Identifier\n",
    "\n",
    "# Here is an entry to evaluate:\n",
    "# {verbatim_text}\n",
    "\n",
    "# Evaluation Task:\n",
    "# Evaluate whether the r√©ponse_llm adequately matches the r√©ponse_attendue using the following scale:\n",
    "\n",
    "# 0: The generated answer does not help or barely helps to answer the question (off-topic, incomplete, or incorrect, vague, too detailed, or not detailed enough).\n",
    "# 1: The generated answer answers the question.\n",
    "# \"\"\",\n",
    "#         # Output\n",
    "#         \"selected_fields\": [\"Classification\", \"Reasoning\"],\n",
    "#         \"prefix\": \"Classification\",\n",
    "#         \"label_type\": \"int\",\n",
    "#         \"response_template\":\n",
    "#         \"\"\"\n",
    "# Please follow the JSON format below:\n",
    "# ```json\n",
    "# {{\n",
    "#   \"Reasoning\": \"Your text here\",\n",
    "#   \"Classification\": \"Your integer here\"\n",
    "# }}\n",
    "# \"\"\",\n",
    "#         \"json_output\": True,\n",
    "\n",
    "#         # Prompt optimization\n",
    "#         \"provider_llm2\": \"azure\",\n",
    "#         \"model_name_llm2\": \"gpt-4o\",\n",
    "#         \"temperature_llm2\": 0.7,\n",
    "#         \"max_iterations\": 3,\n",
    "#         \"use_validation_set\": True,\n",
    "#         \"validation_size\": 5,\n",
    "#         \"random_state\": 42,\n",
    "\n",
    "#         # Majority vote\n",
    "#         \"n_completions\": 1,\n",
    "#     },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run the classification on Annotated Subset\n",
    "\n",
    "Before launching the classification on the entire dataset, we first run it on the subset that has been manually annotated.  \n",
    "This step allows us to compute performance metrics (e.g., **accuracy**, **F1-score**) by comparing LLM predictions to human labels,  \n",
    "and therefore select which (if any) scenario can be used to classify the full, unlabeled dataset.\n",
    "\n",
    "#### Configuration Parameters\n",
    "\n",
    "- `annotation_columns`: The names of the columns containing human annotations.\n",
    "- `labels`: The possible label values (in this case, `[0, 1]` for binary classification).\n",
    "\n",
    "We filter out any rows with missing values in the annotation columns to ensure we're only evaluating on fully labeled data.\n",
    "\n",
    "#### Repeated Runs for Stability\n",
    "\n",
    "LLMs are **stochastic** by nature ‚Äî even with a temperature of `0`, outputs can vary.  \n",
    "To assess how consistent the model is, we introduce the `n_runs` parameter:\n",
    "\n",
    "- `n_runs`: The number of times the classification is repeated for each scenario on the annotated data.\n",
    "\n",
    "We recommend setting `n_runs = 3`, based on findings from **[Paper XX]** (insert reference),  \n",
    "which showed that **three repetitions strike a good balance between stability and cost**.  \n",
    "Running more times improves statistical reliability but increases costs proportionally.\n",
    "\n",
    "#### `n_runs` vs `n_completions`\n",
    "\n",
    "It‚Äôs important to distinguish between these two concepts:\n",
    "\n",
    "- **`n_completions`**:  \n",
    "  Controls how many responses are generated **within a single run** for each data point.  \n",
    "  The final label is determined by **majority vote** over those completions.  \n",
    "  **Example**:  \n",
    "  If `n_completions = 3` and the model returns `[0, 0, 1]`, the selected label will be `0`.\n",
    "\n",
    "- **`n_runs`**:  \n",
    "  Repeats the **entire classification process** multiple times across the same data.  \n",
    "  If you run the scenario three times and get `[0, 0, 1]` for a given entry,  \n",
    "  that variation will be captured when calculating metrics (e.g., **variance**, **disagreement rate**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 'french_prompt' - Train size (all data): 20, No validation set\n",
      "\n",
      "=== Processing Verbatim 1/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Les plantes stockent de l'energie sous forme de sucres produits par la photosynthese durant la journee. La nuit, elles utilisent ces reserves pour maintenir leurs fonctions vitales, en degradant les sucres via la respiration cellulaire pour liberer l'energie necessaire. Ce processus leur permet de continuer a croitre et a se reparer meme en l'absence de lumiere solaire.\n",
      "\n",
      "r√©ponse_llm: Les plantes ont besoin d'energie pour effectuer leurs activites nocturnes, telles que la croissance, le developpement et la maintenance de leurs fonctions metaboliques. Puisqu'elles ne peuvent pas effectuer la photosynthese pendant la nuit, car cela necessite la lumiere du soleil, elles doivent trouver d'autres moyens pour obtenir l'energie necessaire.  Les plantes stockent l'energie produite pendant la journee sous forme de glucides, tels que l'amidon ou le saccharose, dans leurs feuilles, tiges et racines. Pendant la nuit, elles utilisent ces reserves d'energie pour alimenter leurs activites metaboliques.  Voici quelques-unes des facons dont les plantes utilisent l'energie stockee pour leurs activites nocturnes :  * La respiration cellulaire : les plantes utilisent l'energie stockee pour effectuer la respiration cellulaire, qui consiste a convertir les glucides en energie sous forme d'ATP (adenosine triphosphate). * La synthese de composes organiques : les plantes utilisent l'energie stockee pour synthetiser des composes organiques tels que les acides amines, les lipides et les vitamines. * La maintenance des fonctions metaboliques : les plantes utilisent l'energie stockee pour maintenir leurs fonctions metaboliques, telles que la regulation de la temperature, la regulation de l'eau et la defense contre les pathogenes.  En resume, les plantes utilisent l'energie stockee pendant la journee pour alimenter leurs activites nocturnes, en convertissant les glucides en energie sous forme d'ATP, en synthetisant des composes organiques et en maintenant leurs fonctions metaboliques.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse du LLM explique correctement comment les plantes obtiennent de l'√©nergie pendant la nuit. Elle mentionne les points cl√©s pr√©sents dans la r√©ponse attendue : (1) les plantes stockent l'√©nergie sous forme de glucides/sucres pendant la journ√©e, (2) elles utilisent ces r√©serves la nuit via la respiration cellulaire pour produire de l'ATP, et (3) cette √©nergie leur permet de maintenir leurs fonctions vitales et leur croissance en l'absence de lumi√®re. La r√©ponse du LLM est m√™me plus d√©taill√©e que la r√©ponse attendue, expliquant diff√©rentes utilisations de l'√©nergie stock√©e (synth√®se de compos√©s organiques, maintenance m√©tabolique, etc.). Tous les √©l√©ments essentiels de la r√©ponse attendue sont pr√©sents et correctement expliqu√©s.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1094\n",
      "Cost => 0.0065\n",
      "\n",
      "=== Processing Verbatim 2/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Lorsque le carburant est brule dans le moteur, son energie chimique est convertie en chaleur et en energie mecanique. Cette energie mecanique est ensuite transmise aux roues de la voiture, entrainant ainsi son mouvement. Le processus global implique donc une transformation d'energie chimique en energie thermique puis mecanique, propulsant ainsi le vehicule.\n",
      "\n",
      "r√©ponse_llm: Lors de la transformation du carburant en mouvement d'une voiture, plusieurs types d'energie sont impliques. Voici les principaux types d'energie en jeu :  * Energie chimique : le carburant (essence, diesel, etc.) contient de l'energie chimique qui est liberee lors de la combustion. * Energie thermique : la combustion du carburant produit de la chaleur, qui est convertie en energie thermique. * Energie mecanique : l'energie thermique est ensuite convertie en energie mecanique par le moteur, qui la transforme en mouvement rotatif. * Energie cinetique : le mouvement rotatif du moteur est transmis aux roues de la voiture, qui se mettent en mouvement, convertissant ainsi l'energie mecanique en energie cinetique. * Energie potentielle : lors de l'acceleration ou de la montee d'une pente, une partie de l'energie cinetique est convertie en energie potentielle, qui est stockee sous forme de hauteur ou de vitesse potentielle.  Ces types d'energie sont lies les uns aux autres par des processus de conversion, qui permettent de transformer l'energie chimique du carburant en mouvement de la voiture.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse_llm explique correctement la transformation d'√©nergie qui se produit lorsque le carburant est utilis√© pour faire avancer une voiture. Elle mentionne bien la conversion de l'√©nergie chimique (carburant) en √©nergie thermique (chaleur) puis en √©nergie m√©canique (mouvement rotatif du moteur) et finalement en √©nergie cin√©tique (mouvement de la voiture). La r√©ponse est m√™me plus compl√®te que la r√©ponse attendue car elle mentionne √©galement l'√©nergie potentielle. La r√©ponse couvre tous les aspects essentiels mentionn√©s dans la r√©ponse attendue et fournit m√™me des informations suppl√©mentaires pertinentes.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 926\n",
      "Cost => 0.0055\n",
      "\n",
      "=== Processing Verbatim 3/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Lorsque le carburant est brule dans le moteur, son energie chimique est convertie en chaleur et en energie mecanique. Cette energie mecanique est ensuite transmise aux roues de la voiture, entrainant ainsi son mouvement. Le processus global implique donc une transformation d'energie chimique en energie thermique puis mecanique, propulsant ainsi le vehicule.\n",
      "\n",
      "r√©ponse_llm: Le mouvement d'une voiture implique plusieurs formes d'energie :  1. **Energie chimique** : Dans le cas des voitures a moteur a combustion interne, l'energie chimique contenue dans le carburant (essence, diesel) est convertie en energie thermique lors de la combustion.  2. **Energie thermique** : Cette energie est produite par la combustion du carburant et est utilisee pour creer une pression dans les cylindres du moteur, ce qui entraine le mouvement des pistons.  3. **Energie mecanique** : L'energie thermique est transformee en energie mecanique lorsque les pistons du moteur se deplacent, entrainant le vilebrequin et, finalement, les roues de la voiture.  4. **Energie cinetique** : Lorsque la voiture se deplace, elle possede une energie cinetique, qui depend de sa masse et de sa vitesse. C'est l'energie du mouvement de la voiture.  5. **Energie potentielle** : Si la voiture est en hauteur (par exemple, sur une colline), elle possede egalement de l'energie potentielle gravitationnelle, qui peut etre convertie en energie cinetique lorsqu'elle descend.  6. **Energie electrique** : Dans le cas des voitures electriques ou hybrides, l'energie electrique stockee dans les batteries est convertie en energie mecanique pour faire tourner le moteur electrique.  Ces differentes formes d'energie interagissent pour permettre a la voiture de se deplacer efficacement.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue explique la conversion d'√©nergie dans une voiture: √©nergie chimique du carburant ‚Üí √©nergie thermique ‚Üí √©nergie m√©canique transmise aux roues. La r√©ponse du LLM couvre ces m√™mes transformations d'√©nergie (points 1, 2 et 3) mais va plus loin en expliquant √©galement d'autres formes d'√©nergie impliqu√©es (√©nergie cin√©tique, potentielle et √©lectrique). La r√©ponse du LLM est plus compl√®te et d√©taill√©e que la r√©ponse attendue, tout en incluant correctement les informations essentielles demand√©es sur la transformation d'√©nergie permettant le mouvement d'une voiture. Elle r√©pond donc ad√©quatement √† la question.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1031\n",
      "Cost => 0.0060\n",
      "\n",
      "=== Processing Verbatim 4/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Le code plastique le plus facile a recycler est le PET\n",
      "\n",
      "r√©ponse_llm: Si Emma souhaite acheter un jouet facile a recycler, elle devrait opter pour des jouets fabriques a partir de materiaux durables et recyclables, comme le bois, le carton ou certains plastiques recyclables. Voici quelques suggestions :  1. **Jouets en bois** : Ils sont souvent durables et peuvent etre recycles ou compostes a la fin de leur vie. 2. **Jouets en carton** : Les jouets en carton sont generalement recyclables et peuvent etre une option amusante et creative. 3. **Jouets en plastique recycle** : Certains fabricants produisent des jouets a partir de plastique recycle, ce qui reduit l'impact environnemental. 4. **Jouets educatifs** : Des jeux de societe ou des puzzles en materiaux recyclables peuvent egalement etre de bonnes options.  Il est egalement important de verifier les labels de recyclabilite sur l'emballage et de choisir des marques qui s'engagent en faveur de l'environnement.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue indique sp√©cifiquement que 'Le code plastique le plus facile √† recycler est le PET'. Cependant, la r√©ponse du LLM ne mentionne pas du tout le PET. Elle donne des conseils g√©n√©raux sur les jouets recyclables (bois, carton, plastique recycl√©) mais ne r√©pond pas √† la question sp√©cifique sur le code plastique le plus facilement recyclable. La r√©ponse est hors sujet par rapport √† ce qui √©tait attendu.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 769\n",
      "Cost => 0.0044\n",
      "\n",
      "=== Processing Verbatim 5/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: La faible gravite.\n",
      "\n",
      "r√©ponse_llm: La force responsable de l'incapacite des astronautes a marcher dans l'espace comme ils le font sur Terre est l'absence de gravite. Sur Terre, la gravite attire les objets vers le sol, ce qui permet aux humains de marcher en exercant une force contre le sol avec leurs pieds. Dans l'espace, en particulier en microgravite, cette force d'attraction est presque inexistante. En consequence, les astronautes flottent et ne peuvent pas utiliser la meme methode pour se deplacer. Ils doivent utiliser des techniques specifiques et des equipements adaptes pour se deplacer dans un environnement en microgravite.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue est simplement 'La faible gravit√©'. La r√©ponse du LLM explique correctement que c'est l'absence ou la tr√®s faible gravit√© (microgravit√©) qui est responsable de l'incapacit√© des astronautes √† marcher dans l'espace comme sur Terre. Bien que la r√©ponse du LLM soit beaucoup plus d√©taill√©e que la r√©ponse attendue, elle contient l'information essentielle et correcte. La r√©ponse du LLM permet donc de r√©pondre √† la question pos√©e.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 678\n",
      "Cost => 0.0043\n",
      "\n",
      "=== Processing Verbatim 6/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Les plantes stockent de l'energie sous forme de sucres produits par la photosynthese durant la journee. La nuit, elles utilisent ces reserves pour maintenir leurs fonctions vitales, en degradant les sucres via la respiration cellulaire pour liberer l'energie necessaire. Ce processus leur permet de continuer a croitre et a se reparer meme en l'absence de lumiere solaire.\n",
      "\n",
      "r√©ponse_llm: Les plantes continuent leurs activites chaque jour grace a plusieurs processus biologiques essentiels. Voici quelques-uns des principaux mecanismes :  1. **Photosynthese** : C'est le processus par lequel les plantes convertissent la lumiere du soleil, le dioxyde de carbone (CO2) de l'air et l'eau (H2O) du sol en glucose (un sucre) et en oxygene. Cela se produit principalement dans les feuilles, ou la chlorophylle capte la lumiere.  2. **Respiration** : Les plantes respirent egalement, ce qui signifie qu'elles utilisent l'oxygene pour decomposer le glucose et liberer de l'energie. Cette energie est necessaire pour la croissance, la reproduction et d'autres fonctions vitales.  3. **Transpiration** : Les plantes perdent de l'eau par un processus appele transpiration, qui se produit principalement par les stomates (petites ouvertures sur les feuilles). Cela aide a reguler la temperature de la plante et a tirer l'eau et les nutriments du sol.  4. **Croissance et developpement** : Les plantes continuent de croitre en produisant de nouvelles cellules, ce qui leur permet de s'etendre et de se developper. Cela se produit grace a des zones de croissance appelees meristemes.  5. **Reactions aux stimuli** : Les plantes reagissent egalement a leur environnement. Par exemple, elles peuvent orienter leurs feuilles vers la lumiere (phototropisme) ou developper des racines plus profondes en reponse a un manque d'eau.  Ces processus permettent aux plantes de survivre et de prosperer dans leur environnement, en s'adaptant aux conditions changeantes tout au long de la journee et des saisons.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue se concentre sp√©cifiquement sur comment les plantes stockent et utilisent l'√©nergie pendant la nuit, expliquant qu'elles stockent des sucres produits par photosynth√®se le jour et les utilisent la nuit via la respiration cellulaire. La r√©ponse du LLM, bien que d√©taill√©e et correcte sur plusieurs aspects de la physiologie des plantes (photosynth√®se, respiration, transpiration, croissance, r√©actions aux stimuli), ne r√©pond pas directement √† la question implicite sur le fonctionnement nocturne des plantes. Elle donne une vue d'ensemble des processus biologiques des plantes mais ne mentionne pas sp√©cifiquement le stockage d'√©nergie sous forme de sucres pour une utilisation nocturne, qui est le point central de la r√©ponse attendue.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1130\n",
      "Cost => 0.0065\n",
      "\n",
      "=== Processing Verbatim 7/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Lorsque le carburant est brule dans le moteur, son energie chimique est convertie en chaleur et en energie mecanique. Cette energie mecanique est ensuite transmise aux roues de la voiture, entrainant ainsi son mouvement. Le processus global implique donc une transformation d'energie chimique en energie thermique puis mecanique, propulsant ainsi le vehicule.\n",
      "\n",
      "r√©ponse_llm: Le mouvement d'une voiture implique plusieurs formes d'energie. Voici les principales :  1. **Energie chimique** : Cette energie est stockee dans le carburant (essence, diesel, etc.) ou dans les batteries (pour les vehicules electriques). Elle est liberee lors de la combustion du carburant dans le moteur a combustion interne ou lors de la decharge des batteries dans les moteurs electriques.  2. **Energie mecanique** : C'est l'energie associee au mouvement de la voiture. Elle se divise en deux types :    - **Energie cinetique** : C'est l'energie due au mouvement de la voiture. Elle est proportionnelle a la masse de la voiture et au carre de sa vitesse.    - **Energie potentielle** : C'est l'energie due a la position de la voiture, par exemple, lorsqu'elle est en haut d'une colline.  3. **Energie thermique** : Une partie de l'energie chimique est convertie en chaleur, notamment par la friction des pieces mobiles du moteur et des freins. Cette chaleur est souvent dissipee dans l'environnement.  4. **Energie electrique** : Dans les vehicules electriques, l'energie stockee dans les batteries est convertie en energie electrique qui alimente le moteur electrique. Dans les vehicules a combustion interne, l'energie electrique est utilisee pour alimenter divers systemes electroniques et electriques du vehicule (phares, radio, etc.).  5. **Energie sonore** : Le bruit produit par le moteur, les pneus et le vent est une forme d'energie sonore. Bien que cette energie soit generalement consideree comme une perte, elle est une consequence inevitable du mouvement de la voiture.  6. **Energie de deformation** : Lorsque les pneus roulent sur la route, ils se deforment legerement, stockant et liberant de l'energie de maniere cyclique. Cette energie est egalement une forme de perte, mais elle est necessaire pour le bon fonctionnement des pneus.  Ces differentes formes d'energie interagissent et se transforment les unes dans les autres pour permettre le mouvement de la voiture.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue explique sp√©cifiquement comment l'√©nergie chimique du carburant est convertie en chaleur et √©nergie m√©canique pour propulser la voiture. La r√©ponse du LLM va bien au-del√† de ce qui √©tait demand√© en fournissant une explication d√©taill√©e et compl√®te de toutes les formes d'√©nergie impliqu√©es dans le mouvement d'une voiture (chimique, m√©canique, thermique, √©lectrique, sonore, de d√©formation). Elle inclut l'information essentielle contenue dans la r√©ponse attendue (conversion de l'√©nergie chimique en √©nergie m√©canique) mais l'enrichit consid√©rablement avec des explications suppl√©mentaires. La r√©ponse du LLM est donc correcte et permet de r√©pondre √† la question, m√™me si elle est beaucoup plus d√©taill√©e que ce qui √©tait attendu.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1219\n",
      "Cost => 0.0069\n",
      "\n",
      "=== Processing Verbatim 8/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Lorsqu'un crayon est mis dans un verre d'eau, le changement de direction observe est du a un phenomene optique appele refraction. La refraction se produit lorsque la lumiere passe d'un milieu a un autre avec une densite differente, comme de l'air a l'eau. Lorsque la lumiere passe de l'air (le milieu exterieur) a l'eau (le milieu interieur du verre), sa vitesse et sa direction changent.\n",
      "\n",
      "r√©ponse_llm: Lorsqu'un crayon est mis dans un verre d'eau, il semble changer de direction en raison d'un phenomene optique appele refraction. La refraction se produit lorsque la lumiere passe d'un milieu a un autre, comme de l'air a l'eau, et change de direction en raison de la difference de vitesse de propagation de la lumiere dans les deux milieux.  Dans le cas du crayon dans l'eau, la lumiere qui vient du crayon passe de l'air a l'eau et est refractee, ce qui signifie qu'elle change de direction. Cela donne l'impression que le crayon est casse ou qu'il change de direction a la surface de l'eau.  Il est important de noter que ce phenomene n'est pas du a une propriete du crayon lui-meme, mais plutot a la facon dont la lumiere interagit avec les differents milieux. La refraction est un phenomene qui se produit dans de nombreux cas, comme lorsque la lumiere passe d'un milieu a un autre, et peut etre observe dans de nombreux contextes, comme dans les prismes, les lentilles ou les miroirs.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue concernant le ph√©nom√®ne de r√©fraction observ√© lorsqu'un crayon est mis dans un verre d'eau.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse_llm explique correctement que le ph√©nom√®ne observ√© lorsqu'un crayon est mis dans un verre d'eau est la r√©fraction. Elle pr√©cise que ce ph√©nom√®ne se produit lorsque la lumi√®re passe d'un milieu √† un autre (de l'air √† l'eau) et change de direction en raison de la diff√©rence de vitesse de propagation de la lumi√®re dans les deux milieux. La r√©ponse va m√™me plus loin que la r√©ponse attendue en expliquant l'effet visuel (impression que le crayon est cass√©) et en mentionnant d'autres contextes o√π la r√©fraction peut √™tre observ√©e. La r√©ponse_llm couvre tous les √©l√©ments essentiels de la r√©ponse attendue et fournit m√™me des informations suppl√©mentaires pertinentes.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1011\n",
      "Cost => 0.0064\n",
      "\n",
      "=== Processing Verbatim 9/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Ce qui a cree les 5 continents est le deplacement des plaques tectoniques\n",
      "\n",
      "r√©ponse_llm: Le phenomene qui a conduit a la Terre que nous connaissons aujourd'hui est la combinaison de plusieurs processus geologiques et astronomiques qui ont eu lieu sur des millions d'annees. Voici quelques-uns des principaux facteurs qui ont contribue a la formation de notre planete :  * La formation du systeme solaire : Il y a environ 4,6 milliards d'annees, une grande nebuleuse de gaz et de poussieres s'est effondree sous l'effet de la gravite, donnant naissance au Soleil et aux planetes, dont la Terre. * L'accretion : Les particules de poussieres et de gaz se sont regroupees pour former des planetesimes, qui ont ensuite fusionne pour creer la Terre. * La differenciation planetaire : La Terre s'est refroidie et a subi une differenciation, ce qui signifie que les elements les plus lourds, tels que les metaux, se sont regroupes au centre de la planete, tandis que les elements les plus legers, tels que les silicates, se sont regroupes a la surface. * Les processus geologiques : La Terre a subi des processus tels que la tectonique des plaques, l'erosion, la sedimentation et le volcanisme, qui ont faconne la surface de la planete et cree les montagnes, les oceans et les continents que nous connaissons aujourd'hui. * L'evolution de la vie : La vie a emerge sur Terre il y a environ 3,5 milliards d'annees et a evolue au fil du temps, conduisant a la diversite des especes que nous voyons aujourd'hui.  Ces processus ont interagi les uns avec les autres de maniere complexe pour creer la Terre que nous connaissons aujourd'hui, avec ses oceans, ses continents, ses montagnes et ses ecosystemes uniques.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue est concise et sp√©cifique: 'Ce qui a cr√©√© les 5 continents est le d√©placement des plaques tectoniques'. La r√©ponse du LLM est beaucoup plus large et couvre l'ensemble de la formation de la Terre, depuis la naissance du syst√®me solaire jusqu'√† l'√©volution de la vie. Bien que la r√©ponse du LLM mentionne la tectonique des plaques comme l'un des processus g√©ologiques qui ont fa√ßonn√© la Terre ('La Terre a subi des processus tels que la tectonique des plaques... qui ont fa√ßonn√© la surface de la plan√®te et cr√©√© les montagnes, les oc√©ans et les continents'), elle ne met pas sp√©cifiquement l'accent sur le fait que c'est le d√©placement des plaques tectoniques qui a cr√©√© les 5 continents. La r√©ponse est correcte mais trop g√©n√©rale et ne se concentre pas sur l'√©l√©ment pr√©cis demand√©.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1099\n",
      "Cost => 0.0068\n",
      "\n",
      "=== Processing Verbatim 10/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Glace : les molecules sont bien rangees Eau liquide : les molecules sont proches Eau gazeuse : les molecules sont tres eloignees\n",
      "\n",
      "r√©ponse_llm: Les molecules d'eau peuvent exister sous trois etats principaux : solide (glace), liquide (eau) et gazeux (vapeur d'eau). Voici comment les molecules sont arrangees pour chaque etat :  * Glace (solide) : Les molecules d'eau sont arrangees de maniere cristalline, avec des liaisons hydrogene fortes entre les molecules. Chaque molecule d'eau est liee a quatre autres molecules d'eau par des liaisons hydrogene, formant une structure tridimensionnelle rigide. * Eau (liquide) : Les molecules d'eau sont plus libres de se mouvoir et sont arrangees de maniere plus aleatoire. Les liaisons hydrogene sont toujours presentes, mais elles sont plus faibles et plus dynamiques que dans la glace. Les molecules d'eau peuvent se rapprocher ou s'eloigner les unes des autres, ce qui permet a l'eau de prendre la forme de son container. * Vapeur d'eau (gazeux) : Les molecules d'eau sont tres eloignees les unes des autres et sont en constant mouvement. Les liaisons hydrogene sont tres faibles et les molecules d'eau peuvent se deplacer librement dans l'espace. La vapeur d'eau est une collection de molecules d'eau isolees qui ne sont pas liees les unes aux autres de maniere significative.  En resume, les molecules d'eau sont arrangees de maniere de plus en plus aleatoire et de moins en moins liees a mesure que l'on passe de la glace a l'eau et enfin a la vapeur d'eau.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue est simple et concise, indiquant uniquement l'arrangement des mol√©cules dans les trois √©tats de l'eau: 'Glace: les molecules sont bien rangees. Eau liquide: les molecules sont proches. Eau gazeuse: les molecules sont tres eloignees.' La r√©ponse du LLM contient bien ces informations essentielles, mais va beaucoup plus loin en expliquant en d√©tail l'arrangement mol√©culaire dans chaque √©tat, les liaisons hydrog√®ne, et la dynamique des mol√©cules. Pour la glace, elle mentionne que les mol√©cules sont 'arrangees de maniere cristalline' (bien rang√©es). Pour l'eau liquide, elle indique que les mol√©cules sont 'plus libres de se mouvoir' mais toujours proches. Pour la vapeur d'eau, elle pr√©cise que les mol√©cules sont 'tres eloignees les unes des autres'. Bien que la r√©ponse du LLM soit beaucoup plus d√©taill√©e que la r√©ponse attendue, elle contient toutes les informations requises et est correcte scientifiquement.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1060\n",
      "Cost => 0.0071\n",
      "\n",
      "=== Processing Verbatim 11/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Les plantes stockent de l'energie sous forme de sucres produits par la photosynthese durant la journee. La nuit, elles utilisent ces reserves pour maintenir leurs fonctions vitales, en degradant les sucres via la respiration cellulaire pour liberer l'energie necessaire. Ce processus leur permet de continuer a croitre et a se reparer meme en l'absence de lumiere solaire.\n",
      "\n",
      "r√©ponse_llm: Les plantes ont besoin de plusieurs elements essentiels pour continuer leurs activites chaque jour. Voici quelques-uns des principaux facteurs qui leur permettent de survivre et de prosperer :  * La lumiere du soleil : les plantes utilisent la lumiere du soleil pour effectuer la photosynthese, un processus qui leur permet de convertir l'energie lumineuse en energie chimique sous forme de glucose. * L'eau : les plantes ont besoin d'eau pour effectuer la photosynthese, reguler leur temperature et transporter les nutriments et les dechets. * Les nutriments : les plantes ont besoin de nutriments tels que les elements mineraux (azote, phosphore, potassium, etc.) pour croitre et se developper. * Le dioxygene : les plantes ont besoin de dioxygene pour effectuer la respiration, un processus qui leur permet de convertir le glucose en energie. * La temperature : les plantes ont besoin d'une temperature appropriee pour effectuer leurs activites metaboliques.  Les plantes ont egalement developpe des mecanismes pour reguler leurs activites en fonction des conditions environnementales. Par exemple, elles peuvent :  * Reguler leur ouverture et leur fermeture des stomates (petites ouvertures sur les feuilles) pour controler la quantite d'eau et de dioxygene qui penetrent dans la plante. * Ajuster leur angle et leur orientation pour maximiser leur exposition a la lumiere du soleil. * Stocker les nutriments et l'eau pour les periodes de secheresse ou de carence.  En resume, les plantes ont developpe des mecanismes complexes pour continuer leurs activites chaque jour, en utilisant les ressources disponibles et en regulant leurs processus metaboliques en fonction des conditions environnementales.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue se concentre sp√©cifiquement sur comment les plantes g√®rent leur √©nergie pendant la nuit, en expliquant qu'elles stockent l'√©nergie sous forme de sucres pendant la journ√©e (via photosynth√®se) et utilisent ces r√©serves la nuit via la respiration cellulaire. La r√©ponse du LLM, bien que d√©taill√©e et correcte sur les besoins g√©n√©raux des plantes (lumi√®re, eau, nutriments, etc.), ne r√©pond pas directement √† la question implicite sur le fonctionnement nocturne des plantes. Elle mentionne la photosynth√®se et la respiration mais n'explique pas le m√©canisme de stockage d'√©nergie pendant le jour et son utilisation la nuit, qui est le point central de la r√©ponse attendue.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1101\n",
      "Cost => 0.0063\n",
      "\n",
      "=== Processing Verbatim 12/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Le composant reliant les capteurs avec les actionneurs est la carte de controle ou le microcontroleur.\n",
      "\n",
      "r√©ponse_llm: La partie d'un robot qui relie les capteurs et les moteurs entre eux pour aider a eviter les obstacles est le systeme de controle ou l'unite de controle. Cependant, plus specifiquement, il s'agit souvent du microcontroleur ou de l'ordinateur de bord qui execute les algorithmes de controle et de navigation.  Ces algorithmes utilisent les donnees provenant des capteurs (tels que des capteurs ultrasonores, des capteurs a infrarouge, des cameras, etc.) pour detecter les obstacles et calculer les trajectoires a suivre pour les eviter. Les resultats de ces calculs sont ensuite transmis aux moteurs pour ajuster la direction et la vitesse du robot, permettant ainsi une navigation autonome et evitant les collisions.  En resume, c'est l'ensemble du systeme de controle, incluant le logiciel et le materiel, qui joue un role crucial dans la coordination des capteurs et des moteurs pour eviter les obstacles.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue indique que 'le composant reliant les capteurs avec les actionneurs est la carte de contr√¥le ou le microcontr√¥leur'. La r√©ponse du LLM mentionne bien le microcontr√¥leur comme √©l√©ment reliant les capteurs et les moteurs (qui sont des actionneurs), en l'identifiant comme faisant partie du 'syst√®me de contr√¥le' ou 'unit√© de contr√¥le'. La r√©ponse du LLM est plus d√©taill√©e et explique le fonctionnement de ce syst√®me, mais elle contient l'information essentielle demand√©e. Elle identifie correctement que c'est le microcontr√¥leur qui fait le lien entre les capteurs et les actionneurs (moteurs).\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 815\n",
      "Cost => 0.0053\n",
      "\n",
      "=== Processing Verbatim 13/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Cette limitation est due a la sensibilite des cellules photoreceptrices de nos yeux (les cones et les batonnets) qui ne reagissent qua une gamme precise de longueurs donde, environ entre 400 et 700 nanometres.\n",
      "\n",
      "r√©ponse_llm: Notre incapacite a voir tout le spectre electromagnetique est due a la facon dont notre systeme visuel est concu. Les yeux humains sont capables de detecter les longueurs d'onde qui se situent dans la plage de la lumiere visible, qui va approximativement de 380 nanometres (violet) a 780 nanometres (rouge). Cette plage est tres etroite par rapport a l'ensemble du spectre electromagnetique, qui inclut les rayons gamma, les rayons X, les ultraviolets, la lumiere visible, les infrarouges, les micro-ondes et les ondes radio.  Il y a plusieurs raisons pour lesquelles nous ne pouvons pas voir tout le spectre :  * **Limitations physiologiques** : La structure de nos yeux et de notre cerveau est adaptee pour traiter les informations visuelles dans la plage de la lumiere visible. Les cellules photoreceptrices de la retine (les batonnets et les cones) sont sensibles aux longueurs d'onde de la lumiere visible, mais pas aux autres parties du spectre. * **Proprietes des ondes electromagnetiques** : Les differentes parties du spectre electromagnetique ont des proprietes physiques distinctes, comme la longueur d'onde et la frequence, qui les rendent plus ou moins perceptibles par nos sens. Par exemple, les rayons gamma et les rayons X ont des longueurs d'onde tres courtes et des energies tres elevees, ce qui les rend dangereux pour les tissus vivants, mais egalement imperceptibles a nos yeux. * **Interaction avec la matiere** : La facon dont les ondes electromagnetiques interagissent avec la matiere depend de leur longueur d'onde et de leur energie. Certaines parties du spectre, comme les infrarouges, sont absorbees ou reflechies par les objets de maniere differente de la lumiere visible, ce qui les rend imperceptibles a nos yeux.  En resume, notre incapacite a voir tout le spectre electromagnetique est due a la combinaison de facteurs physiologiques, physiques et lies a l'interaction avec la matiere. Cependant, grace a la technologie et aux instruments scientifiques, nous pouvons detecter et etudier les differentes parties du spectre electromagnetique, meme si nous ne pouvons pas les voir directement.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue indique simplement que notre limitation visuelle est due √† la sensibilit√© des cellules photor√©ceptrices (c√¥nes et b√¢tonnets) qui ne r√©agissent qu'√† une gamme pr√©cise de longueurs d'onde entre 400 et 700 nanom√®tres. La r√©ponse du LLM couvre cette information de base (mentionnant les photor√©cepteurs, les b√¢tonnets et les c√¥nes, et la plage de longueurs d'onde visible), mais va beaucoup plus loin en expliquant en d√©tail les raisons physiologiques, les propri√©t√©s des ondes √©lectromagn√©tiques et leur interaction avec la mati√®re. La r√©ponse du LLM est plus compl√®te et d√©taill√©e que la r√©ponse attendue, mais elle contient l'information essentielle demand√©e. Elle r√©pond donc ad√©quatement √† la question.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1214\n",
      "Cost => 0.0068\n",
      "\n",
      "=== Processing Verbatim 14/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Lorsqu'un medecin tapote le genou d'un patient lors d'un examen medical, il evalue le reflexe rotulien ou reflexe patellaire. Ce test permet d'evaluer la fonction du systeme nerveux, en particulier les voies nerveuses impliquees dans les reflexes.\n",
      "\n",
      "r√©ponse_llm: Le test medical peut fournir au medecin une multitude d'informations precieuses sur l'etat de sante d'un patient. Voici quelques exemples de ce que les tests medicaux peuvent indiquer :  1. **Diagnostic de maladies** : Les tests peuvent aider a identifier des maladies specifiques, telles que des infections, des maladies auto-immunes, des troubles metaboliques, ou des cancers.  2. **Evaluation de la fonction organique** : Les tests peuvent evaluer la fonction de divers organes, comme le foie, les reins, le cur, et les poumons.  3. **Surveillance de la sante** : Les tests reguliers peuvent aider a surveiller l'evolution de certaines conditions chroniques, comme le diabete, l'hypertension, ou les maladies cardiovasculaires.  4. **Detection de carences nutritionnelles** : Les tests peuvent reveler des carences en vitamines, mineraux, ou autres nutriments essentiels.  5. **Evaluation de la reponse au traitement** : Les tests peuvent aider a determiner si un traitement medical est efficace ou s'il doit etre ajuste.  6. **Prediction des risques futurs** : Certains tests peuvent identifier des facteurs de risque pour des maladies futures, permettant ainsi des interventions preventives.  7. **Detection de marqueurs genetiques** : Les tests genetiques peuvent reveler des predispositions a certaines maladies hereditaires.  8. **Evaluation de la sante mentale** : Bien que moins courants, certains tests peuvent aider a evaluer des conditions mentales comme la depression ou l'anxiete.  9. **Detection de substances** : Les tests peuvent detecter la presence de drogues, d'alcool, ou d'autres substances dans le corps.  10. **Evaluation de la fertilite** : Les tests peuvent aider a evaluer la fertilite chez les hommes et les femmes.  Les resultats des tests medicaux sont souvent interpretes dans le contexte des symptomes, de l'historique medical du patient, et d'autres examens cliniques. Cela permet au medecin de formuler un diagnostic precis et de recommander un plan de traitement approprie.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue est sp√©cifique et porte sur le r√©flexe rotulien (ou patellaire) que le m√©decin √©value en tapotant le genou d'un patient. Elle explique que ce test √©value la fonction du syst√®me nerveux, particuli√®rement les voies nerveuses impliqu√©es dans les r√©flexes. En revanche, la r√©ponse du LLM est tr√®s g√©n√©rale et pr√©sente une liste de 10 types d'informations que les tests m√©dicaux peuvent fournir, sans jamais mentionner sp√©cifiquement le r√©flexe rotulien ou l'examen du genou. La r√©ponse du LLM est hors sujet par rapport √† la question sp√©cifique qui semble avoir √©t√© pos√©e sur le test du r√©flexe rotulien.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1170\n",
      "Cost => 0.0065\n",
      "\n",
      "=== Processing Verbatim 15/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Le composant reliant les capteurs avec les actionneurs est la carte de controle ou le microcontroleur.\n",
      "\n",
      "r√©ponse_llm: La partie d'un robot qui relie les capteurs et les moteurs entre eux pour aider a eviter les obstacles est generalement le **systeme de controle** ou le **systeme de navigation**. Ce systeme peut inclure plusieurs composants cles :  1. **Microcontroleur ou Microprocesseur** : Le cerveau du robot, qui traite les donnees des capteurs et envoie des commandes aux moteurs. 2. **Capteurs** : Dispositifs qui detectent l'environnement autour du robot, tels que des capteurs ultrasoniques, infrarouges, ou des cameras. 3. **Moteurs** : Composants qui permettent au robot de se deplacer. 4. **Algorithmes de Navigation** : Logiciels ou programmes qui utilisent les donnees des capteurs pour prendre des decisions sur la direction et la vitesse du robot. 5. **Interface de Communication** : Protocoles et circuits qui permettent aux capteurs et aux moteurs de communiquer avec le microcontroleur.  Voici un exemple simplifie de comment ces composants peuvent interagir dans un robot de base :  1. **Capteurs** : Les capteurs detectent un obstacle a proximite. 2. **Microcontroleur** : Le microcontroleur recoit les donnees des capteurs. 3. **Algorithme de Navigation** : Le microcontroleur execute un algorithme pour determiner la meilleure action a prendre (par exemple, tourner a gauche ou a droite). 4. **Moteurs** : Le microcontroleur envoie des commandes aux moteurs pour effectuer l'action determinee.  Un exemple de code simple en Python pour un robot utilisant un microcontroleur comme Arduino pourrait ressembler a ceci :  ```python import time import RPi.GPIO as GPIO  # Configuration des broches TRIG = 23 ECHO = 24 MOTOR_PIN1 = 17 MOTOR_PIN2 = 27  # Initialisation des broches GPIO.setmode(GPIO.BCM) GPIO.setup(TRIG, GPIO.OUT) GPIO.setup(ECHO, GPIO.IN) GPIO.setup(MOTOR_PIN1, GPIO.OUT) GPIO.setup(MOTOR_PIN2, GPIO.OUT)  def mesure_distance():     GPIO.output(TRIG, True)     time.sleep(0.00001)     GPIO.output(TRIG, False)      while GPIO.input(ECHO) == 0:         pulse_start = time.time()      while GPIO.input(ECHO) == 1:         pulse_end = time.time()      pulse_duration = pulse_end - pulse_start     distance = pulse_duration * 17150     return distance  def eviter_obstacle():     distance = mesure_distance()     if distance < 10:  # Si l'obstacle est a moins de 10 cm         GPIO.output(MOTOR_PIN1, GPIO.HIGH)         GPIO.output(MOTOR_PIN2, GPIO.LOW)         time.sleep(0.5)  # Tourner a gauche pendant 0.5 seconde         GPIO.output(MOTOR_PIN1, GPIO.LOW)         GPIO.output(MOTOR_PIN2, GPIO.HIGH)         time.sleep(0.5)  # Tourner a droite pendant 0.5 seconde     else:         GPIO.output(MOTOR_PIN1, GPIO.HIGH)         GPIO.output(MOTOR_PIN2, GPIO.HIGH)  # Avancer  try:     while True:         eviter_obstacle() except KeyboardInterrupt:     GPIO.cleanup() ```  Ce code est un exemple simplifie et peut necessiter des ajustements en fonction du materiel specifique utilise.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue indique simplement que 'le composant reliant les capteurs avec les actionneurs est la carte de contr√¥le ou le microcontr√¥leur'. La r√©ponse du LLM identifie correctement le microcontr√¥leur comme l'un des composants cl√©s qui relie les capteurs et les moteurs (actionneurs). Elle mentionne sp√©cifiquement dans le point 1 que le 'Microcontr√¥leur ou Microprocesseur' est 'le cerveau du robot, qui traite les donn√©es des capteurs et envoie des commandes aux moteurs'. La r√©ponse va m√™me au-del√† en expliquant le syst√®me complet avec des d√©tails sur le fonctionnement, des exemples de code, et d'autres composants. Bien que la r√©ponse soit beaucoup plus d√©taill√©e que n√©cessaire, elle contient l'information essentielle demand√©e et est donc correcte.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1506\n",
      "Cost => 0.0079\n",
      "\n",
      "=== Processing Verbatim 16/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Lorsqu'un medecin tapote le genou d'un patient lors d'un examen medical, il evalue le reflexe rotulien ou reflexe patellaire. Ce test permet d'evaluer la fonction du systeme nerveux, en particulier les voies nerveuses impliquees dans les reflexes.\n",
      "\n",
      "r√©ponse_llm: Un test medical peut indiquer au medecin une variete d'informations importantes pour diagnostiquer et traiter les patients. Voici quelques exemples :  * La presence d'une maladie ou d'une infection : les tests medicaux peuvent detecter des marqueurs specifiques d'une maladie, tels que des anticorps ou des antigenes, pour confirmer un diagnostic. * Le niveau de fonctionnement d'un organe : les tests medicaux peuvent mesurer la fonction renale, la fonction hepatique, la fonction cardiaque, etc. * La presence de substances toxiques : les tests medicaux peuvent detecter la presence de substances toxiques, telles que des drogues, des metaux lourds ou des pesticides, dans le sang ou les urines. * La reponse a un traitement : les tests medicaux peuvent evaluer l'efficacite d'un traitement en mesurant les niveaux de medicaments dans le sang ou en evaluant les changements dans les parametres physiologiques. * La presence de facteurs de risque : les tests medicaux peuvent identifier les facteurs de risque pour certaines maladies, tels que le diabete, l'hypercholesterolemie ou l'hypertension.  Les tests medicaux peuvent prendre differentes formes, notamment :  * Les analyses de sang : pour mesurer les niveaux de substances chimiques, telles que les electrolytes, les enzymes, les hormones, etc. * Les analyses d'urine : pour detecter la presence de substances anormales, telles que des bacteries, des leukocytes, des proteines, etc. * Les imageries medicales : pour visualiser les structures internes du corps, telles que les radiographies, les echographies, les scanners, etc. * Les tests de fonction : pour evaluer la fonction d'un organe ou d'un systeme, telles que les tests de fonction pulmonaire, les tests de fonction cardiaque, etc.  En resume, les tests medicaux sont des outils essentiels pour les medecins pour diagnostiquer, traiter et prevenir les maladies. Ils peuvent fournir des informations precieuses sur la sante d'un patient et aider les medecins a prendre des decisions eclairees pour son traitement.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue est sp√©cifique et porte sur ce que le m√©decin √©value lorsqu'il tapote le genou d'un patient (le r√©flexe rotulien/patellaire) et l'objectif de ce test (√©valuer la fonction du syst√®me nerveux). La r√©ponse du LLM, en revanche, est tr√®s g√©n√©rale et parle des tests m√©dicaux en g√©n√©ral, leurs diff√©rents types et leurs objectifs. Elle ne mentionne pas du tout le r√©flexe rotulien ou le test sp√©cifique du tapotement du genou. La r√©ponse du LLM est compl√®tement hors sujet par rapport √† la question sp√©cifique qui semble avoir √©t√© pos√©e.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1146\n",
      "Cost => 0.0061\n",
      "\n",
      "=== Processing Verbatim 17/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Lorsqu'un medecin tapote le genou d'un patient lors d'un examen medical, il evalue le reflexe rotulien ou reflexe patellaire. Ce test permet d'evaluer la fonction du systeme nerveux, en particulier les voies nerveuses impliquees dans les reflexes.\n",
      "\n",
      "r√©ponse_llm: Lorsqu'un medecin tapote le genou d'un patient lors d'un examen medical, il evalue generalement le reflexe rotulien. Ce reflexe est un exemple de reflexe myotatique, qui est un reflexe inne et involontaire. Le tapotement du tendon rotulien (juste sous la rotule) provoque une contraction du muscle quadriceps, ce qui entraine une extension rapide de la jambe. Ce test permet au medecin d'evaluer la fonction des nerfs et des muscles impliques dans ce reflexe, ainsi que la sante du systeme nerveux central et peripherique.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond ad√©quatement √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse_llm couvre tous les √©l√©ments essentiels de la r√©ponse attendue et va m√™me plus loin. Elle identifie correctement que le m√©decin √©value le r√©flexe rotulien (ou patellaire), explique que ce test permet d'√©valuer la fonction du syst√®me nerveux, et ajoute des d√©tails pertinents sur le m√©canisme du r√©flexe (contraction du quadriceps, extension de la jambe) et pr√©cise qu'il s'agit d'un r√©flexe myotatique inn√© et involontaire. La r√©ponse est plus d√©taill√©e que la r√©ponse attendue mais reste correcte et pertinente.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 760\n",
      "Cost => 0.0049\n",
      "\n",
      "=== Processing Verbatim 18/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Lorsque le carburant est brule dans le moteur, son energie chimique est convertie en chaleur et en energie mecanique. Cette energie mecanique est ensuite transmise aux roues de la voiture, entrainant ainsi son mouvement. Le processus global implique donc une transformation d'energie chimique en energie thermique puis mecanique, propulsant ainsi le vehicule.\n",
      "\n",
      "r√©ponse_llm: La transformation du carburant en mouvement d'une voiture implique plusieurs types d'energie :  1. **Energie chimique** : Le carburant (comme l'essence ou le diesel) contient de l'energie chimique qui est liberee lors de la combustion dans le moteur.  2. **Energie thermique** : Lors de la combustion, l'energie chimique est convertie en energie thermique (chaleur). Cette chaleur augmente la temperature des gaz dans les cylindres du moteur.  3. **Energie mecanique** : L'energie thermique generee par la combustion pousse les pistons du moteur, ce qui transforme l'energie thermique en energie mecanique. Cette energie est ensuite utilisee pour faire tourner le vilebrequin.  4. **Energie cinetique** : Finalement, l'energie mecanique est transmise aux roues de la voiture, ce qui produit un mouvement, ou energie cinetique, permettant a la voiture de se deplacer.  Ces transformations d'energie sont essentielles au fonctionnement d'un vehicule a moteur a combustion interne.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse_llm couvre tous les √©l√©ments essentiels de la r√©ponse attendue et va m√™me plus loin. Elle explique clairement la transformation de l'√©nergie chimique du carburant en √©nergie thermique lors de la combustion, puis en √©nergie m√©canique qui est transmise aux roues. La r√©ponse du LLM est plus d√©taill√©e et structur√©e, mentionnant explicitement l'√©nergie cin√©tique comme √©tape finale, ce qui est implicite dans la r√©ponse attendue. La r√©ponse est correcte, bien organis√©e et fournit une explication compl√®te du processus de transformation d'√©nergie dans une voiture.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 889\n",
      "Cost => 0.0053\n",
      "\n",
      "=== Processing Verbatim 19/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Les fossiles plus anciens se trouvent le plus souvent dans la couche rocheuse la plus basse\n",
      "\n",
      "r√©ponse_llm: Les fossiles les plus anciens se trouvent generalement dans les couches les plus profondes de la croute terrestre, qui est la couche externe solide de la Terre. La croute terrestre est divisee en deux types principaux : la croute continentale et la croute oceanique. Les fossiles les plus anciens sont souvent trouves dans les roches sedimentaires de la croute continentale, qui peuvent etre tres anciennes.  Les roches sedimentaires qui contiennent des fossiles peuvent etre trouvees a des profondeurs variables, mais elles sont generalement exposees a la surface ou a des profondeurs relativement peu profondes dans la croute terrestre. Par exemple, les roches sedimentaires du Precambrien, qui datent de plus de 541 millions d'annees, peuvent contenir des fossiles tres anciens et sont souvent trouvees a des profondeurs de quelques centaines de metres a quelques kilometres sous la surface.  Il est important de noter que la profondeur a laquelle les fossiles sont trouves peut varier en fonction de la geologie locale et des processus tectoniques qui ont affecte la region au fil du temps. Les fossiles peuvent etre deplaces et enfouis a des profondeurs plus grandes par des processus tels que la subduction, l'erosion et la sedimentation.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue indique simplement que 'Les fossiles plus anciens se trouvent le plus souvent dans la couche rocheuse la plus basse'. La r√©ponse du LLM contient bien cette information dans sa premi√®re phrase ('Les fossiles les plus anciens se trouvent generalement dans les couches les plus profondes de la croute terrestre'), ce qui correspond √† l'id√©e principale de la r√©ponse attendue. Bien que la r√©ponse du LLM soit beaucoup plus d√©taill√©e et √©labor√©e, avec des explications sur les types de cro√ªte terrestre, les roches s√©dimentaires et les processus g√©ologiques, elle inclut l'information essentielle demand√©e. La r√©ponse est donc correcte et permet de r√©pondre √† la question.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 899\n",
      "Cost => 0.0056\n",
      "\n",
      "=== Processing Verbatim 20/20 ===\n",
      "Prompt:\n",
      "\n",
      "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
      "\n",
      "Les donn√©es comprennent les colonnes suivantes :\n",
      "- \"r√©ponse_attendue\": Un passage de r√©ponse consid√©r√©e comme satisfaisante\n",
      "- \"r√©ponse_llm\": La r√©ponse fournie par le LLM, √† juger\n",
      "- \"iteration\": Identifiant\n",
      "\n",
      "Voici une entr√©e √† √©valuer :\n",
      "r√©ponse_attendue: Cette limitation est due a la sensibilite des cellules photoreceptrices de nos yeux (les cones et les batonnets) qui ne reagissent qua une gamme precise de longueurs donde, environ entre 400 et 700 nanometres.\n",
      "\n",
      "r√©ponse_llm: La capacite des humains a percevoir la lumiere et les couleurs est limitee par la structure de nos yeux et la maniere dont notre cerveau interprete les signaux visuels. Voici quelques raisons pour lesquelles nous ne pouvons pas voir tout le spectre electromagnetique :  1. **Recepteurs Visuels** : Les yeux humains possedent trois types de photorecepteurs appeles cones, qui sont sensibles a differentes longueurs d'onde de la lumiere. Ces cones sont principalement sensibles aux longueurs d'onde correspondant aux couleurs rouge, vert et bleu. Cependant, il existe d'autres longueurs d'onde dans le spectre electromagnetique (comme les ultraviolets et les infrarouges) auxquelles nos cones ne sont pas sensibles.  2. **Spectre Visible** : Le spectre visible pour les humains se situe approximativement entre 380 et 740 nanometres (nm). En dehors de cette plage, nous ne pouvons pas percevoir la lumiere. Par exemple, les ultraviolets (en dessous de 380 nm) et les infrarouges (au-dessus de 740 nm) sont invisibles pour nous.  3. **Evolution** : Nos yeux se sont developpes pour percevoir les longueurs d'onde qui sont les plus utiles pour notre survie et notre interaction avec l'environnement. Les longueurs d'onde visibles sont celles qui sont principalement reflechies par le soleil et les objets de notre environnement, ce qui nous permet de voir clairement et de naviguer dans notre monde.  4. **Technologie** : Pour voir des parties du spectre electromagnetique en dehors du visible, nous utilisons des technologies specialisees. Par exemple, les cameras infrarouges peuvent detecter la chaleur, et les cameras ultraviolettes peuvent capturer des images dans cette partie du spectre.  En resume, notre incapacite a voir tout le spectre electromagnetique est due a la limitation de nos recepteurs visuels et a l'evolution de notre systeme visuel pour repondre aux besoins de notre environnement naturel.\n",
      "\n",
      "\n",
      "\n",
      "T√¢che d‚Äô√©valuation :\n",
      "√âvaluer si la r√©ponse_llm r√©pond ad√©quatement, c'est √† dire qu'elle correspond √† la r√©ponse attendue, en utilisant l‚Äô√©chelle suivante :\n",
      "\n",
      "0 : La r√©ponse g√©n√©r√©e ne permet pas ou difficilement de r√©pondre √† la question pos√©e (hors sujet, incompl√®te ou incorrecte, vague, trop ou pas assez d√©taill√©e).\n",
      "1 : La r√©ponse g√©n√©r√©e permet de r√©pondre √† la question.\n",
      "\n",
      "\n",
      "\n",
      "S'il te plait, suis le format JSON ci-dessous :\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
      "  \"Classification\": \"Ton integer ici\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "Je vais √©valuer si la r√©ponse du LLM correspond √† la r√©ponse attendue.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Raisonnement\": \"La r√©ponse attendue indique sp√©cifiquement que la limitation de notre vision est due √† la sensibilit√© des cellules photor√©ceptrices (c√¥nes et b√¢tonnets) qui ne r√©agissent qu'√† une gamme de longueurs d'onde entre 400 et 700 nanom√®tres. La r√©ponse du LLM couvre bien ce point dans sa section 'Spectre Visible' en mentionnant que le spectre visible pour les humains se situe entre 380 et 740 nanom√®tres, ce qui est tr√®s proche de la plage mentionn√©e dans la r√©ponse attendue. De plus, la r√©ponse du LLM mentionne √©galement les photor√©cepteurs (c√¥nes) dans sa section 'R√©cepteurs Visuels'. La r√©ponse du LLM est plus d√©taill√©e et fournit des informations suppl√©mentaires sur l'√©volution et la technologie, mais elle contient l'information essentielle demand√©e dans la r√©ponse attendue.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1189\n",
      "Cost => 0.0071\n",
      "\n",
      "Columns in detailed_results_df:\n",
      "['sample_id', 'split', 'verbatim', 'iteration', 'Rater_Oli', 'Rater_chloe', 'Rater_RA', 'ModelPrediction', 'Raisonnement', 'run', 'prompt_name', 'use_validation_set']\n"
     ]
    }
   ],
   "source": [
    "# 9) Run scenarios and get results\n",
    "\n",
    "annotation_columns = [\"Rater_Oli\", \"Rater_chloe\", \"Rater_RA\"]\n",
    "labels = [0,1,2]\n",
    "\n",
    "# Filter labeled data (drop rows with NaN in any annotation column)\n",
    "labeled_data = data.dropna(subset=annotation_columns)\n",
    "unlabeled_data = data[~data.index.isin(labeled_data.index)]\n",
    "\n",
    "n_runs = 1  # Number of runs per scenario\n",
    "verbose = True  # Whether to print verbose output\n",
    "\n",
    "# Run the scenarios - this only runs the LLM and saves all the generated labels\n",
    "complex_case_for_metrics = run_scenarios(\n",
    "    scenarios=scenarios,\n",
    "    data=labeled_data,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    n_runs=n_runs,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Saving / Re-Loading the Results\n",
    "\n",
    "This step provides an option to save the classification results to a file for future reference or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possibility to save the results\n",
    "\n",
    "# Save the annotated results to a CSV file\n",
    "complex_case_for_metrics.to_csv(\"data/binary_user_case/outputs/binary_case_for_metrics.csv\", sep=\";\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, load the annotated results from the CSV file if needed\n",
    "\n",
    "complex_case_for_metrics = pd.read_csv(\n",
    "    \"data/binary_user_case/outputs/binary_case_for_metrics.csv\",\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8-sig\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance Against Human Annotations\n",
    "\n",
    "To determine whether the model's classification is reliable and can be used to annotate the rest of the unlabeled dataset,  \n",
    "it is recommended to evaluate its alignment with human annotations.  \n",
    "If the alignment is sufficiently high, you may choose to rely on the model-generated labels for the remaining data.\n",
    "\n",
    "We propose **four types of analysis**, depending on your goals:\n",
    "\n",
    "- **If you want to measure agreement between annotators**:  \n",
    "  Use **Cohen's Kappa**, a simple and widely used metric for inter-rater agreement.\n",
    "\n",
    "- **If you need detailed per-class performance metrics** (e.g., recall, true positives, false positives):  \n",
    "  Use **Classification Metrics**. This method gives a descriptive breakdown of model performance by class.\n",
    "\n",
    "- **If you have multiple manual annotations and want a more robust estimate**:  \n",
    "  Use **Krippendorff's Alpha**. This method provides:\n",
    "  - A confidence interval for the agreement, computed via bootstrapping\n",
    "  - An estimate of the risk that the true alpha value lies outside this interval\n",
    "\n",
    "- **If you have multiple annotation columns (‚â• 3)** and want to assess whether the model can \"replace\" or **outperform individual annotators**,  \n",
    "  and you can afford to annotate 50‚Äì100 entries:  \n",
    "  Use the **Alt-Test**. This stricter test compares the model to each annotator using a **leave-one-out** approach.\n",
    "\n",
    "Among the available methods, **Krippendorff‚Äôs Alpha** and the **Alt-Test** are the ones we consider more **rigorous and robust**.\n",
    "\n",
    "> **Note 1**: The final decision on whether the model's performance is ‚Äúgood enough‚Äù depends on your research domain,  \n",
    "> acceptable error tolerance, and practical factors such as annotation cost and time. It can be totally valid to accept the model based solely on its Cohen‚Äôs kappa score,\n",
    " if it is approximately equivalent to human inter-rater agreement.\n",
    "\n",
    "> **Note 2**: If the agreement between human annotators is low, the issue likely lies in the codebook (e.g., unclear guidelines) or the annotation task itself.\n",
    "> In such cases, it‚Äôs unrealistic to expect the LLM to achieve high performance if humans themselves struggle to agree on the correct labels.\n",
    "\n",
    "> **Note 3**: If you're not satisfied with the model‚Äôs performance, you can go back and **adjust the scenario** (this may include updating the codebook, adding examples, using another model...)  \n",
    "> ‚ö†Ô∏è However, if you do this **multiple times**, it is strongly recommended to use a **validation set** to avoid overfitting to your annotated subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's Kappa\n",
    "\n",
    "This analysis provides:\n",
    "\n",
    "- **Mean agreement between the LLM and all human annotators** (when multiple annotators are available)\n",
    "- **Mean agreement among human annotators** (when multiple annotators are available)\n",
    "- **Individual agreement scores** for all pairwise comparisons\n",
    "\n",
    "#### Weighting Options\n",
    "\n",
    "You can set kappa_weights to different values. Use:\n",
    "\n",
    "- **unweighted (remove the parameter)**:  \n",
    "  Treats all disagreements equally.  \n",
    "  _Example: Disagreeing between `0` and `1` is treated the same as between `0` and `2`._\n",
    "\n",
    "- **linear**:  \n",
    "  Weights disagreements by their distance.  \n",
    "  _Example: A disagreement between `0` and `2` is considered twice as bad as between `0` and `1`._\n",
    "\n",
    "- **quadratic**:  \n",
    "  Weights disagreements by the square of their distance.  \n",
    "  _Example: A disagreement between `0` and `2` is considered four times as bad as between `0` and `1`._\n",
    "\n",
    "> **Note **: If `n_runs` > 1, the reported metrics will include **variability across runs**, allowing you to assess the **consistency** of LLM performance.  \n",
    "> Lower variance indicates more stable and reliable model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Columns in detailed_results_df (in compute_kappa_metrics) ===\n",
      "['sample_id', 'split', 'verbatim', 'iteration', 'Rater_Oli', 'Rater_chloe', 'Rater_RA', 'ModelPrediction', 'Raisonnement', 'run', 'prompt_name', 'use_validation_set']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>iteration</th>\n",
       "      <th>run</th>\n",
       "      <th>use_validation_set</th>\n",
       "      <th>N_train</th>\n",
       "      <th>N_val</th>\n",
       "      <th>accuracy_GT_train</th>\n",
       "      <th>kappa_GT_train</th>\n",
       "      <th>mean_kappa_llm_human</th>\n",
       "      <th>mean_human_human_agreement</th>\n",
       "      <th>Rater_Oli_kappa_GT</th>\n",
       "      <th>Rater_chloe_kappa_GT</th>\n",
       "      <th>Rater_RA_kappa_GT</th>\n",
       "      <th>n_runs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>french_prompt</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.606481</td>\n",
       "      <td>0.671222</td>\n",
       "      <td>0.79798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>french_prompt</td>\n",
       "      <td>1</td>\n",
       "      <td>aggregated</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.606481</td>\n",
       "      <td>0.671222</td>\n",
       "      <td>0.79798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt_name  iteration         run  use_validation_set  N_train  N_val  \\\n",
       "0  french_prompt          1           1               False       20      0   \n",
       "1  french_prompt          1  aggregated               False       20      0   \n",
       "\n",
       "   accuracy_GT_train  kappa_GT_train  mean_kappa_llm_human  \\\n",
       "0               0.85          0.6875              0.606481   \n",
       "1               0.85          0.6875              0.606481   \n",
       "\n",
       "   mean_human_human_agreement  Rater_Oli_kappa_GT  Rater_chloe_kappa_GT  \\\n",
       "0                    0.671222             0.79798                   1.0   \n",
       "1                    0.671222             0.79798                   1.0   \n",
       "\n",
       "   Rater_RA_kappa_GT  n_runs  \n",
       "0           0.705882     NaN  \n",
       "1           0.705882     1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10) Compute metrics from the detailed results\n",
    "# First, compute kappa metrics\n",
    "kappa_df, detailed_kappa_metrics = compute_kappa_metrics(\n",
    "    detailed_results_df=complex_case_for_metrics,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    show_runs=True\n",
    ")\n",
    "\n",
    "kappa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Detailed Kappa Metrics ===\n",
      "\n",
      "Scenario: french_prompt_iteration_1\n",
      "\n",
      "LLM vs Human Annotators:\n",
      "  Human_Annotator  Cohens_Kappa\n",
      "0       Rater_Oli      0.897959\n",
      "1     Rater_chloe      0.897959\n",
      "2        Rater_RA      0.615385\n",
      "\n",
      "Human vs Human Annotators:\n",
      "   Annotator_1  Annotator_2  Cohens_Kappa\n",
      "0    Rater_Oli  Rater_chloe      0.797980\n",
      "1    Rater_Oli     Rater_RA      0.509804\n",
      "2  Rater_chloe     Rater_RA      0.705882\n",
      "\n",
      "Scenario: english_prompt_iteration_1.0\n",
      "\n",
      "LLM vs Human Annotators:\n",
      "  Human_Annotator  Cohens_Kappa\n",
      "0       Rater_Oli      1.000000\n",
      "1     Rater_chloe      0.615385\n",
      "2        Rater_RA      0.285714\n",
      "\n",
      "Human vs Human Annotators:\n",
      "   Annotator_1  Annotator_2  Cohens_Kappa\n",
      "0    Rater_Oli  Rater_chloe      0.615385\n",
      "1    Rater_Oli     Rater_RA      0.285714\n",
      "2  Rater_chloe     Rater_RA      0.545455\n",
      "\n",
      "Scenario: english_prompt_iteration_2.0\n",
      "\n",
      "LLM vs Human Annotators:\n",
      "  Human_Annotator  Cohens_Kappa\n",
      "0       Rater_Oli      1.000000\n",
      "1     Rater_chloe      0.615385\n",
      "2        Rater_RA      0.285714\n",
      "\n",
      "Human vs Human Annotators:\n",
      "   Annotator_1  Annotator_2  Cohens_Kappa\n",
      "0    Rater_Oli  Rater_chloe      0.615385\n",
      "1    Rater_Oli     Rater_RA      0.285714\n",
      "2  Rater_chloe     Rater_RA      0.545455\n",
      "\n",
      "Scenario: english_prompt_iteration_3.0\n",
      "\n",
      "LLM vs Human Annotators:\n",
      "  Human_Annotator  Cohens_Kappa\n",
      "0       Rater_Oli      0.545455\n",
      "1     Rater_chloe      0.285714\n",
      "2        Rater_RA      0.117647\n",
      "\n",
      "Human vs Human Annotators:\n",
      "   Annotator_1  Annotator_2  Cohens_Kappa\n",
      "0    Rater_Oli  Rater_chloe      0.615385\n",
      "1    Rater_Oli     Rater_RA      0.285714\n",
      "2  Rater_chloe     Rater_RA      0.545455\n"
     ]
    }
   ],
   "source": [
    "# Additional details about the kappa metrics\n",
    "\n",
    "print(\"\\n=== Detailed Kappa Metrics ===\")\n",
    "if detailed_kappa_metrics:\n",
    "    for scenario_key, metrics in detailed_kappa_metrics.items():\n",
    "        print(f\"\\nScenario: {scenario_key}\")\n",
    "        \n",
    "        print(\"\\nLLM vs Human Annotators:\")\n",
    "        print(metrics['llm_vs_human_df'])\n",
    "        \n",
    "        print(\"\\nHuman vs Human Annotators:\")\n",
    "        print(metrics['human_vs_human_df'])\n",
    "else:\n",
    "    print(\"No detailed kappa metrics available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics (Per-Class Analysis)\n",
    "\n",
    "Analyze detailed classification metrics for each class, focusing on **recall** and **confusion matrix elements**.\n",
    "\n",
    "This analysis uses the **majority vote from human annotations** as the ground truth and provides:\n",
    "\n",
    "#### Global Metrics (prefix: `global_*`)\n",
    "\n",
    "- `global_accuracy_train`: Overall accuracy on training data\n",
    "- `global_recall_train`: Macro recall on training data\n",
    "- `global_error_rate_train`: 1 - accuracy\n",
    "\n",
    "(And similarly for validation data with suffix `_val`, if `use_validation_set = True`)\n",
    "\n",
    "#### Per-Class Metrics (prefix: `class_<label>_*_train`)\n",
    "\n",
    "For each class label (e.g., `0`, `1`), the following are computed:\n",
    "\n",
    "- `class_<label>_recall_train`: Proportion of actual class instances correctly identified (True Positives)\n",
    "- `class_<label>_error_rate_train`: Proportion of actual class instances incorrectly classified (Miss Rate)\n",
    "- `class_<label>_correct_count_train`: Number of correctly predicted instances\n",
    "- `class_<label>_missed_count_train`: Number of missed instances (False Negatives)\n",
    "- `class_<label>_false_positives_train`: Number of incorrect predictions *as* this class (False Positives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Columns in detailed_results_df (in compute_classification_metrics_from_results) ===\n",
      "['sample_id', 'split', 'verbatim', 'iteration', 'Rater_Oli', 'Rater_chloe', 'Rater_RA', 'ModelPrediction', 'Raisonnement', 'run', 'prompt_name', 'use_validation_set', 'Reasoning', 'prompt_iteration', 'best_accuracy', 'total_iterations', 'is_best_iteration']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>iteration</th>\n",
       "      <th>run</th>\n",
       "      <th>use_validation_set</th>\n",
       "      <th>N_train</th>\n",
       "      <th>N_val</th>\n",
       "      <th>prompt_iteration</th>\n",
       "      <th>global_accuracy_train</th>\n",
       "      <th>global_recall_train</th>\n",
       "      <th>global_error_rate_train</th>\n",
       "      <th>class_0_recall_train</th>\n",
       "      <th>class_0_error_rate_train</th>\n",
       "      <th>class_0_correct_count_train</th>\n",
       "      <th>class_0_missed_count_train</th>\n",
       "      <th>class_0_false_positives_train</th>\n",
       "      <th>class_1_recall_train</th>\n",
       "      <th>class_1_error_rate_train</th>\n",
       "      <th>class_1_correct_count_train</th>\n",
       "      <th>class_1_missed_count_train</th>\n",
       "      <th>class_1_false_positives_train</th>\n",
       "      <th>class_2_recall_train</th>\n",
       "      <th>class_2_error_rate_train</th>\n",
       "      <th>class_2_correct_count_train</th>\n",
       "      <th>class_2_missed_count_train</th>\n",
       "      <th>class_2_false_positives_train</th>\n",
       "      <th>global_accuracy_val</th>\n",
       "      <th>global_recall_val</th>\n",
       "      <th>global_error_rate_val</th>\n",
       "      <th>class_0_recall_val</th>\n",
       "      <th>class_0_error_rate_val</th>\n",
       "      <th>class_0_correct_count_val</th>\n",
       "      <th>class_0_missed_count_val</th>\n",
       "      <th>class_0_false_positives_val</th>\n",
       "      <th>class_1_recall_val</th>\n",
       "      <th>class_1_error_rate_val</th>\n",
       "      <th>class_1_correct_count_val</th>\n",
       "      <th>class_1_missed_count_val</th>\n",
       "      <th>class_1_false_positives_val</th>\n",
       "      <th>class_2_recall_val</th>\n",
       "      <th>class_2_error_rate_val</th>\n",
       "      <th>class_2_correct_count_val</th>\n",
       "      <th>class_2_missed_count_val</th>\n",
       "      <th>class_2_false_positives_val</th>\n",
       "      <th>n_runs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>french_prompt</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>french_prompt</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>english_prompt</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>english_prompt</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>english_prompt</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>english_prompt</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>english_prompt</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>english_prompt</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>english_prompt</td>\n",
       "      <td>3</td>\n",
       "      <td>aggregated</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>french_prompt</td>\n",
       "      <td>1</td>\n",
       "      <td>aggregated</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prompt_name  iteration         run  use_validation_set  N_train  N_val  \\\n",
       "0   french_prompt          1           1               False       20      0   \n",
       "1   french_prompt          1           2               False       20      0   \n",
       "2  english_prompt          1           1                True        5      5   \n",
       "3  english_prompt          1           2                True        5      5   \n",
       "4  english_prompt          2           1                True        5      5   \n",
       "5  english_prompt          2           2                True        5      5   \n",
       "6  english_prompt          3           1                True        5      5   \n",
       "7  english_prompt          3           2                True        5      5   \n",
       "8  english_prompt          3  aggregated                True       30     30   \n",
       "9   french_prompt          1  aggregated               False       40      0   \n",
       "\n",
       "   prompt_iteration  global_accuracy_train  global_recall_train  \\\n",
       "0               NaN               0.950000             0.629630   \n",
       "1               NaN               0.950000             0.629630   \n",
       "2               1.0               0.800000             0.555556   \n",
       "3               1.0               0.800000             0.555556   \n",
       "4               2.0               0.600000             0.444444   \n",
       "5               2.0               0.800000             0.555556   \n",
       "6               3.0               0.800000             0.555556   \n",
       "7               3.0               0.600000             0.444444   \n",
       "8               NaN               0.733333             0.518519   \n",
       "9               NaN               0.950000             0.629630   \n",
       "\n",
       "   global_error_rate_train  class_0_recall_train  class_0_error_rate_train  \\\n",
       "0                 0.050000              0.888889                  0.111111   \n",
       "1                 0.050000              0.888889                  0.111111   \n",
       "2                 0.200000              0.666667                  0.333333   \n",
       "3                 0.200000              0.666667                  0.333333   \n",
       "4                 0.400000              0.333333                  0.666667   \n",
       "5                 0.200000              0.666667                  0.333333   \n",
       "6                 0.200000              0.666667                  0.333333   \n",
       "7                 0.400000              0.333333                  0.666667   \n",
       "8                 0.266667              0.555556                  0.444444   \n",
       "9                 0.050000              0.888889                  0.111111   \n",
       "\n",
       "   class_0_correct_count_train  class_0_missed_count_train  \\\n",
       "0                            8                           1   \n",
       "1                            8                           1   \n",
       "2                            2                           1   \n",
       "3                            2                           1   \n",
       "4                            1                           2   \n",
       "5                            2                           1   \n",
       "6                            2                           1   \n",
       "7                            1                           2   \n",
       "8                           10                           8   \n",
       "9                           16                           2   \n",
       "\n",
       "   class_0_false_positives_train  class_1_recall_train  \\\n",
       "0                              0                   1.0   \n",
       "1                              0                   1.0   \n",
       "2                              0                   1.0   \n",
       "3                              0                   1.0   \n",
       "4                              0                   1.0   \n",
       "5                              0                   1.0   \n",
       "6                              0                   1.0   \n",
       "7                              0                   1.0   \n",
       "8                              0                   1.0   \n",
       "9                              0                   1.0   \n",
       "\n",
       "   class_1_error_rate_train  class_1_correct_count_train  \\\n",
       "0                       0.0                           11   \n",
       "1                       0.0                           11   \n",
       "2                       0.0                            2   \n",
       "3                       0.0                            2   \n",
       "4                       0.0                            2   \n",
       "5                       0.0                            2   \n",
       "6                       0.0                            2   \n",
       "7                       0.0                            2   \n",
       "8                       0.0                           12   \n",
       "9                       0.0                           22   \n",
       "\n",
       "   class_1_missed_count_train  class_1_false_positives_train  \\\n",
       "0                           0                              1   \n",
       "1                           0                              1   \n",
       "2                           0                              1   \n",
       "3                           0                              1   \n",
       "4                           0                              2   \n",
       "5                           0                              1   \n",
       "6                           0                              1   \n",
       "7                           0                              2   \n",
       "8                           0                              8   \n",
       "9                           0                              2   \n",
       "\n",
       "   class_2_recall_train  class_2_error_rate_train  \\\n",
       "0                   0.0                       1.0   \n",
       "1                   0.0                       1.0   \n",
       "2                   0.0                       1.0   \n",
       "3                   0.0                       1.0   \n",
       "4                   0.0                       1.0   \n",
       "5                   0.0                       1.0   \n",
       "6                   0.0                       1.0   \n",
       "7                   0.0                       1.0   \n",
       "8                   0.0                       1.0   \n",
       "9                   0.0                       1.0   \n",
       "\n",
       "   class_2_correct_count_train  class_2_missed_count_train  \\\n",
       "0                            0                           0   \n",
       "1                            0                           0   \n",
       "2                            0                           0   \n",
       "3                            0                           0   \n",
       "4                            0                           0   \n",
       "5                            0                           0   \n",
       "6                            0                           0   \n",
       "7                            0                           0   \n",
       "8                            0                           0   \n",
       "9                            0                           0   \n",
       "\n",
       "   class_2_false_positives_train  global_accuracy_val  global_recall_val  \\\n",
       "0                              0                  NaN                NaN   \n",
       "1                              0                  NaN                NaN   \n",
       "2                              0                  1.0           0.666667   \n",
       "3                              0                  1.0           0.666667   \n",
       "4                              0                  1.0           0.666667   \n",
       "5                              0                  1.0           0.666667   \n",
       "6                              0                  1.0           0.666667   \n",
       "7                              0                  1.0           0.666667   \n",
       "8                              0                  1.0           0.666667   \n",
       "9                              0                  NaN                NaN   \n",
       "\n",
       "   global_error_rate_val  class_0_recall_val  class_0_error_rate_val  \\\n",
       "0                    NaN                 NaN                     NaN   \n",
       "1                    NaN                 NaN                     NaN   \n",
       "2                    0.0                 1.0                     0.0   \n",
       "3                    0.0                 1.0                     0.0   \n",
       "4                    0.0                 1.0                     0.0   \n",
       "5                    0.0                 1.0                     0.0   \n",
       "6                    0.0                 1.0                     0.0   \n",
       "7                    0.0                 1.0                     0.0   \n",
       "8                    0.0                 1.0                     0.0   \n",
       "9                    NaN                 NaN                     NaN   \n",
       "\n",
       "   class_0_correct_count_val  class_0_missed_count_val  \\\n",
       "0                        NaN                       NaN   \n",
       "1                        NaN                       NaN   \n",
       "2                        2.0                       0.0   \n",
       "3                        2.0                       0.0   \n",
       "4                        2.0                       0.0   \n",
       "5                        2.0                       0.0   \n",
       "6                        2.0                       0.0   \n",
       "7                        2.0                       0.0   \n",
       "8                       12.0                       0.0   \n",
       "9                        NaN                       NaN   \n",
       "\n",
       "   class_0_false_positives_val  class_1_recall_val  class_1_error_rate_val  \\\n",
       "0                          NaN                 NaN                     NaN   \n",
       "1                          NaN                 NaN                     NaN   \n",
       "2                          0.0                 1.0                     0.0   \n",
       "3                          0.0                 1.0                     0.0   \n",
       "4                          0.0                 1.0                     0.0   \n",
       "5                          0.0                 1.0                     0.0   \n",
       "6                          0.0                 1.0                     0.0   \n",
       "7                          0.0                 1.0                     0.0   \n",
       "8                          0.0                 1.0                     0.0   \n",
       "9                          NaN                 NaN                     NaN   \n",
       "\n",
       "   class_1_correct_count_val  class_1_missed_count_val  \\\n",
       "0                        NaN                       NaN   \n",
       "1                        NaN                       NaN   \n",
       "2                        3.0                       0.0   \n",
       "3                        3.0                       0.0   \n",
       "4                        3.0                       0.0   \n",
       "5                        3.0                       0.0   \n",
       "6                        3.0                       0.0   \n",
       "7                        3.0                       0.0   \n",
       "8                       18.0                       0.0   \n",
       "9                        NaN                       NaN   \n",
       "\n",
       "   class_1_false_positives_val  class_2_recall_val  class_2_error_rate_val  \\\n",
       "0                          NaN                 NaN                     NaN   \n",
       "1                          NaN                 NaN                     NaN   \n",
       "2                          0.0                 0.0                     1.0   \n",
       "3                          0.0                 0.0                     1.0   \n",
       "4                          0.0                 0.0                     1.0   \n",
       "5                          0.0                 0.0                     1.0   \n",
       "6                          0.0                 0.0                     1.0   \n",
       "7                          0.0                 0.0                     1.0   \n",
       "8                          0.0                 0.0                     1.0   \n",
       "9                          NaN                 NaN                     NaN   \n",
       "\n",
       "   class_2_correct_count_val  class_2_missed_count_val  \\\n",
       "0                        NaN                       NaN   \n",
       "1                        NaN                       NaN   \n",
       "2                        0.0                       0.0   \n",
       "3                        0.0                       0.0   \n",
       "4                        0.0                       0.0   \n",
       "5                        0.0                       0.0   \n",
       "6                        0.0                       0.0   \n",
       "7                        0.0                       0.0   \n",
       "8                        0.0                       0.0   \n",
       "9                        NaN                       NaN   \n",
       "\n",
       "   class_2_false_positives_val  n_runs  \n",
       "0                          NaN     NaN  \n",
       "1                          NaN     NaN  \n",
       "2                          0.0     NaN  \n",
       "3                          0.0     NaN  \n",
       "4                          0.0     NaN  \n",
       "5                          0.0     NaN  \n",
       "6                          0.0     NaN  \n",
       "7                          0.0     NaN  \n",
       "8                          0.0     2.0  \n",
       "9                          NaN     2.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute classification metrics\n",
    "classification_df = compute_classification_metrics_from_results(\n",
    "    detailed_results_df=complex_case_for_metrics,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    show_runs=True\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)    # show all columns\n",
    "classification_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krippendorff‚Äôs‚ÄØŒ± Non‚ÄëInferiority Test  \n",
    "*(Requires ‚â•‚ÄØ3 human annotation columns)*\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "This test evaluates whether the model's annotations are **statistically non-inferior** to fully human-annotated data.  \n",
    "If successful, this means the model can probably take over the annotation of the remaining, unlabeled data.\n",
    "\n",
    "#### How the Test Works\n",
    "\n",
    "- **Human reliability (`Œ±_human`)**  \n",
    "  Krippendorff‚Äôs Œ± is computed across all *n* human annotators.\n",
    "\n",
    "- **Model reliability (`Œ±_model`)**  \n",
    "  For each possible panel of (*n‚ÄØ‚àí‚ÄØ1*) humans + the model, compute Krippendorff‚Äôs Œ±.  \n",
    "  The final value is the **mean** Œ± across all such combinations.\n",
    "\n",
    "- **Effect size (Œî)**  \n",
    "  \\[\n",
    "  \\Delta = \\alpha_{\\text{model}} - \\alpha_{\\text{human}}\n",
    "  \\]  \n",
    "  - Positive Œî ‚Üí Model improves reliability  \n",
    "  - Negative Œî ‚Üí Performance drop\n",
    "\n",
    "- **Uncertainty estimation via bootstrapping**  \n",
    "  The dataset is resampled thousands of times (e.g., 2,000) to recompute Œî.  \n",
    "  A **90‚ÄØ% confidence interval (CI)** (configurable) is constructed to show where the true Œî likely lies.\n",
    "\n",
    "\n",
    "- **Non‚ÄëInferiority Margin (`Œ¥`)**\n",
    "    You define `Œ¥` (commonly set to **‚àí0.05**) as the **largest acceptable drop** in Œ± when using the model.\n",
    "\n",
    "- **Decision rule**:  \n",
    "  If the entire confidence interval lies **above `Œ¥`**, the model is declared **non-inferior**.  \n",
    "  With a 90‚ÄØ% CI, this reflects a **5‚ÄØ% one-sided risk** of wrongly approving a model worse than the lower born of the CI.\n",
    "\n",
    "#### Interpretation Cheatsheet\n",
    "\n",
    "| CI Position                 | What It Means for Deployment                                               |\n",
    "|----------------------------|-----------------------------------------------------------------------------|\n",
    "| CI fully above **0**       | ‚úÖ Model is **statistically superior** to humans  |\n",
    "| CI fully above **Œ¥**, but crosses 0 | ‚úÖ Model is **non-inferior** (small, acceptable loss)     |\n",
    "| CI touches or falls below **Œ¥** | ‚ùå Model is possibly worse than the humans by the Œ¥ margin|\n",
    "\n",
    "#### Why ‚Äú5‚ÄØ% Risk‚Äù?\n",
    "\n",
    "- A 90‚ÄØ% CI corresponds to a **one-sided Œ± = 0.05** non-inferiority test.\n",
    "- This 5‚ÄØ% risk applies to the **margin Œ¥**, not to zero.\n",
    "- If the CI just touches Œ¥ ‚Üí ‚âà‚ÄØ5‚ÄØ% chance that the **true Œî ‚â§ Œ¥**\n",
    "- If the CI is well above Œ¥ ‚Üí Risk that **true Œî ‚â§ 0** is even lower than 5‚ÄØ%\n",
    "\n",
    "#### Settings and Their Effects\n",
    "\n",
    "| Setting                        | Increase ‚Üí                          | Decrease ‚Üí                          |\n",
    "|-------------------------------|-------------------------------------|-------------------------------------|\n",
    "| **Confidence level** (e.g. 90‚ÄØ% ‚Üí 95‚ÄØ%) | ‚Äì CI gets **wider**<br>‚Äì Test becomes **stricter**<br>‚Äì Type I error drops (5‚ÄØ% ‚Üí 2.5‚ÄØ%) | ‚Äì CI gets **narrower**<br>‚Äì Easier to declare non-inferiority<br>‚Äì Higher false positive risk |\n",
    "| **Non-inferiority margin `Œ¥`** (e.g. ‚àí0.05 ‚Üí ‚àí0.10) | ‚Äì You tolerate a **larger drop**<br>‚Äì Easier for model to pass<br>‚Äì Lower guaranteed quality | ‚Äì You demand **closer match to humans**<br>‚Äì Harder to pass<br>‚Äì Stronger quality guarantee |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Non-inferiority Test: english_prompt_iteration_3 ===\n",
      "Human trios Œ±: 0.4568 ¬± 0.0000\n",
      "Model trios Œ±: 0.4719 ¬± 0.0000\n",
      "Œî = model ‚àí human = +0.0152 ¬± 0.0000\n",
      "90% CI: [-0.1614, 0.1564]\n",
      "Non-inferiority demonstrated in 0/2 runs\n",
      "‚ùå Non-inferiority NOT demonstrated in any run (margin = -0.05)\n",
      "\n",
      "=== Non-inferiority Test: french_prompt_iteration_1 ===\n",
      "Human trios Œ±: 0.6722 ¬± 0.0000\n",
      "Model trios Œ±: 0.7586 ¬± 0.0000\n",
      "Œî = model ‚àí human = +0.0864 ¬± 0.0000\n",
      "90% CI: [0.0226, 0.1567]\n",
      "Non-inferiority demonstrated in 2/2 runs\n",
      "‚úÖ Non-inferiority consistently demonstrated across all runs (margin = -0.05)\n"
     ]
    }
   ],
   "source": [
    "# Run the non-inferiority test\n",
    "non_inferiority_results = compute_krippendorff_non_inferiority(\n",
    "    detailed_results_df=complex_case_for_metrics,\n",
    "    annotation_columns=annotation_columns,\n",
    "    model_column=\"ModelPrediction\",\n",
    "    level_of_measurement='ordinal',\n",
    "    non_inferiority_margin=-0.05,\n",
    "    n_bootstrap=2000, \n",
    "    confidence_level=90.0,\n",
    "    random_seed=42, \n",
    "    verbose=False   \n",
    ")\n",
    "\n",
    "# Print results in a formatted way\n",
    "print_non_inferiority_results(non_inferiority_results, show_per_run=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Annotator Test (ALT-Test)\n",
    "\n",
    "The **ALT-Test** evaluates whether an LLM can perform **as well as or better than human annotators**, based on a **leave-one-human-out** approach.\n",
    "\n",
    "This method requires **at least 3 human annotation columns**.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "- The LLM is compared against **each human annotator**, one at a time.\n",
    "- For each comparison:\n",
    "  - One human is **excluded**\n",
    "  - The model‚Äôs predictions are evaluated **against the remaining human annotations**\n",
    "  - This simulates a realistic setting where the LLM replaces a single annotator and is judged by agreement with the rest\n",
    "\n",
    "#### Key Metrics in Output\n",
    "\n",
    "- **`winning_rate_train`**: Proportion of annotators for which the LLM performs as well or better (after adjusting for Œµ)\n",
    "- **`passed_alt_test_train`**: `True` if the LLM passes the test (i.e., `winning_rate ‚â• 0.5`)\n",
    "- **`avg_adv_prob_train`**: Average advantage probability, how likely the model is better across comparisons\n",
    "- **`p_values_train`**: List of p-values for each comparison\n",
    "\n",
    "#### Interpreting `Œµ` (Epsilon)\n",
    "\n",
    "- `Œµ` accounts for the **cost/effort/time trade-off** between using an LLM and a human annotator.\n",
    "- Higher `Œµ` gives the model more leeway, useful when **human annotations are costly**.\n",
    "- Recommendations from the original paper:\n",
    "  - `Œµ = 0.2` ‚Üí when humans are **experts**\n",
    "  - `Œµ = 0.1` ‚Üí when humans are **crowdworkers**\n",
    "\n",
    "> If `winning_rate ‚â• 0.5`, the LLM is considered **statistically competitive with human annotators** for this dataset and scenario (the LLM is \"better\" than half the humans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Columns in detailed_results_df (in run_alt_test_on_results) ===\n",
      "['sample_id', 'split', 'verbatim', 'iteration', 'Rater_Oli', 'Rater_chloe', 'Rater_RA', 'ModelPrediction', 'Raisonnement', 'run', 'prompt_name', 'use_validation_set', 'Reasoning', 'prompt_iteration', 'best_accuracy', 'total_iterations', 'is_best_iteration']\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 20 valid labels\n",
      "  Rater_Oli: 20 valid labels\n",
      "  Rater_chloe: 20 valid labels\n",
      "  Rater_RA: 20 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_chloe: int64\n",
      "  Rater_RA: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_chloe type after conversion: <class 'numpy.int32'>\n",
      "Rater_RA type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=0.950\n",
      "Rater_chloe: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_RA: p=0.0002 => rejectH0=True | rho_f=1.000, rho_h=0.850\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 1.000\n",
      "Average Advantage Probability (rho) = 1.000\n",
      "Passed Alt-Test? => True\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 20 valid labels\n",
      "  Rater_Oli: 20 valid labels\n",
      "  Rater_chloe: 20 valid labels\n",
      "  Rater_RA: 20 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_chloe: int64\n",
      "  Rater_RA: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_chloe type after conversion: <class 'numpy.int32'>\n",
      "Rater_RA type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=0.950\n",
      "Rater_chloe: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_RA: p=0.0002 => rejectH0=True | rho_f=1.000, rho_h=0.850\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 1.000\n",
      "Average Advantage Probability (rho) = 1.000\n",
      "Passed Alt-Test? => True\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 5 valid labels\n",
      "  Rater_Oli: 5 valid labels\n",
      "  Rater_chloe: 5 valid labels\n",
      "  Rater_RA: 5 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_chloe: int64\n",
      "  Rater_RA: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_chloe type after conversion: <class 'numpy.int32'>\n",
      "Rater_RA type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_chloe: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_RA: p=0.0581 => rejectH0=False | rho_f=1.000, rho_h=0.800\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.667\n",
      "Average Advantage Probability (rho) = 1.000\n",
      "Passed Alt-Test? => True\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 5 valid labels\n",
      "  Rater_Oli: 5 valid labels\n",
      "  Rater_chloe: 5 valid labels\n",
      "  Rater_RA: 5 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_chloe: int64\n",
      "  Rater_RA: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_chloe type after conversion: <class 'numpy.int32'>\n",
      "Rater_RA type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_chloe: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_RA: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 1.000\n",
      "Average Advantage Probability (rho) = 1.000\n",
      "Passed Alt-Test? => True\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 5 valid labels\n",
      "  Rater_Oli: 5 valid labels\n",
      "  Rater_chloe: 5 valid labels\n",
      "  Rater_RA: 5 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_chloe: int64\n",
      "  Rater_RA: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_chloe type after conversion: <class 'numpy.int32'>\n",
      "Rater_RA type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_chloe: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_RA: p=0.0581 => rejectH0=False | rho_f=1.000, rho_h=0.800\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.667\n",
      "Average Advantage Probability (rho) = 1.000\n",
      "Passed Alt-Test? => True\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 5 valid labels\n",
      "  Rater_Oli: 5 valid labels\n",
      "  Rater_chloe: 5 valid labels\n",
      "  Rater_RA: 5 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_chloe: int64\n",
      "  Rater_RA: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_chloe type after conversion: <class 'numpy.int32'>\n",
      "Rater_RA type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_chloe: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_RA: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 1.000\n",
      "Average Advantage Probability (rho) = 1.000\n",
      "Passed Alt-Test? => True\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 5 valid labels\n",
      "  Rater_Oli: 5 valid labels\n",
      "  Rater_chloe: 5 valid labels\n",
      "  Rater_RA: 5 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_chloe: int64\n",
      "  Rater_RA: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_chloe type after conversion: <class 'numpy.int32'>\n",
      "Rater_RA type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.5000 => rejectH0=False | rho_f=0.800, rho_h=1.000\n",
      "Rater_chloe: p=0.5000 => rejectH0=False | rho_f=0.800, rho_h=1.000\n",
      "Rater_RA: p=0.2807 => rejectH0=False | rho_f=0.800, rho_h=0.800\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.000\n",
      "Average Advantage Probability (rho) = 0.800\n",
      "Passed Alt-Test? => False\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 5 valid labels\n",
      "  Rater_Oli: 5 valid labels\n",
      "  Rater_chloe: 5 valid labels\n",
      "  Rater_RA: 5 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_chloe: int64\n",
      "  Rater_RA: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_chloe type after conversion: <class 'numpy.int32'>\n",
      "Rater_RA type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_chloe: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_RA: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 1.000\n",
      "Average Advantage Probability (rho) = 1.000\n",
      "Passed Alt-Test? => True\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 5 valid labels\n",
      "  Rater_Oli: 5 valid labels\n",
      "  Rater_chloe: 5 valid labels\n",
      "  Rater_RA: 5 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_chloe: int64\n",
      "  Rater_RA: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_chloe type after conversion: <class 'numpy.int32'>\n",
      "Rater_RA type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_chloe: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_RA: p=0.0581 => rejectH0=False | rho_f=1.000, rho_h=0.800\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.667\n",
      "Average Advantage Probability (rho) = 1.000\n",
      "Passed Alt-Test? => True\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 5 valid labels\n",
      "  Rater_Oli: 5 valid labels\n",
      "  Rater_chloe: 5 valid labels\n",
      "  Rater_RA: 5 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_chloe: int64\n",
      "  Rater_RA: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_chloe type after conversion: <class 'numpy.int32'>\n",
      "Rater_RA type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_chloe: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_RA: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 1.000\n",
      "Average Advantage Probability (rho) = 1.000\n",
      "Passed Alt-Test? => True\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 5 valid labels\n",
      "  Rater_Oli: 5 valid labels\n",
      "  Rater_chloe: 5 valid labels\n",
      "  Rater_RA: 5 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_chloe: int64\n",
      "  Rater_RA: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_chloe type after conversion: <class 'numpy.int32'>\n",
      "Rater_RA type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_chloe: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_RA: p=0.0581 => rejectH0=False | rho_f=1.000, rho_h=0.800\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.667\n",
      "Average Advantage Probability (rho) = 1.000\n",
      "Passed Alt-Test? => True\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 5 valid labels\n",
      "  Rater_Oli: 5 valid labels\n",
      "  Rater_chloe: 5 valid labels\n",
      "  Rater_RA: 5 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_chloe: int64\n",
      "  Rater_RA: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_chloe type after conversion: <class 'numpy.int32'>\n",
      "Rater_RA type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_chloe: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_RA: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 1.000\n",
      "Average Advantage Probability (rho) = 1.000\n",
      "Passed Alt-Test? => True\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 5 valid labels\n",
      "  Rater_Oli: 5 valid labels\n",
      "  Rater_chloe: 5 valid labels\n",
      "  Rater_RA: 5 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_chloe: int64\n",
      "  Rater_RA: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_chloe type after conversion: <class 'numpy.int32'>\n",
      "Rater_RA type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.5000 => rejectH0=False | rho_f=0.800, rho_h=1.000\n",
      "Rater_chloe: p=0.5000 => rejectH0=False | rho_f=0.800, rho_h=1.000\n",
      "Rater_RA: p=0.2807 => rejectH0=False | rho_f=0.800, rho_h=0.800\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.000\n",
      "Average Advantage Probability (rho) = 0.800\n",
      "Passed Alt-Test? => False\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 5 valid labels\n",
      "  Rater_Oli: 5 valid labels\n",
      "  Rater_chloe: 5 valid labels\n",
      "  Rater_RA: 5 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_chloe: int64\n",
      "  Rater_RA: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_chloe type after conversion: <class 'numpy.int32'>\n",
      "Rater_RA type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_chloe: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "Rater_RA: p=0.0000 => rejectH0=True | rho_f=1.000, rho_h=1.000\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 1.000\n",
      "Average Advantage Probability (rho) = 1.000\n",
      "Passed Alt-Test? => True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>N_train</th>\n",
       "      <th>prompt_iteration</th>\n",
       "      <th>winning_rate_train</th>\n",
       "      <th>passed_alt_test_train</th>\n",
       "      <th>avg_adv_prob_train</th>\n",
       "      <th>p_values_train</th>\n",
       "      <th>winning_rate_val</th>\n",
       "      <th>passed_alt_test_val</th>\n",
       "      <th>avg_adv_prob_val</th>\n",
       "      <th>p_values_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>french_prompt</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3.9749874992087996e-05, 0.0, 0.0002056469356819377]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>french_prompt</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3.9749874992087996e-05, 0.0, 0.0002056469356819377]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>english_prompt</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.05805826175840779]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>english_prompt</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.05805826175840779]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>english_prompt</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.8</td>\n",
       "      <td>[0.5, 0.5, 0.28071902212526284]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>english_prompt</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.05805826175840779]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>english_prompt</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.05805826175840779]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>english_prompt</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.8</td>\n",
       "      <td>[0.5, 0.5, 0.28071902212526284]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>english_prompt</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9</td>\n",
       "      <td>[0.25, 0.25, 0.1693886419418353]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>french_prompt</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3.9749874992087996e-05, 0.0, 0.0002056469356819377]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prompt_name  N_train  prompt_iteration  winning_rate_train  \\\n",
       "0   french_prompt       20               NaN            1.000000   \n",
       "1   french_prompt       20               NaN            1.000000   \n",
       "2  english_prompt        5               1.0            0.666667   \n",
       "3  english_prompt        5               1.0            0.666667   \n",
       "4  english_prompt        5               2.0            0.000000   \n",
       "5  english_prompt        5               2.0            0.666667   \n",
       "6  english_prompt        5               3.0            0.666667   \n",
       "7  english_prompt        5               3.0            0.000000   \n",
       "8  english_prompt       30               NaN            0.000000   \n",
       "9   french_prompt       40               NaN            1.000000   \n",
       "\n",
       "   passed_alt_test_train  avg_adv_prob_train  \\\n",
       "0                   True                 1.0   \n",
       "1                   True                 1.0   \n",
       "2                   True                 1.0   \n",
       "3                   True                 1.0   \n",
       "4                  False                 0.8   \n",
       "5                   True                 1.0   \n",
       "6                   True                 1.0   \n",
       "7                  False                 0.8   \n",
       "8                  False                 0.9   \n",
       "9                   True                 1.0   \n",
       "\n",
       "                                         p_values_train  winning_rate_val  \\\n",
       "0  [3.9749874992087996e-05, 0.0, 0.0002056469356819377]               NaN   \n",
       "1  [3.9749874992087996e-05, 0.0, 0.0002056469356819377]               NaN   \n",
       "2                       [0.0, 0.0, 0.05805826175840779]               1.0   \n",
       "3                       [0.0, 0.0, 0.05805826175840779]               1.0   \n",
       "4                       [0.5, 0.5, 0.28071902212526284]               1.0   \n",
       "5                       [0.0, 0.0, 0.05805826175840779]               1.0   \n",
       "6                       [0.0, 0.0, 0.05805826175840779]               1.0   \n",
       "7                       [0.5, 0.5, 0.28071902212526284]               1.0   \n",
       "8                      [0.25, 0.25, 0.1693886419418353]               1.0   \n",
       "9  [3.9749874992087996e-05, 0.0, 0.0002056469356819377]               NaN   \n",
       "\n",
       "  passed_alt_test_val  avg_adv_prob_val     p_values_val  \n",
       "0                 NaN               NaN              NaN  \n",
       "1                 NaN               NaN              NaN  \n",
       "2                True               1.0  [0.0, 0.0, 0.0]  \n",
       "3                True               1.0  [0.0, 0.0, 0.0]  \n",
       "4                True               1.0  [0.0, 0.0, 0.0]  \n",
       "5                True               1.0  [0.0, 0.0, 0.0]  \n",
       "6                True               1.0  [0.0, 0.0, 0.0]  \n",
       "7                True               1.0  [0.0, 0.0, 0.0]  \n",
       "8                True               1.0  [0.0, 0.0, 0.0]  \n",
       "9                 NaN               NaN              NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run ALT test\n",
    "epsilon = 0.2  # Epsilon parameter for ALT test\n",
    "alt_test_df = run_alt_test_on_results(\n",
    "    detailed_results_df=complex_case_for_metrics,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    epsilon=epsilon,\n",
    "    alpha=0.05,\n",
    "    verbose=verbose,\n",
    "    show_runs=True\n",
    ")\n",
    "alt_test_df = alt_test_df.drop(\n",
    "    columns=[\"iteration\", \"run\", \"use_validation_set\", \"N_val\", \"n_runs\"]\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)   # show full content in each cell\n",
    "alt_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step: Classify the Full Dataset\n",
    "\n",
    "If you are satisfied with the evaluation metrics, you can now use the **best-performing scenario** to classify the **entire unlabeled dataset**.\n",
    "\n",
    "Simply **copy the chosen scenario** and run the classification.\n",
    "\n",
    "> This time, only **one run is needed**, since you're not computing evaluation metrics (there are no human labels to compare against).\n",
    "\n",
    "If you're **not satisfied with the results**, feel free to continue exploring and testing **different scenarios**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = [\n",
    "    {\n",
    "        # LLM settings\n",
    "        \"provider_llm1\": \"azure\",\n",
    "        \"model_name_llm1\": \"gpt-4o\",\n",
    "        \"temperature_llm1\": 0,\n",
    "        \"prompt_name\": \"few_shot\",\n",
    "        \"subsample_size\": -1,  # Size of data subset to use\n",
    "\n",
    "        # Prompt configuration\n",
    "        \"template\": \"\"\"\n",
    "Vous √™tes un assistant charg√© d‚Äô√©valuer des entr√©es de donn√©es.\n",
    "\n",
    "Les donn√©es comprennent les colonnes suivantes :\n",
    "- \"r√©ponse\": La r√©ponse √† √©valuer\n",
    "- \"type\": Le type de r√©ponse attendue\n",
    "- \"mots-cl√©s\": Les mots-cl√©s attendus. Tous les mot-cl√©s ne doivent pas n√©cessairement √™tre pr√©sents dans la r√©ponse.\n",
    "- \"process\": L'explication du processus, si celui-ci est attendu.\n",
    "- \"annotation\": La grille d'annotation pour cette t√¢che.\n",
    "- \"exemple_faux\": Un exemple de r√©ponse fausse (0)\n",
    "- \"exemple_moyen\": Un exemple de r√©ponse partiellement correcte (1)\n",
    "- \"exemple_juste\": Un exemple de r√©ponse correcte (2)\n",
    "\n",
    "Voici une entr√©e √† √©valuer :\n",
    "{verbatim_text}\n",
    "\n",
    "T√¢che d‚Äô√©valuation :\n",
    "Pour chaque entr√©e, √©valuer si la r√©ponse est fausse, partiellement correcte ou correcte, en utilisant l‚Äô√©chelle fournie (annotation).\n",
    "La r√©ponse est √©crite par des enfants, l'orthographe et la grammaire ne sont pas importantes.\n",
    "La r√©ponse n'a pas besoin d'√™tre parfaitement similaire √† l'exemple juste pour √™tre consid√©r√©e comme un 2. Si la r√©ponse est correcte dans l'esprit, elle peut √™tre consid√©r√©e comme un 2 plut√¥t que comme un 1.\n",
    "R√©pondre en donnant le Raisonnement et la Classification (0, 1 ou 2) de la r√©ponse.\n",
    "\"\"\",\n",
    "        # Output\n",
    "        \"selected_fields\": [\"Classification\", \"Reasoning\"],\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\":\n",
    "        \"\"\"\n",
    "S'il te plait, suis le format JSON ci-dessous :\n",
    "```json\n",
    "{{\n",
    "  \"Raisonnement\": \"Ton raisonnement ici\",\n",
    "  \"Classification\": \"Ton integer ici\"\n",
    "}}\n",
    "\"\"\",\n",
    "        \"json_output\": True,\n",
    "\n",
    "        # Prompt optimization\n",
    "        \"provider_llm2\": \"azure\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "        \"max_iterations\": 1,\n",
    "        \"use_validation_set\": False,\n",
    "        \"validation_size\": 10,\n",
    "        \"random_state\": 42,\n",
    "\n",
    "        # Majority vote\n",
    "        \"n_completions\": 1,\n",
    "\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run the scenario\n",
    "complex_case_fully_annotated = run_scenarios(\n",
    "    scenarios=scenario,\n",
    "    data=data,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    n_runs=n_runs,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_case_fully_annotated.to_csv(\"data/multiclass_user_case/outputs/multiclass_case_fully_annotated.csv\", sep=\";\", index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
