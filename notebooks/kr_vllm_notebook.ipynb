{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kids Reflect vLLM Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will analyze the Kids Reflect dataset using both Azure OpenAI and vLLM for comparison. This notebook is specifically optimized for running on a supercomputer environment, taking advantage of multiple GPUs and high-performance computing resources.\n",
    "\n",
    "To help you navigate this notebook, here is a step-by-step outline of what we will do:\n",
    "\n",
    "1. **Configure vLLM for Supercomputer Environment**  \n",
    "   - Set environment variables to optimize vLLM for high-performance computing\n",
    "   - Verify GPU availability and configuration\n",
    "\n",
    "2. **Load and Preprocess the Dataset**  \n",
    "   - Load the Kids Reflect dataset\n",
    "   - Clean and normalize text columns\n",
    "   - Convert integer columns to the appropriate data type\n",
    "   - Create verbatim text for analysis\n",
    "\n",
    "3. **Prepare Training and Validation Data**  \n",
    "   - Filter labeled data\n",
    "   - Split data into training and validation sets\n",
    "\n",
    "4. **Define Prompt Templates and Scenarios**  \n",
    "   - Create templates for both Azure OpenAI and vLLM scenarios\n",
    "   - Configure model parameters for optimal performance\n",
    "\n",
    "5. **Run Iterative Prompt Improvement**  \n",
    "   - Execute each scenario separately to monitor progress\n",
    "   - Track GPU usage during execution\n",
    "\n",
    "6. **Analyze and Visualize Results**  \n",
    "   - Compare performance between Azure OpenAI and vLLM\n",
    "   - Visualize kappa values across iterations\n",
    "   - Save results for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure vLLM for Supercomputer Environment\n",
    "\n",
    "Before we begin, we need to configure vLLM to take full advantage of the supercomputer environment. This involves setting environment variables that control how vLLM utilizes the available GPU resources.\n",
    "\n",
    "### Key Configuration Parameters:\n",
    "\n",
    "- **VLLM_MODEL_PATH**: Path to the model or HuggingFace model ID\n",
    "- **VLLM_DTYPE**: Data type for model weights (float16 for efficiency)\n",
    "- **VLLM_GPU_MEMORY_UTILIZATION**: Target GPU memory utilization (0.95 or 95% for supercomputers)\n",
    "- **VLLM_TENSOR_PARALLEL_SIZE**: Number of GPUs to use for tensor parallelism (4 for multi-GPU setups)\n",
    "- **VLLM_MAX_MODEL_LEN**: Maximum sequence length (2048 tokens)\n",
    "- **VLLM_ENABLE_PREFIX_CACHING**: Enable prefix caching for better performance\n",
    "- **VLLM_WORKER_MULTIPROC_METHOD**: Worker multiprocessing method (spawn for better compatibility)\n",
    "\n",
    "These settings are optimized for high-performance computing environments with multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/lustre/fsn1/projects/rech/imi/uqd59cu/qualitative_analysis_project\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: VLLM_MODEL_PATH=TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "env: VLLM_DTYPE=float16\n",
      "env: VLLM_GPU_MEMORY_UTILIZATION=0.95\n",
      "env: VLLM_TENSOR_PARALLEL_SIZE=4\n",
      "env: VLLM_MAX_MODEL_LEN=2048\n",
      "env: VLLM_ENABLE_PREFIX_CACHING=true\n",
      "env: VLLM_WORKER_MULTIPROC_METHOD=spawn\n",
      "\"Current vLLM configuration:\"\n",
      "\"VLLM_MODEL_PATH: $VLLM_MODEL_PATH\"\n",
      "\"VLLM_GPU_MEMORY_UTILIZATION: $VLLM_GPU_MEMORY_UTILIZATION\"\n",
      "\"VLLM_TENSOR_PARALLEL_SIZE: $VLLM_TENSOR_PARALLEL_SIZE\"\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables for vLLM\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "os.environ[\"VLLM_CONFIGURE_LOGGING\"] = \"0\"  # Disable logging\n",
    "os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"FLASH_ATTN\"  # Try FLASH_ATTN backend\n",
    "\n",
    "# Print configuration for debugging\n",
    "print(\"vLLM Environment Configuration:\")\n",
    "print(f\"VLLM_WORKER_MULTIPROC_METHOD = {os.environ.get('VLLM_WORKER_MULTIPROC_METHOD')}\")\n",
    "print(f\"VLLM_CONFIGURE_LOGGING = {os.environ.get('VLLM_CONFIGURE_LOGGING')}\")\n",
    "print(f\"VLLM_ATTENTION_BACKEND = {os.environ.get('VLLM_ATTENTION_BACKEND')}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qualitative_analysis import config\n",
    "# Make a copy of the original config to avoid modifying the module's dictionary\n",
    "vllm_config = dict(config.MODEL_CONFIG[\"vllm\"])\n",
    "\n",
    "# Update with Jean Zay specific settings\n",
    "vllm_config.update({\n",
    "    \"dtype\": \"half\",  # Use half precision (equivalent to float16)\n",
    "    \"enforce_eager\": \"true\",  # Force eager execution mode\n",
    "    \"disable_async_output_proc\": \"true\",  # Disable async output processing\n",
    "    \"tensor_parallel_size\": \"1\",  # Start with 1, increase if needed\n",
    "    \"enable_prefix_caching\": \"true\",  # Enable prefix caching for better performance\n",
    "})\n",
    "\n",
    "# Replace the vLLM config with our updated version\n",
    "config.MODEL_CONFIG[\"vllm\"] = vllm_config\n",
    "\n",
    "print(\"Updated vLLM configuration:\")\n",
    "for key, value in config.MODEL_CONFIG[\"vllm\"].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability\n",
    "\n",
    "Before proceeding, it's important to verify that GPUs are available and properly configured. This step helps identify any potential issues with GPU allocation or configuration before running the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exï¿½cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup\n",
    "\n",
    "Now we'll import the necessary libraries and modules for our analysis. The qualitative_analysis package provides functions for data loading, preprocessing, and model interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: vLLM is not available. VLLMLLMClient will not be usable.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from qualitative_analysis import (\n",
    "    clean_and_normalize,\n",
    "    load_data,\n",
    "    sanitize_dataframe,\n",
    ")\n",
    "from qualitative_analysis.prompt_engineering import run_iterative_prompt_improvement\n",
    "from qualitative_analysis.alt_test import benjamini_yekutieli_correction\n",
    "# Define data directory\n",
    "data_dir = \"data/complex_user_case\"\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The Kids Reflect dataset contains entries from children who engaged in a four-step process to formulate divergent questions about a reference text. Each entry includes:\n",
    "\n",
    "- **Reference**: The text that children read beforehand\n",
    "- **IDENTIFY**: Where the child identifies a knowledge gap related to the reference text\n",
    "- **GUESS**: Where the child makes a guess about what the answer could be\n",
    "- **SEEK**: Where the child formulates a question to seek the answer\n",
    "- **ASSESS**: Where the child evaluates whether an answer was found\n",
    "\n",
    "The dataset also includes validity ratings for each step and overall mechanical ratings, as well as annotations from three human raters (Chloe, Oli, and Gaia).\n",
    "\n",
    "### Data Preprocessing Steps\n",
    "\n",
    "1. Load the dataset from the Excel file\n",
    "2. Clean and normalize text columns\n",
    "3. Convert integer columns to the appropriate data type\n",
    "4. Sanitize the DataFrame to handle any inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>key</th>\n",
       "      <th>reference</th>\n",
       "      <th>IDENTIFY</th>\n",
       "      <th>GUESS</th>\n",
       "      <th>SEEK</th>\n",
       "      <th>ASSESS</th>\n",
       "      <th>identify_cues</th>\n",
       "      <th>guess_cues</th>\n",
       "      <th>...</th>\n",
       "      <th>Identify_validity</th>\n",
       "      <th>Guess_validity</th>\n",
       "      <th>Seek_validity</th>\n",
       "      <th>Assess_validity</th>\n",
       "      <th>mechanical_rating</th>\n",
       "      <th>Rater_Oli</th>\n",
       "      <th>Unvalid_Oli</th>\n",
       "      <th>Rater_Gaia</th>\n",
       "      <th>Unvalid_Gaia</th>\n",
       "      <th>Invalid_Gaia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aac13</td>\n",
       "      <td>3</td>\n",
       "      <td>aac13_3</td>\n",
       "      <td>Toutankhamon etait un pharaon, un roi de l'Egy...</td>\n",
       "      <td>L'Egypte antique</td>\n",
       "      <td>C'est un ancien pays de l'Afrique</td>\n",
       "      <td>Qu'est ce que l'Egypte antique</td>\n",
       "      <td>Qui</td>\n",
       "      <td>{\"1\":\"L'Egypte antique\",\"2\":\"Le principe de mo...</td>\n",
       "      <td>{\"1\":\"C'est un ancien pays de l'Afrique\",\"2\":\"...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aac13</td>\n",
       "      <td>4</td>\n",
       "      <td>aac13_4</td>\n",
       "      <td>La tres grande majorite de lor disponible dans...</td>\n",
       "      <td>Les composants electriques</td>\n",
       "      <td>Les composants electroniques sont par exemple ...</td>\n",
       "      <td>Qu'est-ce qu'une composants electroniques</td>\n",
       "      <td>Non</td>\n",
       "      <td>{\"1\":\"UtilitÃ© de l'or pour les couronnes denta...</td>\n",
       "      <td>{\"1\":\"Les couronnes en or sont plus solides et...</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aac13</td>\n",
       "      <td>5</td>\n",
       "      <td>aac13_5</td>\n",
       "      <td>Des scientifiques ont revele lexistence de tra...</td>\n",
       "      <td>Des premiers humains l'Australie</td>\n",
       "      <td>C'est un pays du Sud</td>\n",
       "      <td>Ou se trouve l'Australie</td>\n",
       "      <td>Oui</td>\n",
       "      <td>{\"1\":\"La formation des traces dans les roches\"...</td>\n",
       "      <td>{\"1\":\"Ces traces se forment automatiquement qu...</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aac13</td>\n",
       "      <td>6</td>\n",
       "      <td>aac13_6</td>\n",
       "      <td>La religion de la Grece antique comprend plusi...</td>\n",
       "      <td>Une mythologie</td>\n",
       "      <td>L'olympe est un endroit en Grece</td>\n",
       "      <td>Qu'est ce qu'une mythologie</td>\n",
       "      <td>Non</td>\n",
       "      <td>{\"1\":\"Une mythologie\",\"2\":\"Les autres mytholog...</td>\n",
       "      <td>{\"1\":\"Une mythologie est un ensemble de contes...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aac24</td>\n",
       "      <td>3</td>\n",
       "      <td>aac24_3</td>\n",
       "      <td>Toutankhamon etait un pharaon, un roi de l'Egy...</td>\n",
       "      <td>Toutankhamon</td>\n",
       "      <td>Roi Pharaon</td>\n",
       "      <td>Quand les Pharaons sont-ils apparus</td>\n",
       "      <td>J'ai trouve ma reponse</td>\n",
       "      <td>{\"1\":\"L'Egypte antique\",\"2\":\"Le principe de mo...</td>\n",
       "      <td>{\"1\":\"C'est un ancien pays de l'Afrique\",\"2\":\"...</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  Iteration      key  \\\n",
       "0  aac13          3  aac13_3   \n",
       "1  aac13          4  aac13_4   \n",
       "2  aac13          5  aac13_5   \n",
       "3  aac13          6  aac13_6   \n",
       "4  aac24          3  aac24_3   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Toutankhamon etait un pharaon, un roi de l'Egy...   \n",
       "1  La tres grande majorite de lor disponible dans...   \n",
       "2  Des scientifiques ont revele lexistence de tra...   \n",
       "3  La religion de la Grece antique comprend plusi...   \n",
       "4  Toutankhamon etait un pharaon, un roi de l'Egy...   \n",
       "\n",
       "                           IDENTIFY  \\\n",
       "0                  L'Egypte antique   \n",
       "1        Les composants electriques   \n",
       "2  Des premiers humains l'Australie   \n",
       "3                    Une mythologie   \n",
       "4                      Toutankhamon   \n",
       "\n",
       "                                               GUESS  \\\n",
       "0                 C'est un ancien pays de l'Afrique    \n",
       "1  Les composants electroniques sont par exemple ...   \n",
       "2                               C'est un pays du Sud   \n",
       "3                   L'olympe est un endroit en Grece   \n",
       "4                                        Roi Pharaon   \n",
       "\n",
       "                                        SEEK                  ASSESS  \\\n",
       "0             Qu'est ce que l'Egypte antique                     Qui   \n",
       "1  Qu'est-ce qu'une composants electroniques                     Non   \n",
       "2                   Ou se trouve l'Australie                     Oui   \n",
       "3                Qu'est ce qu'une mythologie                     Non   \n",
       "4        Quand les Pharaons sont-ils apparus  J'ai trouve ma reponse   \n",
       "\n",
       "                                       identify_cues  \\\n",
       "0  {\"1\":\"L'Egypte antique\",\"2\":\"Le principe de mo...   \n",
       "1  {\"1\":\"UtilitÃ© de l'or pour les couronnes denta...   \n",
       "2  {\"1\":\"La formation des traces dans les roches\"...   \n",
       "3  {\"1\":\"Une mythologie\",\"2\":\"Les autres mytholog...   \n",
       "4  {\"1\":\"L'Egypte antique\",\"2\":\"Le principe de mo...   \n",
       "\n",
       "                                          guess_cues  ... Identify_validity  \\\n",
       "0  {\"1\":\"C'est un ancien pays de l'Afrique\",\"2\":\"...  ...                 1   \n",
       "1  {\"1\":\"Les couronnes en or sont plus solides et...  ...                 3   \n",
       "2  {\"1\":\"Ces traces se forment automatiquement qu...  ...                 2   \n",
       "3  {\"1\":\"Une mythologie est un ensemble de contes...  ...                 1   \n",
       "4  {\"1\":\"C'est un ancien pays de l'Afrique\",\"2\":\"...  ...              <NA>   \n",
       "\n",
       "  Guess_validity Seek_validity  Assess_validity  mechanical_rating  Rater_Oli  \\\n",
       "0              1             1             <NA>               <NA>       <NA>   \n",
       "1           <NA>          <NA>             <NA>               <NA>       <NA>   \n",
       "2              3             3             <NA>                  0       <NA>   \n",
       "3              3             1             <NA>                  0       <NA>   \n",
       "4           <NA>             3             <NA>               <NA>       <NA>   \n",
       "\n",
       "   Unvalid_Oli  Rater_Gaia  Unvalid_Gaia  Invalid_Gaia  \n",
       "0        False        <NA>         False         False  \n",
       "1        False        <NA>         False         False  \n",
       "2        False        <NA>         False         False  \n",
       "3        False        <NA>         False         False  \n",
       "4        False        <NA>         False         False  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to your dataset\n",
    "data_file_path = os.path.join(data_dir, \"complex_data.xlsx\")\n",
    "\n",
    "# Load the data\n",
    "data = load_data(data_file_path, file_type='xlsx', delimiter=';')\n",
    "\n",
    "# 1) Now define the new column names for cleaning\n",
    "text_columns = [\"reference\", \"IDENTIFY\", \"GUESS\", \"SEEK\", \"ASSESS\", \"assess_cues\"]\n",
    "integer_columns = [\"Identify_validity\", \"Guess_validity\", \"Seek_validity\", \"Assess_validity\", \"mechanical_rating\", \"Rater_Chloe\", \"Rater_Oli\", \"Rater_Gaia\"]\n",
    "\n",
    "# 2) Clean and normalize the new columns\n",
    "for col in text_columns:\n",
    "    data[col] = clean_and_normalize(data[col])\n",
    "\n",
    "# 3) Convert selected columns to integers, preserving NaNs\n",
    "for col in integer_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors=\"coerce\").astype(\"Int64\")  # Uses nullable integer type\n",
    "\n",
    "# 4) Sanitize the DataFrame\n",
    "data = sanitize_dataframe(data)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Verbatim Text\n",
    "\n",
    "Now we'll combine the different columns into a single verbatim text for each entry. This format makes it easier for the language model to process the entire entry as a cohesive unit.\n",
    "\n",
    "The verbatim text includes:\n",
    "- The unique key identifier\n",
    "- The reference text\n",
    "- The IDENTIFY, GUESS, SEEK, and ASSESS steps\n",
    "- The validity ratings for each step\n",
    "- The mechanical rating (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of verbatims: 921\n",
      "Verbatim example:\n",
      "key: aac13_3\n",
      "\n",
      "reference: Toutankhamon etait un pharaon, un roi de l'Egypte antique. Il est tres connu aujourdhui parce que des archeologues ont retrouve son cercueil intacte avec tous ses tresors, en 1922. Pour les Egyptiens, il y avait une vie apres la mort, une vie eternelle. Cest pour cela que le corps devait etre conserve dans le meilleur etat possible, cest ce quon appelle la momification. Cest aussi pour cela que lon retrouve aujourdhui de la nourriture, des armes ou des tresors dans les tombeaux. Ces objets accompagnaient le pharaon dans sa vie apres la mort.\n",
      "\n",
      "IDENTIFY: L'Egypte antique\n",
      "\n",
      "GUESS: C'est un ancien pays de l'Afrique \n",
      "\n",
      "SEEK: Qu'est ce que l'Egypte antique\n",
      "\n",
      "ASSESS: Qui\n",
      "\n",
      "assess_cues: {\"1\":\"L'Egypte antique est une ancienne civilisation de l'Afrique du Nord\",\"2\":\"La momification revient a conserver le corps dans une boite et le mettre dans une piece sans lumiere et sans air\",\"3\":\"Le premier Pharaon a vecu a 3000 av. J.-C.\"}\n",
      "\n",
      "Identify_validity: 1\n",
      "\n",
      "Guess_validity: 1\n",
      "\n",
      "Seek_validity: 1\n",
      "\n",
      "Assess_validity: <NA>\n",
      "\n",
      "mechanical_rating: <NA>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine texts and entries\n",
    "data['verbatim'] = data.apply(\n",
    "    lambda row: (\n",
    "        f\"key: {row['key']}\\n\\n\"\n",
    "        f\"reference: {row['reference']}\\n\\n\"\n",
    "        f\"IDENTIFY: {row['IDENTIFY']}\\n\\n\"\n",
    "        f\"GUESS: {row['GUESS']}\\n\\n\"\n",
    "        f\"SEEK: {row['SEEK']}\\n\\n\"\n",
    "        f\"ASSESS: {row['ASSESS']}\\n\\n\"\n",
    "        f\"assess_cues: {row['assess_cues']}\\n\\n\"\n",
    "        f\"Identify_validity: {row['Identify_validity']}\\n\\n\"\n",
    "        f\"Guess_validity: {row['Guess_validity']}\\n\\n\"\n",
    "        f\"Seek_validity: {row['Seek_validity']}\\n\\n\"\n",
    "        f\"Assess_validity: {row['Assess_validity']}\\n\\n\"\n",
    "        f\"mechanical_rating: {row['mechanical_rating']}\\n\\n\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Extract the list of verbatims\n",
    "verbatims = data['verbatim'].tolist()\n",
    "\n",
    "print(f\"Total number of verbatims: {len(verbatims)}\")\n",
    "print(f\"Verbatim example:\\n{verbatims[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training and Validation Data\n",
    "\n",
    "To evaluate the performance of our models, we need to split the data into training and validation sets. We'll use the training set to train the models and the validation set to evaluate their performance.\n",
    "\n",
    "### Steps:\n",
    "1. Identify labeled data (entries with annotations from all three raters)\n",
    "2. Create a subset of the labeled data for analysis\n",
    "3. Split the subset into training (70%) and validation (30%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled rows: 75\n",
      "Number of unlabeled rows: 846\n"
     ]
    }
   ],
   "source": [
    "# Identify the columns that represent your human ratings\n",
    "annotation_columns = ['Rater_Chloe', 'Rater_Oli', 'Rater_Gaia']\n",
    "\n",
    "# Filter labeled data (drop rows with NaN in any annotation column)\n",
    "labeled_data = data.dropna(subset=annotation_columns)\n",
    "\n",
    "# Filter unlabeled data\n",
    "unlabeled_data = data[~data.index.isin(labeled_data.index)]\n",
    "\n",
    "print(\"Number of labeled rows:\", len(labeled_data))\n",
    "print(\"Number of unlabeled rows:\", len(unlabeled_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Templates\n",
    "\n",
    "Now we'll define the prompt templates that will be used to instruct the language models. These templates include:\n",
    "\n",
    "1. **Common Template**: The main instructions for evaluating the validity of a cycle\n",
    "2. **Response Template**: The format in which the model should provide its response\n",
    "\n",
    "The templates include detailed instructions on how to evaluate each step of the cycle and determine overall validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "annotation_columns = ['Rater_Chloe', 'Rater_Oli', 'Rater_Gaia']\n",
    "labels = [0,1]\n",
    "epsilon = 0.2\n",
    "\n",
    "# Scenarios define the configuration for each experiment run\n",
    "# You can specify different providers and models by changing:\n",
    "#   - provider_llm1: The provider for the main LLM (e.g., \"azure\", \"openai\", \"together\", \"vllm\")\n",
    "#   - model_name_llm1: The model name for the main LLM\n",
    "#   - provider_llm2: The provider for the improver LLM\n",
    "#   - model_name_llm2: The model name for the improver LLM\n",
    "#\n",
    "# For vLLM provider:\n",
    "#   - The model_name_llm1 should be a HuggingFace model ID (e.g., \"meta-llama/Llama-2-7b-chat-hf\")\n",
    "#   - For testing, you can use a small model like \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "#\n",
    "# Other parameters can be adjusted as needed (max_iterations, json_output, etc.)\n",
    "scenarios = [\n",
    "    {\n",
    "        \"provider_llm1\": \"azure\",\n",
    "        \"model_name_llm1\": \"gpt-4o\",\n",
    "        \"temperature_llm1\": 0,\n",
    "\n",
    "        # For the \"improver\" LLM2\n",
    "        \"provider_llm2\": \"azure\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "\n",
    "        \"max_iterations\": 1,\n",
    "        \"n_completions\": 1,\n",
    "        \"prompt_name\": \"Basic\",\n",
    "        \n",
    "        # Data configuration\n",
    "        \"subsample_size\": 3,  # Size of data subset to use\n",
    "        \"use_validation_set\": False,  # Whether to use a validation set\n",
    "        \"validation_size\": 10,  # Size of validation set (if used)\n",
    "        \"random_state\": 42,  # Random state for reproducibility\n",
    "\n",
    "        # Our initial prompt\n",
    "        \"template\": \"\"\"\n",
    "Here is an entry to evaluate:\n",
    "{verbatim_text}\n",
    "\n",
    "If a numeric value is present in the mechanical_rating column, copy it as the correct label. Otherwise, put a \"0\".\n",
    "\"\"\",\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"json_output\": True,\n",
    "        \"selected_fields\": [\"Classification\"],\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\":\n",
    "        \"\"\"\n",
    "Please follow the JSON format below:\n",
    "```json\n",
    "{{\n",
    "  \"Reasoning\": \"Your text here\",\n",
    "  \"Classification\": \"Your integer here\"\n",
    "}}\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"provider_llm1\": \"vllm\",\n",
    "        \"model_name_llm1\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        \"temperature_llm1\": 0,\n",
    "\n",
    "        # For the \"improver\" LLM2\n",
    "        \"provider_llm2\": \"azure\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "\n",
    "        \"max_iterations\": 1,\n",
    "        \"n_completions\": 1,\n",
    "        \"prompt_name\": \"Basic\",\n",
    "        \n",
    "        # Data configuration\n",
    "        \"subsample_size\": 3,  # Size of data subset to use\n",
    "        \"use_validation_set\": False,  # Whether to use a validation set\n",
    "        \"validation_size\": 10,  # Size of validation set (if used)\n",
    "        \"random_state\": 42,  # Random state for reproducibility\n",
    "\n",
    "        # Our initial prompt\n",
    "        \"template\": \"\"\"\n",
    "Here is an entry to evaluate:\n",
    "{verbatim_text}\n",
    "\n",
    "If a numeric value is present in the mechanical_rating column, copy it as the correct label. Otherwise, put a \"0\".\n",
    "\"\"\",\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"json_output\": True,\n",
    "        \"selected_fields\": [\"Classification\"],\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\":\n",
    "        \"\"\"\n",
    "Please follow the JSON format below:\n",
    "```json\n",
    "{{\n",
    "  \"Reasoning\": \"Your text here\",\n",
    "  \"Classification\": \"Your integer here\"\n",
    "}}\n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 'Basic' - Train size (all data): 3, No validation set\n",
      "\n",
      "=== Iteration 1/1 ===\n",
      "\n",
      "=== Processing Verbatim 1/3 ===\n",
      "Prompt:\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "key: bc3_3\n",
      "\n",
      "reference: Toutankhamon etait un pharaon, un roi de l'Egypte antique. Il est tres connu aujourdhui parce que des archeologues ont retrouve son cercueil intacte avec tous ses tresors, en 1922. Pour les Egyptiens, il y avait une vie apres la mort, une vie eternelle. Cest pour cela que le corps devait etre conserve dans le meilleur etat possible, cest ce quon appelle la momification. Cest aussi pour cela que lon retrouve aujourdhui de la nourriture, des armes ou des tresors dans les tombeaux. Ces objets accompagnaient le pharaon dans sa vie apres la mort.\n",
      "\n",
      "IDENTIFY: Qu'est-ce que l'Egypte antique\n",
      "\n",
      "GUESS: Je pense que les l'Egypte antique est une epoque\n",
      "\n",
      "SEEK: Qu'est-ce que l'Egypte antique\n",
      "\n",
      "ASSESS: Oui\n",
      "\n",
      "assess_cues: {\"1\":\"L'Egypte antique est une ancienne civilisation de l'Afrique du Nord\",\"2\":\"La momification revient a conserver le corps dans une boite et le mettre dans une piece sans lumiere et sans air\",\"3\":\"Le premier Pharaon a vecu a 3000 av. J.-C.\"}\n",
      "\n",
      "Identify_validity: 1\n",
      "\n",
      "Guess_validity: <NA>\n",
      "\n",
      "Seek_validity: 1\n",
      "\n",
      "Assess_validity: <NA>\n",
      "\n",
      "mechanical_rating: <NA>\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The entry does not provide a numeric value in the mechanical_rating column, so there is no integer to copy as the correct label.\",\n",
      "  \"Classification\": null\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [None]\n",
      "Final (majority) label => None\n",
      "Tokens used => 422\n",
      "Cost => 0.0014\n",
      "\n",
      "=== Processing Verbatim 2/3 ===\n",
      "Prompt:\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "key: bc12_7\n",
      "\n",
      "reference: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite. Selon la legende, Rome aurait ete fondee par Romulus qui aurai donne son nom a la ville. Au depart, ce n'etait qu'un groupe de quelques villages, puis l'Empire romain va couvrir une grande partie de l'Europe et entourer toute la mer Mediterranee. La religion romaine comptait de nombreux dieux, inspires en partie des dieux grecs.\n",
      "\n",
      "IDENTIFY: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite\n",
      "\n",
      "GUESS: Il y a plusieurs dieux Romains\n",
      "\n",
      "SEEK: Il y a Arphodite Romulusn\n",
      "\n",
      "ASSESS: Il y a douze dieux romains comme Venus la deesse de la beaute, Appolo le dieu du soleil\n",
      "\n",
      "assess_cues: {\"1\":\"Dans la legende, Romulus est le premier roi de Rome\",\"2\":\"L'Antiquite s'est etendue entre 3200 avant J.C, jusqu'a l'annee 476\",\"4\":\"La Mediterranee touche plusieurs continents\"}\n",
      "\n",
      "Identify_validity: <NA>\n",
      "\n",
      "Guess_validity: <NA>\n",
      "\n",
      "Seek_validity: <NA>\n",
      "\n",
      "Assess_validity: <NA>\n",
      "\n",
      "mechanical_rating: <NA>\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The entry does not provide a numeric value in the mechanical_rating column, so there is no correct label to copy. The entry contains information about ancient Rome, its founding by Romulus, the expansion of the Roman Empire, and Roman religion. However, the validity of the IDENTIFY, GUESS, SEEK, and ASSESS sections is marked as <NA>, indicating that they have not been evaluated or classified. Therefore, without a numeric value in mechanical_rating, no classification can be assigned.\",\n",
      "  \"Classification\": null\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [None]\n",
      "Final (majority) label => None\n",
      "Tokens used => 473\n",
      "Cost => 0.0021\n",
      "\n",
      "=== Processing Verbatim 3/3 ===\n",
      "Prompt:\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "key: bc19_7\n",
      "\n",
      "reference: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite. Selon la legende, Rome aurait ete fondee par Romulus qui aurai donne son nom a la ville. Au depart, ce n'etait qu'un groupe de quelques villages, puis l'Empire romain va couvrir une grande partie de l'Europe et entourer toute la mer Mediterranee. La religion romaine comptait de nombreux dieux, inspires en partie des dieux grecs.\n",
      "\n",
      "IDENTIFY: Y a-t-il eu plus de dieux grecs ou plus de dieux romains\n",
      "\n",
      "GUESS: Il y a eu plus de dieux grecs\n",
      "\n",
      "SEEK: Y a-t-il eu plus de dieux grecs en plus de dieux romains\n",
      "\n",
      "ASSESS: Il y a eu plus de dieux grecs\n",
      "\n",
      "assess_cues: {\"1\":\"Dans la legende, Romulus est le premier roi de Rome\",\"2\":\"L'Antiquite s'est etendue entre 3200 avant J.C, jusqu'a l'annee 476\",\"4\":\"La Mediterranee touche plusieurs continents\"}\n",
      "\n",
      "Identify_validity: <NA>\n",
      "\n",
      "Guess_validity: <NA>\n",
      "\n",
      "Seek_validity: <NA>\n",
      "\n",
      "Assess_validity: <NA>\n",
      "\n",
      "mechanical_rating: <NA>\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The entry does not provide a numeric value in the mechanical_rating column, so there is no correct label to copy. The entry discusses the history of ancient Rome and mentions the influence of Greek gods on Roman religion, but does not provide a definitive answer to the question of whether there were more Greek gods or Roman gods. The assess_cues provided do not directly relate to the number of gods in either pantheon.\",\n",
      "  \"Classification\": \"NA\"\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => ['NA']\n",
      "Final (majority) label => NA\n",
      "Tokens used => 451\n",
      "Cost => 0.0019\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'NA'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m best_accuracy_overall \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_runs):\n\u001b[1;32m---> 48\u001b[0m     best_prompt, best_accuracy, iteration_rows \u001b[38;5;241m=\u001b[39m \u001b[43mrun_iterative_prompt_improvement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This can now be None\u001b[39;49;00m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mannotation_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43malt_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples_to_give\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# Store the results from this run\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m iteration_rows:\n",
      "File \u001b[1;32mc:\\Users\\ocler\\Documents\\AcadÃ©mique\\Inria\\qualitative_analysis_project\\qualitative_analysis\\prompt_engineering.py:414\u001b[0m, in \u001b[0;36mrun_iterative_prompt_improvement\u001b[1;34m(scenario, train_data, val_data, annotation_columns, labels, alt_test, errors_examples, examples_to_give, epsilon, verbose)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# Apply label type conversion to ground truth and predictions\u001b[39;00m\n\u001b[0;32m    411\u001b[0m y_true_train \u001b[38;5;241m=\u001b[39m convert_labels(\n\u001b[0;32m    412\u001b[0m     train_data[ground_truth_column]\u001b[38;5;241m.\u001b[39mtolist(), label_type\n\u001b[0;32m    413\u001b[0m )\n\u001b[1;32m--> 414\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_labels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModelPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_type\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m accuracy_train \u001b[38;5;241m=\u001b[39m accuracy_score(y_true_train, y_pred_train)\n\u001b[0;32m    419\u001b[0m kappa_train \u001b[38;5;241m=\u001b[39m compute_cohens_kappa(y_true_train, y_pred_train, labels\u001b[38;5;241m=\u001b[39mlabels)\n",
      "File \u001b[1;32mc:\\Users\\ocler\\Documents\\AcadÃ©mique\\Inria\\qualitative_analysis_project\\qualitative_analysis\\prompt_engineering.py:62\u001b[0m, in \u001b[0;36mconvert_labels\u001b[1;34m(labels, label_type)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03mConvert a list of labels to the specified type.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    The list of converted labels.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# Convert all labels to integers\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m label_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Convert all labels to strings\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mstr\u001b[39m(label) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(label) \u001b[38;5;28;01melse\u001b[39;00m label \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "File \u001b[1;32mc:\\Users\\ocler\\Documents\\AcadÃ©mique\\Inria\\qualitative_analysis_project\\qualitative_analysis\\prompt_engineering.py:63\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03mConvert a list of labels to the specified type.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    The list of converted labels.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# Convert all labels to integers\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m---> 63\u001b[0m         \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(label) \u001b[38;5;28;01melse\u001b[39;00m label\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels\n\u001b[0;32m     65\u001b[0m     ]\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m label_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Convert all labels to strings\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mstr\u001b[39m(label) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(label) \u001b[38;5;28;01melse\u001b[39;00m label \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'NA'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Number of runs per scenario (can be adjusted)\n",
    "n_runs = 2\n",
    "\n",
    "# For the final summary dataframe\n",
    "all_aggregated_results = []\n",
    "\n",
    "# For storing all individual run results\n",
    "all_detailed_results = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    # Extract data configuration from scenario\n",
    "    subsample_size = scenario.get(\"subsample_size\", 20)\n",
    "    use_validation_set = scenario.get(\"use_validation_set\", True)\n",
    "    validation_size = scenario.get(\"validation_size\", 10)\n",
    "    random_state = scenario.get(\"random_state\", 42)\n",
    "    \n",
    "    # Step 1: Get a stratified subset of samples\n",
    "    data_subset, _ = train_test_split(\n",
    "        labeled_data,\n",
    "        train_size=subsample_size,\n",
    "        # stratify=labeled_data['label'] if 'label' in labeled_data.columns else None,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Step 2: Split subset into train/val if use_validation_set is True\n",
    "    if use_validation_set:\n",
    "        train_data, val_data = train_test_split(\n",
    "            data_subset,\n",
    "            test_size=validation_size,\n",
    "            # stratify=data_subset['label'] if 'label' in data_subset.columns else None,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        print(f\"Scenario '{scenario['prompt_name']}' - Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
    "    else:\n",
    "        # Use all data for training\n",
    "        train_data = data_subset\n",
    "        val_data = None  # No validation set\n",
    "        print(f\"Scenario '{scenario['prompt_name']}' - Train size (all data): {len(train_data)}, No validation set\")\n",
    "    \n",
    "    scenario_runs = []\n",
    "    best_prompt_overall = None\n",
    "    best_accuracy_overall = -1\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        best_prompt, best_accuracy, iteration_rows = run_iterative_prompt_improvement(\n",
    "            scenario=scenario,\n",
    "            train_data=train_data,\n",
    "            val_data=val_data,  # This can now be None\n",
    "            annotation_columns=annotation_columns,\n",
    "            labels=labels,\n",
    "            alt_test=True,\n",
    "            errors_examples=0.5,\n",
    "            examples_to_give=4,\n",
    "            epsilon=epsilon,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Store the results from this run\n",
    "        for row in iteration_rows:\n",
    "            row['run'] = run + 1\n",
    "            scenario_runs.append(row)\n",
    "        \n",
    "        # Track the best prompt across all runs\n",
    "        if best_accuracy > best_accuracy_overall:\n",
    "            best_accuracy_overall = best_accuracy\n",
    "            best_prompt_overall = best_prompt\n",
    "    \n",
    "    # Store all detailed results\n",
    "    all_detailed_results.extend(scenario_runs)\n",
    "    \n",
    "    # Group rows by iteration\n",
    "    iterations = set([row['iteration'] for row in scenario_runs])\n",
    "    for iteration in iterations:\n",
    "        iteration_rows = [row for row in scenario_runs if row['iteration'] == iteration]\n",
    "        \n",
    "        # Compute aggregated metrics for this iteration\n",
    "        aggregated_metrics = {}\n",
    "        for metric in ['kappa_train', 'kappa_val', 'accuracy_train', 'accuracy_val', \n",
    "                      'winning_rate_train', 'avg_adv_prob_train', 'winning_rate_val', \n",
    "                      'avg_adv_prob_val', 'tokens_used', 'cost', 'running_time_s']:\n",
    "            # Filter out None or NaN values before computing mean\n",
    "            valid_values = [row[metric] for row in iteration_rows if row[metric] is not None and not (isinstance(row[metric], float) and np.isnan(row[metric]))]\n",
    "            aggregated_metrics[metric] = np.mean(valid_values) if valid_values else None\n",
    "        \n",
    "        # Handle p-values specially - compute mean p-values for each annotator\n",
    "        if 'p_values_train' in iteration_rows[0] and iteration_rows[0]['p_values_train'] is not None:\n",
    "            # Get the number of annotators (length of p_values list)\n",
    "            n_annotators = len(iteration_rows[0]['p_values_train'])\n",
    "            \n",
    "            # Initialize lists to store p-values for each annotator across runs\n",
    "            p_values_train_by_annotator = [[] for _ in range(n_annotators)]\n",
    "            p_values_val_by_annotator = [[] for _ in range(n_annotators)]\n",
    "            \n",
    "            # Collect p-values for each annotator across runs\n",
    "            for row in iteration_rows:\n",
    "                if row['p_values_train'] is not None:\n",
    "                    for i, p_val in enumerate(row['p_values_train']):\n",
    "                        if not np.isnan(p_val):\n",
    "                            p_values_train_by_annotator[i].append(p_val)\n",
    "                \n",
    "                if use_validation_set and 'p_values_val' in row and row['p_values_val'] is not None:\n",
    "                    for i, p_val in enumerate(row['p_values_val']):\n",
    "                        if not np.isnan(p_val):\n",
    "                            p_values_val_by_annotator[i].append(p_val)\n",
    "            \n",
    "            # Compute mean p-values for each annotator\n",
    "            mean_p_values_train = [np.mean(p_vals) if p_vals else np.nan for p_vals in p_values_train_by_annotator]\n",
    "            \n",
    "            # Store mean p-values\n",
    "            aggregated_metrics['p_values_train'] = mean_p_values_train\n",
    "            \n",
    "            # Compute passed_alt_test based on mean p-values with proper Benjamini-Yekutieli correction\n",
    "            alpha = 0.05\n",
    "            # Filter out NaN values for the correction procedure\n",
    "            valid_p_values_train = [p_val for p_val in mean_p_values_train if not np.isnan(p_val)]\n",
    "            \n",
    "            # Apply Benjamini-Yekutieli correction to the p-values\n",
    "            if valid_p_values_train:\n",
    "                train_rejections = benjamini_yekutieli_correction(valid_p_values_train, alpha=alpha)\n",
    "                winning_rate_train = np.mean(train_rejections)\n",
    "                aggregated_metrics['passed_alt_test_train'] = winning_rate_train >= 0.5\n",
    "            else:\n",
    "                aggregated_metrics['passed_alt_test_train'] = None\n",
    "            \n",
    "            # Handle validation p-values if using validation set\n",
    "            if use_validation_set:\n",
    "                mean_p_values_val = [np.mean(p_vals) if p_vals else np.nan for p_vals in p_values_val_by_annotator]\n",
    "                aggregated_metrics['p_values_val'] = mean_p_values_val\n",
    "                \n",
    "                # Apply Benjamini-Yekutieli correction to validation p-values\n",
    "                valid_p_values_val = [p_val for p_val in mean_p_values_val if not np.isnan(p_val)]\n",
    "                if valid_p_values_val:\n",
    "                    val_rejections = benjamini_yekutieli_correction(valid_p_values_val, alpha=alpha)\n",
    "                    winning_rate_val = np.mean(val_rejections)\n",
    "                    aggregated_metrics['passed_alt_test_val'] = winning_rate_val >= 0.5\n",
    "                else:\n",
    "                    aggregated_metrics['passed_alt_test_val'] = None\n",
    "            else:\n",
    "                aggregated_metrics['p_values_val'] = None\n",
    "                aggregated_metrics['passed_alt_test_val'] = None\n",
    "        \n",
    "        # Add other necessary fields\n",
    "        aggregated_metrics['data_set'] = iteration_rows[0]['data_set']\n",
    "        aggregated_metrics['N_train'] = iteration_rows[0]['N_train']\n",
    "        aggregated_metrics['N_val'] = iteration_rows[0]['N_val']\n",
    "        aggregated_metrics['model'] = iteration_rows[0]['model']\n",
    "        aggregated_metrics['prompt_name'] = iteration_rows[0]['prompt_name']\n",
    "        aggregated_metrics['iteration'] = iteration\n",
    "        aggregated_metrics['n_runs'] = n_runs\n",
    "        aggregated_metrics['use_validation_set'] = use_validation_set\n",
    "        \n",
    "        # Add human annotator accuracies\n",
    "        for annotator in annotation_columns:\n",
    "            train_acc_key = f\"{annotator}_train_acc\"\n",
    "            val_acc_key = f\"{annotator}_val_acc\"\n",
    "            \n",
    "            # Filter out None or NaN values before computing mean\n",
    "            valid_train_values = [row[train_acc_key] for row in iteration_rows if train_acc_key in row and row[train_acc_key] is not None and not (isinstance(row[train_acc_key], float) and np.isnan(row[train_acc_key]))]\n",
    "            \n",
    "            aggregated_metrics[train_acc_key] = np.mean(valid_train_values) if valid_train_values else None\n",
    "            \n",
    "            # Only compute validation metrics if using validation set\n",
    "            if use_validation_set:\n",
    "                valid_val_values = [row[val_acc_key] for row in iteration_rows if val_acc_key in row and row[val_acc_key] is not None and not (isinstance(row[val_acc_key], float) and np.isnan(row[val_acc_key]))]\n",
    "                aggregated_metrics[val_acc_key] = np.mean(valid_val_values) if valid_val_values else None\n",
    "            else:\n",
    "                aggregated_metrics[val_acc_key] = None\n",
    "        \n",
    "        all_aggregated_results.append(aggregated_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Create the final summary dataframe\n",
    "summary_df = pd.DataFrame(all_aggregated_results)\n",
    "\n",
    "# Define the desired column order for summary dataframe\n",
    "summary_columns = [\n",
    "    'data_set', 'N_train', 'N_val', 'model', 'prompt_name', 'iteration', 'n_runs', 'use_validation_set',\n",
    "    'kappa_train', 'kappa_val', 'accuracy_train', 'accuracy_val',\n",
    "    'winning_rate_train', 'passed_alt_test_train', 'avg_adv_prob_train', \n",
    "    'winning_rate_val', 'passed_alt_test_val', 'avg_adv_prob_val',\n",
    "    'tokens_used', 'cost', 'running_time_s'\n",
    "]\n",
    "\n",
    "# Add human annotator columns\n",
    "for annotator in annotation_columns:\n",
    "    summary_columns.extend([f\"{annotator}_train_acc\", f\"{annotator}_val_acc\"])\n",
    "\n",
    "# Add p-values columns if they exist\n",
    "if 'p_values_train' in summary_df.columns:\n",
    "    summary_columns.extend(['p_values_train', 'p_values_val'])\n",
    "\n",
    "# Reorder columns (only include columns that exist in the dataframe)\n",
    "available_columns = [col for col in summary_columns if col in summary_df.columns]\n",
    "summary_df = summary_df[available_columns]\n",
    "\n",
    "# Store the detailed results in a separate dataframe\n",
    "detailed_df = pd.DataFrame(all_detailed_results)\n",
    "\n",
    "# Define the desired column order for detailed dataframe\n",
    "detailed_columns = [\n",
    "    'run', 'data_set', 'N_train', 'N_val', 'model', 'prompt_name', 'iteration',\n",
    "    'kappa_train', 'kappa_val', 'accuracy_train', 'accuracy_val',\n",
    "    'winning_rate_train', 'passed_alt_test_train', 'avg_adv_prob_train',\n",
    "    'winning_rate_val', 'passed_alt_test_val', 'avg_adv_prob_val',\n",
    "    'tokens_used', 'cost', 'running_time_s'\n",
    "]\n",
    "\n",
    "# Add human annotator columns\n",
    "for annotator in annotation_columns:\n",
    "    detailed_columns.extend([f\"{annotator}_train_acc\", f\"{annotator}_val_acc\"])\n",
    "\n",
    "# Add p-values columns if they exist\n",
    "if 'p_values_train' in detailed_df.columns:\n",
    "    detailed_columns.extend(['p_values_train', 'p_values_val'])\n",
    "\n",
    "# Reorder columns (only include columns that exist in the dataframe)\n",
    "available_detailed_columns = [col for col in detailed_columns if col in detailed_df.columns]\n",
    "detailed_df = detailed_df[available_detailed_columns]\n",
    "\n",
    "# Store the best prompt for reference\n",
    "best_prompt_result = best_prompt_overall\n",
    "\n",
    "summary_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
