{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kids Reflect vLLM Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will analyze the Kids Reflect dataset using both Azure OpenAI and vLLM for comparison. This notebook is specifically optimized for running on a supercomputer environment, taking advantage of multiple GPUs and high-performance computing resources.\n",
    "\n",
    "To help you navigate this notebook, here is a step-by-step outline of what we will do:\n",
    "\n",
    "1. **Configure vLLM for Supercomputer Environment**  \n",
    "   - Set environment variables to optimize vLLM for high-performance computing\n",
    "   - Verify GPU availability and configuration\n",
    "\n",
    "2. **Load and Preprocess the Dataset**  \n",
    "   - Load the Kids Reflect dataset\n",
    "   - Clean and normalize text columns\n",
    "   - Convert integer columns to the appropriate data type\n",
    "   - Create verbatim text for analysis\n",
    "\n",
    "3. **Prepare Training and Validation Data**  \n",
    "   - Filter labeled data\n",
    "   - Split data into training and validation sets\n",
    "\n",
    "4. **Define Prompt Templates and Scenarios**  \n",
    "   - Create templates for both Azure OpenAI and vLLM scenarios\n",
    "   - Configure model parameters for optimal performance\n",
    "\n",
    "5. **Run Iterative Prompt Improvement**  \n",
    "   - Execute each scenario separately to monitor progress\n",
    "   - Track GPU usage during execution\n",
    "\n",
    "6. **Analyze and Visualize Results**  \n",
    "   - Compare performance between Azure OpenAI and vLLM\n",
    "   - Visualize kappa values across iterations\n",
    "   - Save results for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure vLLM for Supercomputer Environment\n",
    "\n",
    "Before we begin, we need to configure vLLM to take full advantage of the supercomputer environment. This involves setting environment variables that control how vLLM utilizes the available GPU resources.\n",
    "\n",
    "### Key Configuration Parameters:\n",
    "\n",
    "- **VLLM_MODEL_PATH**: Path to the model or HuggingFace model ID\n",
    "- **VLLM_DTYPE**: Data type for model weights (float16 for efficiency)\n",
    "- **VLLM_GPU_MEMORY_UTILIZATION**: Target GPU memory utilization (0.95 or 95% for supercomputers)\n",
    "- **VLLM_TENSOR_PARALLEL_SIZE**: Number of GPUs to use for tensor parallelism (4 for multi-GPU setups)\n",
    "- **VLLM_MAX_MODEL_LEN**: Maximum sequence length (2048 tokens)\n",
    "- **VLLM_ENABLE_PREFIX_CACHING**: Enable prefix caching for better performance\n",
    "- **VLLM_WORKER_MULTIPROC_METHOD**: Worker multiprocessing method (spawn for better compatibility)\n",
    "\n",
    "These settings are optimized for high-performance computing environments with multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set vLLM environment variables for supercomputer\n",
    "%env VLLM_MODEL_PATH=TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "%env VLLM_DTYPE=float16\n",
    "%env VLLM_GPU_MEMORY_UTILIZATION=0.95\n",
    "%env VLLM_TENSOR_PARALLEL_SIZE=4\n",
    "%env VLLM_MAX_MODEL_LEN=2048\n",
    "%env VLLM_ENABLE_PREFIX_CACHING=true\n",
    "%env VLLM_WORKER_MULTIPROC_METHOD=spawn\n",
    "\n",
    "# Display current configuration\n",
    "!echo \"Current vLLM configuration:\"\n",
    "!echo \"VLLM_MODEL_PATH: $VLLM_MODEL_PATH\"\n",
    "!echo \"VLLM_GPU_MEMORY_UTILIZATION: $VLLM_GPU_MEMORY_UTILIZATION\"\n",
    "!echo \"VLLM_TENSOR_PARALLEL_SIZE: $VLLM_TENSOR_PARALLEL_SIZE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability\n",
    "\n",
    "Before proceeding, it's important to verify that GPUs are available and properly configured. This step helps identify any potential issues with GPU allocation or configuration before running the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup\n",
    "\n",
    "Now we'll import the necessary libraries and modules for our analysis. The qualitative_analysis package provides functions for data loading, preprocessing, and model interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from qualitative_analysis import (\n",
    "    clean_and_normalize,\n",
    "    load_data,\n",
    "    sanitize_dataframe,\n",
    ")\n",
    "from qualitative_analysis.prompt_engineering import run_iterative_prompt_improvement\n",
    "from qualitative_analysis.alt_test import benjamini_yekutieli_correction\n",
    "# Define data directory\n",
    "data_dir = 'exploratory_data'\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The Kids Reflect dataset contains entries from children who engaged in a four-step process to formulate divergent questions about a reference text. Each entry includes:\n",
    "\n",
    "- **Reference**: The text that children read beforehand\n",
    "- **IDENTIFY**: Where the child identifies a knowledge gap related to the reference text\n",
    "- **GUESS**: Where the child makes a guess about what the answer could be\n",
    "- **SEEK**: Where the child formulates a question to seek the answer\n",
    "- **ASSESS**: Where the child evaluates whether an answer was found\n",
    "\n",
    "The dataset also includes validity ratings for each step and overall mechanical ratings, as well as annotations from three human raters (Chloe, Oli, and Gaia).\n",
    "\n",
    "### Data Preprocessing Steps\n",
    "\n",
    "1. Load the dataset from the Excel file\n",
    "2. Clean and normalize text columns\n",
    "3. Convert integer columns to the appropriate data type\n",
    "4. Sanitize the DataFrame to handle any inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your dataset\n",
    "data_file_path = os.path.join(data_dir, 'Kids_Reflect_3anno.xlsx')\n",
    "\n",
    "# Load the data\n",
    "data = load_data(data_file_path, file_type='xlsx', delimiter=';')\n",
    "\n",
    "# 1) Now define the new column names for cleaning\n",
    "text_columns = [\"reference\", \"IDENTIFY\", \"GUESS\", \"SEEK\", \"ASSESS\", \"assess_cues\"]\n",
    "integer_columns = [\"Identify_validity\", \"Guess_validity\", \"Seek_validity\", \"Assess_validity\", \"mechanical_rating\", \"Rater_Chloe\", \"Rater_Oli\", \"Rater_Gaia\"]\n",
    "\n",
    "# 2) Clean and normalize the new columns\n",
    "for col in text_columns:\n",
    "    data[col] = clean_and_normalize(data[col])\n",
    "\n",
    "# 3) Convert selected columns to integers, preserving NaNs\n",
    "for col in integer_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors=\"coerce\").astype(\"Int64\")  # Uses nullable integer type\n",
    "\n",
    "# 4) Sanitize the DataFrame\n",
    "data = sanitize_dataframe(data)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Verbatim Text\n",
    "\n",
    "Now we'll combine the different columns into a single verbatim text for each entry. This format makes it easier for the language model to process the entire entry as a cohesive unit.\n",
    "\n",
    "The verbatim text includes:\n",
    "- The unique key identifier\n",
    "- The reference text\n",
    "- The IDENTIFY, GUESS, SEEK, and ASSESS steps\n",
    "- The validity ratings for each step\n",
    "- The mechanical rating (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine texts and entries\n",
    "data['verbatim'] = data.apply(\n",
    "    lambda row: (\n",
    "        f\"key: {row['key']}\\n\\n\"\n",
    "        f\"reference: {row['reference']}\\n\\n\"\n",
    "        f\"IDENTIFY: {row['IDENTIFY']}\\n\\n\"\n",
    "        f\"GUESS: {row['GUESS']}\\n\\n\"\n",
    "        f\"SEEK: {row['SEEK']}\\n\\n\"\n",
    "        f\"ASSESS: {row['ASSESS']}\\n\\n\"\n",
    "        f\"assess_cues: {row['assess_cues']}\\n\\n\"\n",
    "        f\"Identify_validity: {row['Identify_validity']}\\n\\n\"\n",
    "        f\"Guess_validity: {row['Guess_validity']}\\n\\n\"\n",
    "        f\"Seek_validity: {row['Seek_validity']}\\n\\n\"\n",
    "        f\"Assess_validity: {row['Assess_validity']}\\n\\n\"\n",
    "        f\"mechanical_rating: {row['mechanical_rating']}\\n\\n\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Extract the list of verbatims\n",
    "verbatims = data['verbatim'].tolist()\n",
    "\n",
    "print(f\"Total number of verbatims: {len(verbatims)}\")\n",
    "print(f\"Verbatim example:\\n{verbatims[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training and Validation Data\n",
    "\n",
    "To evaluate the performance of our models, we need to split the data into training and validation sets. We'll use the training set to train the models and the validation set to evaluate their performance.\n",
    "\n",
    "### Steps:\n",
    "1. Identify labeled data (entries with annotations from all three raters)\n",
    "2. Create a subset of the labeled data for analysis\n",
    "3. Split the subset into training (70%) and validation (30%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the columns that represent your human ratings\n",
    "annotation_columns = ['Rater_Chloe', 'Rater_Oli', 'Rater_Gaia']\n",
    "\n",
    "# Filter labeled data (drop rows with NaN in any annotation column)\n",
    "labeled_data = data.dropna(subset=annotation_columns)\n",
    "\n",
    "# Filter unlabeled data\n",
    "unlabeled_data = data[~data.index.isin(labeled_data.index)]\n",
    "\n",
    "print(\"Number of labeled rows:\", len(labeled_data))\n",
    "print(\"Number of unlabeled rows:\", len(unlabeled_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Templates\n",
    "\n",
    "Now we'll define the prompt templates that will be used to instruct the language models. These templates include:\n",
    "\n",
    "1. **Common Template**: The main instructions for evaluating the validity of a cycle\n",
    "2. **Response Template**: The format in which the model should provide its response\n",
    "\n",
    "The templates include detailed instructions on how to evaluate each step of the cycle and determine overall validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "annotation_columns = ['Rater_Chloe', 'Rater_Oli', 'Rater_Gaia']\n",
    "labels = [0,1]\n",
    "epsilon = 0.2\n",
    "\n",
    "# Scenarios define the configuration for each experiment run\n",
    "# You can specify different providers and models by changing:\n",
    "#   - provider_llm1: The provider for the main LLM (e.g., \"azure\", \"openai\", \"together\", \"vllm\")\n",
    "#   - model_name_llm1: The model name for the main LLM\n",
    "#   - provider_llm2: The provider for the improver LLM\n",
    "#   - model_name_llm2: The model name for the improver LLM\n",
    "#\n",
    "# For vLLM provider:\n",
    "#   - The model_name_llm1 should be a HuggingFace model ID (e.g., \"meta-llama/Llama-2-7b-chat-hf\")\n",
    "#   - For testing, you can use a small model like \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "#\n",
    "# Other parameters can be adjusted as needed (max_iterations, json_output, etc.)\n",
    "scenarios = [\n",
    "    {\n",
    "        \"provider_llm1\": \"azure\",\n",
    "        \"model_name_llm1\": \"gpt-4o\",\n",
    "        \"temperature_llm1\": 0,\n",
    "\n",
    "        # For the \"improver\" LLM2\n",
    "        \"provider_llm2\": \"azure\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "\n",
    "        \"max_iterations\": 1,\n",
    "        \"n_completions\": 1,\n",
    "        \"prompt_name\": \"Basic\",\n",
    "        \n",
    "        # Data configuration\n",
    "        \"subsample_size\": 74,  # Size of data subset to use\n",
    "        \"use_validation_set\": False,  # Whether to use a validation set\n",
    "        \"validation_size\": 10,  # Size of validation set (if used)\n",
    "        \"random_state\": 42,  # Random state for reproducibility\n",
    "\n",
    "        # Our initial prompt\n",
    "        \"template\": \"\"\"\n",
    "Here is an entry to evaluate:\n",
    "{verbatim_text}\n",
    "\n",
    "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
    "\"\"\",\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"json_output\": True,\n",
    "        \"selected_fields\": [\"Classification\"],\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\":\n",
    "        \"\"\"\n",
    "Please follow the JSON format below:\n",
    "```json\n",
    "{{\n",
    "  \"Reasoning\": \"Your text here\",\n",
    "  \"Classification\": \"Your integer here\"\n",
    "}}\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"provider_llm1\": \"vllm\",\n",
    "        \"model_name_llm1\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"temperature_llm1\": 0,\n",
    "\n",
    "        # For the \"improver\" LLM2\n",
    "        \"provider_llm2\": \"azure\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "\n",
    "        \"max_iterations\": 1,\n",
    "        \"n_completions\": 1,\n",
    "        \"prompt_name\": \"Basic\",\n",
    "        \n",
    "        # Data configuration\n",
    "        \"subsample_size\": 74,  # Size of data subset to use\n",
    "        \"use_validation_set\": False,  # Whether to use a validation set\n",
    "        \"validation_size\": 10,  # Size of validation set (if used)\n",
    "        \"random_state\": 42,  # Random state for reproducibility\n",
    "\n",
    "        # Our initial prompt\n",
    "        \"template\": \"\"\"\n",
    "Here is an entry to evaluate:\n",
    "{verbatim_text}\n",
    "\n",
    "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
    "\"\"\",\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"json_output\": True,\n",
    "        \"selected_fields\": [\"Classification\"],\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\":\n",
    "        \"\"\"\n",
    "Please follow the JSON format below:\n",
    "```json\n",
    "{{\n",
    "  \"Reasoning\": \"Your text here\",\n",
    "  \"Classification\": \"Your integer here\"\n",
    "}}\n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Number of runs per scenario (can be adjusted)\n",
    "n_runs = 3\n",
    "\n",
    "# For the final summary dataframe\n",
    "all_aggregated_results = []\n",
    "\n",
    "# For storing all individual run results\n",
    "all_detailed_results = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    # Extract data configuration from scenario\n",
    "    subsample_size = scenario.get(\"subsample_size\", 20)\n",
    "    use_validation_set = scenario.get(\"use_validation_set\", True)\n",
    "    validation_size = scenario.get(\"validation_size\", 10)\n",
    "    random_state = scenario.get(\"random_state\", 42)\n",
    "    \n",
    "    # Step 1: Get a stratified subset of samples\n",
    "    data_subset, _ = train_test_split(\n",
    "        labeled_data,\n",
    "        train_size=subsample_size,\n",
    "        # stratify=labeled_data['label'] if 'label' in labeled_data.columns else None,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Step 2: Split subset into train/val if use_validation_set is True\n",
    "    if use_validation_set:\n",
    "        train_data, val_data = train_test_split(\n",
    "            data_subset,\n",
    "            test_size=validation_size,\n",
    "            # stratify=data_subset['label'] if 'label' in data_subset.columns else None,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        print(f\"Scenario '{scenario['prompt_name']}' - Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
    "    else:\n",
    "        # Use all data for training\n",
    "        train_data = data_subset\n",
    "        val_data = None  # No validation set\n",
    "        print(f\"Scenario '{scenario['prompt_name']}' - Train size (all data): {len(train_data)}, No validation set\")\n",
    "    \n",
    "    scenario_runs = []\n",
    "    best_prompt_overall = None\n",
    "    best_accuracy_overall = -1\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        best_prompt, best_accuracy, iteration_rows = run_iterative_prompt_improvement(\n",
    "            scenario=scenario,\n",
    "            train_data=train_data,\n",
    "            val_data=val_data,  # This can now be None\n",
    "            annotation_columns=annotation_columns,\n",
    "            labels=labels,\n",
    "            alt_test=True,\n",
    "            errors_examples=0.5,\n",
    "            examples_to_give=4,\n",
    "            epsilon=epsilon,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Store the results from this run\n",
    "        for row in iteration_rows:\n",
    "            row['run'] = run + 1\n",
    "            scenario_runs.append(row)\n",
    "        \n",
    "        # Track the best prompt across all runs\n",
    "        if best_accuracy > best_accuracy_overall:\n",
    "            best_accuracy_overall = best_accuracy\n",
    "            best_prompt_overall = best_prompt\n",
    "    \n",
    "    # Store all detailed results\n",
    "    all_detailed_results.extend(scenario_runs)\n",
    "    \n",
    "    # Group rows by iteration\n",
    "    iterations = set([row['iteration'] for row in scenario_runs])\n",
    "    for iteration in iterations:\n",
    "        iteration_rows = [row for row in scenario_runs if row['iteration'] == iteration]\n",
    "        \n",
    "        # Compute aggregated metrics for this iteration\n",
    "        aggregated_metrics = {}\n",
    "        for metric in ['kappa_train', 'kappa_val', 'accuracy_train', 'accuracy_val', \n",
    "                      'winning_rate_train', 'avg_adv_prob_train', 'winning_rate_val', \n",
    "                      'avg_adv_prob_val', 'tokens_used', 'cost', 'running_time_s']:\n",
    "            # Filter out None or NaN values before computing mean\n",
    "            valid_values = [row[metric] for row in iteration_rows if row[metric] is not None and not (isinstance(row[metric], float) and np.isnan(row[metric]))]\n",
    "            aggregated_metrics[metric] = np.mean(valid_values) if valid_values else None\n",
    "        \n",
    "        # Handle p-values specially - compute mean p-values for each annotator\n",
    "        if 'p_values_train' in iteration_rows[0] and iteration_rows[0]['p_values_train'] is not None:\n",
    "            # Get the number of annotators (length of p_values list)\n",
    "            n_annotators = len(iteration_rows[0]['p_values_train'])\n",
    "            \n",
    "            # Initialize lists to store p-values for each annotator across runs\n",
    "            p_values_train_by_annotator = [[] for _ in range(n_annotators)]\n",
    "            p_values_val_by_annotator = [[] for _ in range(n_annotators)]\n",
    "            \n",
    "            # Collect p-values for each annotator across runs\n",
    "            for row in iteration_rows:\n",
    "                if row['p_values_train'] is not None:\n",
    "                    for i, p_val in enumerate(row['p_values_train']):\n",
    "                        if not np.isnan(p_val):\n",
    "                            p_values_train_by_annotator[i].append(p_val)\n",
    "                \n",
    "                if use_validation_set and 'p_values_val' in row and row['p_values_val'] is not None:\n",
    "                    for i, p_val in enumerate(row['p_values_val']):\n",
    "                        if not np.isnan(p_val):\n",
    "                            p_values_val_by_annotator[i].append(p_val)\n",
    "            \n",
    "            # Compute mean p-values for each annotator\n",
    "            mean_p_values_train = [np.mean(p_vals) if p_vals else np.nan for p_vals in p_values_train_by_annotator]\n",
    "            \n",
    "            # Store mean p-values\n",
    "            aggregated_metrics['p_values_train'] = mean_p_values_train\n",
    "            \n",
    "            # Compute passed_alt_test based on mean p-values with proper Benjamini-Yekutieli correction\n",
    "            alpha = 0.05\n",
    "            # Filter out NaN values for the correction procedure\n",
    "            valid_p_values_train = [p_val for p_val in mean_p_values_train if not np.isnan(p_val)]\n",
    "            \n",
    "            # Apply Benjamini-Yekutieli correction to the p-values\n",
    "            if valid_p_values_train:\n",
    "                train_rejections = benjamini_yekutieli_correction(valid_p_values_train, alpha=alpha)\n",
    "                winning_rate_train = np.mean(train_rejections)\n",
    "                aggregated_metrics['passed_alt_test_train'] = winning_rate_train >= 0.5\n",
    "            else:\n",
    "                aggregated_metrics['passed_alt_test_train'] = None\n",
    "            \n",
    "            # Handle validation p-values if using validation set\n",
    "            if use_validation_set:\n",
    "                mean_p_values_val = [np.mean(p_vals) if p_vals else np.nan for p_vals in p_values_val_by_annotator]\n",
    "                aggregated_metrics['p_values_val'] = mean_p_values_val\n",
    "                \n",
    "                # Apply Benjamini-Yekutieli correction to validation p-values\n",
    "                valid_p_values_val = [p_val for p_val in mean_p_values_val if not np.isnan(p_val)]\n",
    "                if valid_p_values_val:\n",
    "                    val_rejections = benjamini_yekutieli_correction(valid_p_values_val, alpha=alpha)\n",
    "                    winning_rate_val = np.mean(val_rejections)\n",
    "                    aggregated_metrics['passed_alt_test_val'] = winning_rate_val >= 0.5\n",
    "                else:\n",
    "                    aggregated_metrics['passed_alt_test_val'] = None\n",
    "            else:\n",
    "                aggregated_metrics['p_values_val'] = None\n",
    "                aggregated_metrics['passed_alt_test_val'] = None\n",
    "        \n",
    "        # Add other necessary fields\n",
    "        aggregated_metrics['data_set'] = iteration_rows[0]['data_set']\n",
    "        aggregated_metrics['N_train'] = iteration_rows[0]['N_train']\n",
    "        aggregated_metrics['N_val'] = iteration_rows[0]['N_val']\n",
    "        aggregated_metrics['model'] = iteration_rows[0]['model']\n",
    "        aggregated_metrics['prompt_name'] = iteration_rows[0]['prompt_name']\n",
    "        aggregated_metrics['iteration'] = iteration\n",
    "        aggregated_metrics['n_runs'] = n_runs\n",
    "        aggregated_metrics['use_validation_set'] = use_validation_set\n",
    "        \n",
    "        # Add human annotator accuracies\n",
    "        for annotator in annotation_columns:\n",
    "            train_acc_key = f\"{annotator}_train_acc\"\n",
    "            val_acc_key = f\"{annotator}_val_acc\"\n",
    "            \n",
    "            # Filter out None or NaN values before computing mean\n",
    "            valid_train_values = [row[train_acc_key] for row in iteration_rows if train_acc_key in row and row[train_acc_key] is not None and not (isinstance(row[train_acc_key], float) and np.isnan(row[train_acc_key]))]\n",
    "            \n",
    "            aggregated_metrics[train_acc_key] = np.mean(valid_train_values) if valid_train_values else None\n",
    "            \n",
    "            # Only compute validation metrics if using validation set\n",
    "            if use_validation_set:\n",
    "                valid_val_values = [row[val_acc_key] for row in iteration_rows if val_acc_key in row and row[val_acc_key] is not None and not (isinstance(row[val_acc_key], float) and np.isnan(row[val_acc_key]))]\n",
    "                aggregated_metrics[val_acc_key] = np.mean(valid_val_values) if valid_val_values else None\n",
    "            else:\n",
    "                aggregated_metrics[val_acc_key] = None\n",
    "        \n",
    "        all_aggregated_results.append(aggregated_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Create the final summary dataframe\n",
    "summary_df = pd.DataFrame(all_aggregated_results)\n",
    "\n",
    "# Define the desired column order for summary dataframe\n",
    "summary_columns = [\n",
    "    'data_set', 'N_train', 'N_val', 'model', 'prompt_name', 'iteration', 'n_runs', 'use_validation_set',\n",
    "    'kappa_train', 'kappa_val', 'accuracy_train', 'accuracy_val',\n",
    "    'winning_rate_train', 'passed_alt_test_train', 'avg_adv_prob_train', \n",
    "    'winning_rate_val', 'passed_alt_test_val', 'avg_adv_prob_val',\n",
    "    'tokens_used', 'cost', 'running_time_s'\n",
    "]\n",
    "\n",
    "# Add human annotator columns\n",
    "for annotator in annotation_columns:\n",
    "    summary_columns.extend([f\"{annotator}_train_acc\", f\"{annotator}_val_acc\"])\n",
    "\n",
    "# Add p-values columns if they exist\n",
    "if 'p_values_train' in summary_df.columns:\n",
    "    summary_columns.extend(['p_values_train', 'p_values_val'])\n",
    "\n",
    "# Reorder columns (only include columns that exist in the dataframe)\n",
    "available_columns = [col for col in summary_columns if col in summary_df.columns]\n",
    "summary_df = summary_df[available_columns]\n",
    "\n",
    "# Store the detailed results in a separate dataframe\n",
    "detailed_df = pd.DataFrame(all_detailed_results)\n",
    "\n",
    "# Define the desired column order for detailed dataframe\n",
    "detailed_columns = [\n",
    "    'run', 'data_set', 'N_train', 'N_val', 'model', 'prompt_name', 'iteration',\n",
    "    'kappa_train', 'kappa_val', 'accuracy_train', 'accuracy_val',\n",
    "    'winning_rate_train', 'passed_alt_test_train', 'avg_adv_prob_train',\n",
    "    'winning_rate_val', 'passed_alt_test_val', 'avg_adv_prob_val',\n",
    "    'tokens_used', 'cost', 'running_time_s'\n",
    "]\n",
    "\n",
    "# Add human annotator columns\n",
    "for annotator in annotation_columns:\n",
    "    detailed_columns.extend([f\"{annotator}_train_acc\", f\"{annotator}_val_acc\"])\n",
    "\n",
    "# Add p-values columns if they exist\n",
    "if 'p_values_train' in detailed_df.columns:\n",
    "    detailed_columns.extend(['p_values_train', 'p_values_val'])\n",
    "\n",
    "# Reorder columns (only include columns that exist in the dataframe)\n",
    "available_detailed_columns = [col for col in detailed_columns if col in detailed_df.columns]\n",
    "detailed_df = detailed_df[available_detailed_columns]\n",
    "\n",
    "# Store the best prompt for reference\n",
    "best_prompt_result = best_prompt_overall\n",
    "\n",
    "summary_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
