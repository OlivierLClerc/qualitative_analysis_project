{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kids Reflect vLLM Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will analyze the Kids Reflect dataset using both Azure OpenAI and vLLM for comparison. This notebook is specifically optimized for running on a supercomputer environment, taking advantage of multiple GPUs and high-performance computing resources.\n",
    "\n",
    "To help you navigate this notebook, here is a step-by-step outline of what we will do:\n",
    "\n",
    "1. **Configure vLLM for Supercomputer Environment**  \n",
    "   - Set environment variables to optimize vLLM for high-performance computing\n",
    "   - Verify GPU availability and configuration\n",
    "\n",
    "2. **Load and Preprocess the Dataset**  \n",
    "   - Load the Kids Reflect dataset\n",
    "   - Clean and normalize text columns\n",
    "   - Convert integer columns to the appropriate data type\n",
    "   - Create verbatim text for analysis\n",
    "\n",
    "3. **Prepare Training and Validation Data**  \n",
    "   - Filter labeled data\n",
    "   - Split data into training and validation sets\n",
    "\n",
    "4. **Define Prompt Templates and Scenarios**  \n",
    "   - Create templates for both Azure OpenAI and vLLM scenarios\n",
    "   - Configure model parameters for optimal performance\n",
    "\n",
    "5. **Run Iterative Prompt Improvement**  \n",
    "   - Execute each scenario separately to monitor progress\n",
    "   - Track GPU usage during execution\n",
    "\n",
    "6. **Analyze and Visualize Results**  \n",
    "   - Compare performance between Azure OpenAI and vLLM\n",
    "   - Visualize kappa values across iterations\n",
    "   - Save results for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure vLLM for Supercomputer Environment\n",
    "\n",
    "Before we begin, we need to configure vLLM to take full advantage of the supercomputer environment. This involves setting environment variables that control how vLLM utilizes the available GPU resources.\n",
    "\n",
    "### Key Configuration Parameters:\n",
    "\n",
    "- **VLLM_MODEL_PATH**: Path to the model or HuggingFace model ID\n",
    "- **VLLM_DTYPE**: Data type for model weights (float16 for efficiency)\n",
    "- **VLLM_GPU_MEMORY_UTILIZATION**: Target GPU memory utilization (0.95 or 95% for supercomputers)\n",
    "- **VLLM_TENSOR_PARALLEL_SIZE**: Number of GPUs to use for tensor parallelism (4 for multi-GPU setups)\n",
    "- **VLLM_MAX_MODEL_LEN**: Maximum sequence length (2048 tokens)\n",
    "- **VLLM_ENABLE_PREFIX_CACHING**: Enable prefix caching for better performance\n",
    "- **VLLM_WORKER_MULTIPROC_METHOD**: Worker multiprocessing method (spawn for better compatibility)\n",
    "\n",
    "These settings are optimized for high-performance computing environments with multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: VLLM_MODEL_PATH=TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "env: VLLM_DTYPE=float16\n",
      "env: VLLM_GPU_MEMORY_UTILIZATION=0.95\n",
      "env: VLLM_TENSOR_PARALLEL_SIZE=4\n",
      "env: VLLM_MAX_MODEL_LEN=2048\n",
      "env: VLLM_ENABLE_PREFIX_CACHING=true\n",
      "env: VLLM_WORKER_MULTIPROC_METHOD=spawn\n",
      "\"Current vLLM configuration:\"\n",
      "\"VLLM_MODEL_PATH: $VLLM_MODEL_PATH\"\n",
      "\"VLLM_GPU_MEMORY_UTILIZATION: $VLLM_GPU_MEMORY_UTILIZATION\"\n",
      "\"VLLM_TENSOR_PARALLEL_SIZE: $VLLM_TENSOR_PARALLEL_SIZE\"\n"
     ]
    }
   ],
   "source": [
    "# Set vLLM environment variables for supercomputer\n",
    "%env VLLM_MODEL_PATH=TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "%env VLLM_DTYPE=float16\n",
    "%env VLLM_GPU_MEMORY_UTILIZATION=0.95\n",
    "%env VLLM_TENSOR_PARALLEL_SIZE=4\n",
    "%env VLLM_MAX_MODEL_LEN=2048\n",
    "%env VLLM_ENABLE_PREFIX_CACHING=true\n",
    "%env VLLM_WORKER_MULTIPROC_METHOD=spawn\n",
    "\n",
    "# Display current configuration\n",
    "!echo \"Current vLLM configuration:\"\n",
    "!echo \"VLLM_MODEL_PATH: $VLLM_MODEL_PATH\"\n",
    "!echo \"VLLM_GPU_MEMORY_UTILIZATION: $VLLM_GPU_MEMORY_UTILIZATION\"\n",
    "!echo \"VLLM_TENSOR_PARALLEL_SIZE: $VLLM_TENSOR_PARALLEL_SIZE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability\n",
    "\n",
    "Before proceeding, it's important to verify that GPUs are available and properly configured. This step helps identify any potential issues with GPU allocation or configuration before running the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup\n",
    "\n",
    "Now we'll import the necessary libraries and modules for our analysis. The qualitative_analysis package provides functions for data loading, preprocessing, and model interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: vLLM is not available. VLLMLLMClient will not be usable.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from qualitative_analysis import (\n",
    "    clean_and_normalize,\n",
    "    load_data,\n",
    "    sanitize_dataframe,\n",
    ")\n",
    "from qualitative_analysis.prompt_engineering import run_iterative_prompt_improvement\n",
    "\n",
    "# Define data directory\n",
    "data_dir = 'exploratory_data'\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The Kids Reflect dataset contains entries from children who engaged in a four-step process to formulate divergent questions about a reference text. Each entry includes:\n",
    "\n",
    "- **Reference**: The text that children read beforehand\n",
    "- **IDENTIFY**: Where the child identifies a knowledge gap related to the reference text\n",
    "- **GUESS**: Where the child makes a guess about what the answer could be\n",
    "- **SEEK**: Where the child formulates a question to seek the answer\n",
    "- **ASSESS**: Where the child evaluates whether an answer was found\n",
    "\n",
    "The dataset also includes validity ratings for each step and overall mechanical ratings, as well as annotations from three human raters (Chloe, Oli, and Gaia).\n",
    "\n",
    "### Data Preprocessing Steps\n",
    "\n",
    "1. Load the dataset from the Excel file\n",
    "2. Clean and normalize text columns\n",
    "3. Convert integer columns to the appropriate data type\n",
    "4. Sanitize the DataFrame to handle any inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>key</th>\n",
       "      <th>reference</th>\n",
       "      <th>IDENTIFY</th>\n",
       "      <th>GUESS</th>\n",
       "      <th>SEEK</th>\n",
       "      <th>ASSESS</th>\n",
       "      <th>identify_cues</th>\n",
       "      <th>guess_cues</th>\n",
       "      <th>...</th>\n",
       "      <th>Identify_validity</th>\n",
       "      <th>Guess_validity</th>\n",
       "      <th>Seek_validity</th>\n",
       "      <th>Assess_validity</th>\n",
       "      <th>mechanical_rating</th>\n",
       "      <th>Rater_Oli</th>\n",
       "      <th>Unvalid_Oli</th>\n",
       "      <th>Rater_Gaia</th>\n",
       "      <th>Unvalid_Gaia</th>\n",
       "      <th>Invalid_Gaia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aac13</td>\n",
       "      <td>3</td>\n",
       "      <td>aac13_3</td>\n",
       "      <td>Toutankhamon etait un pharaon, un roi de l'Egy...</td>\n",
       "      <td>L'Egypte antique</td>\n",
       "      <td>C'est un ancien pays de l'Afrique</td>\n",
       "      <td>Qu'est ce que l'Egypte antique</td>\n",
       "      <td>Qui</td>\n",
       "      <td>{\"1\":\"L'Egypte antique\",\"2\":\"Le principe de mo...</td>\n",
       "      <td>{\"1\":\"C'est un ancien pays de l'Afrique\",\"2\":\"...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aac13</td>\n",
       "      <td>4</td>\n",
       "      <td>aac13_4</td>\n",
       "      <td>La tres grande majorite de lor disponible dans...</td>\n",
       "      <td>Les composants electriques</td>\n",
       "      <td>Les composants electroniques sont par exemple ...</td>\n",
       "      <td>Qu'est-ce qu'une composants electroniques</td>\n",
       "      <td>Non</td>\n",
       "      <td>{\"1\":\"Utilité de l'or pour les couronnes denta...</td>\n",
       "      <td>{\"1\":\"Les couronnes en or sont plus solides et...</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aac13</td>\n",
       "      <td>5</td>\n",
       "      <td>aac13_5</td>\n",
       "      <td>Des scientifiques ont revele lexistence de tra...</td>\n",
       "      <td>Des premiers humains l'Australie</td>\n",
       "      <td>C'est un pays du Sud</td>\n",
       "      <td>Ou se trouve l'Australie</td>\n",
       "      <td>Oui</td>\n",
       "      <td>{\"1\":\"La formation des traces dans les roches\"...</td>\n",
       "      <td>{\"1\":\"Ces traces se forment automatiquement qu...</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aac13</td>\n",
       "      <td>6</td>\n",
       "      <td>aac13_6</td>\n",
       "      <td>La religion de la Grece antique comprend plusi...</td>\n",
       "      <td>Une mythologie</td>\n",
       "      <td>L'olympe est un endroit en Grece</td>\n",
       "      <td>Qu'est ce qu'une mythologie</td>\n",
       "      <td>Non</td>\n",
       "      <td>{\"1\":\"Une mythologie\",\"2\":\"Les autres mytholog...</td>\n",
       "      <td>{\"1\":\"Une mythologie est un ensemble de contes...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aac24</td>\n",
       "      <td>3</td>\n",
       "      <td>aac24_3</td>\n",
       "      <td>Toutankhamon etait un pharaon, un roi de l'Egy...</td>\n",
       "      <td>Toutankhamon</td>\n",
       "      <td>Roi Pharaon</td>\n",
       "      <td>Quand les Pharaons sont-ils apparus</td>\n",
       "      <td>J'ai trouve ma reponse</td>\n",
       "      <td>{\"1\":\"L'Egypte antique\",\"2\":\"Le principe de mo...</td>\n",
       "      <td>{\"1\":\"C'est un ancien pays de l'Afrique\",\"2\":\"...</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  Iteration      key  \\\n",
       "0  aac13          3  aac13_3   \n",
       "1  aac13          4  aac13_4   \n",
       "2  aac13          5  aac13_5   \n",
       "3  aac13          6  aac13_6   \n",
       "4  aac24          3  aac24_3   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Toutankhamon etait un pharaon, un roi de l'Egy...   \n",
       "1  La tres grande majorite de lor disponible dans...   \n",
       "2  Des scientifiques ont revele lexistence de tra...   \n",
       "3  La religion de la Grece antique comprend plusi...   \n",
       "4  Toutankhamon etait un pharaon, un roi de l'Egy...   \n",
       "\n",
       "                           IDENTIFY  \\\n",
       "0                  L'Egypte antique   \n",
       "1        Les composants electriques   \n",
       "2  Des premiers humains l'Australie   \n",
       "3                    Une mythologie   \n",
       "4                      Toutankhamon   \n",
       "\n",
       "                                               GUESS  \\\n",
       "0                 C'est un ancien pays de l'Afrique    \n",
       "1  Les composants electroniques sont par exemple ...   \n",
       "2                               C'est un pays du Sud   \n",
       "3                   L'olympe est un endroit en Grece   \n",
       "4                                        Roi Pharaon   \n",
       "\n",
       "                                        SEEK                  ASSESS  \\\n",
       "0             Qu'est ce que l'Egypte antique                     Qui   \n",
       "1  Qu'est-ce qu'une composants electroniques                     Non   \n",
       "2                   Ou se trouve l'Australie                     Oui   \n",
       "3                Qu'est ce qu'une mythologie                     Non   \n",
       "4        Quand les Pharaons sont-ils apparus  J'ai trouve ma reponse   \n",
       "\n",
       "                                       identify_cues  \\\n",
       "0  {\"1\":\"L'Egypte antique\",\"2\":\"Le principe de mo...   \n",
       "1  {\"1\":\"Utilité de l'or pour les couronnes denta...   \n",
       "2  {\"1\":\"La formation des traces dans les roches\"...   \n",
       "3  {\"1\":\"Une mythologie\",\"2\":\"Les autres mytholog...   \n",
       "4  {\"1\":\"L'Egypte antique\",\"2\":\"Le principe de mo...   \n",
       "\n",
       "                                          guess_cues  ... Identify_validity  \\\n",
       "0  {\"1\":\"C'est un ancien pays de l'Afrique\",\"2\":\"...  ...                 1   \n",
       "1  {\"1\":\"Les couronnes en or sont plus solides et...  ...                 3   \n",
       "2  {\"1\":\"Ces traces se forment automatiquement qu...  ...                 2   \n",
       "3  {\"1\":\"Une mythologie est un ensemble de contes...  ...                 1   \n",
       "4  {\"1\":\"C'est un ancien pays de l'Afrique\",\"2\":\"...  ...              <NA>   \n",
       "\n",
       "  Guess_validity Seek_validity  Assess_validity  mechanical_rating  Rater_Oli  \\\n",
       "0              1             1             <NA>               <NA>       <NA>   \n",
       "1           <NA>          <NA>             <NA>               <NA>       <NA>   \n",
       "2              3             3             <NA>                  0       <NA>   \n",
       "3              3             1             <NA>                  0       <NA>   \n",
       "4           <NA>             3             <NA>               <NA>       <NA>   \n",
       "\n",
       "   Unvalid_Oli  Rater_Gaia  Unvalid_Gaia  Invalid_Gaia  \n",
       "0        False        <NA>         False         False  \n",
       "1        False        <NA>         False         False  \n",
       "2        False        <NA>         False         False  \n",
       "3        False        <NA>         False         False  \n",
       "4        False        <NA>         False         False  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to your dataset\n",
    "data_file_path = os.path.join(data_dir, 'Kids_Reflect_3anno.xlsx')\n",
    "\n",
    "# Load the data\n",
    "data = load_data(data_file_path, file_type='xlsx', delimiter=';')\n",
    "\n",
    "# 1) Now define the new column names for cleaning\n",
    "text_columns = [\"reference\", \"IDENTIFY\", \"GUESS\", \"SEEK\", \"ASSESS\", \"assess_cues\"]\n",
    "integer_columns = [\"Identify_validity\", \"Guess_validity\", \"Seek_validity\", \"Assess_validity\", \"mechanical_rating\", \"Rater_Chloe\", \"Rater_Oli\", \"Rater_Gaia\"]\n",
    "\n",
    "# 2) Clean and normalize the new columns\n",
    "for col in text_columns:\n",
    "    data[col] = clean_and_normalize(data[col])\n",
    "\n",
    "# 3) Convert selected columns to integers, preserving NaNs\n",
    "for col in integer_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors=\"coerce\").astype(\"Int64\")  # Uses nullable integer type\n",
    "\n",
    "# 4) Sanitize the DataFrame\n",
    "data = sanitize_dataframe(data)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Verbatim Text\n",
    "\n",
    "Now we'll combine the different columns into a single verbatim text for each entry. This format makes it easier for the language model to process the entire entry as a cohesive unit.\n",
    "\n",
    "The verbatim text includes:\n",
    "- The unique key identifier\n",
    "- The reference text\n",
    "- The IDENTIFY, GUESS, SEEK, and ASSESS steps\n",
    "- The validity ratings for each step\n",
    "- The mechanical rating (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of verbatims: 921\n",
      "Verbatim example:\n",
      "key: aac13_3\n",
      "\n",
      "reference: Toutankhamon etait un pharaon, un roi de l'Egypte antique. Il est tres connu aujourdhui parce que des archeologues ont retrouve son cercueil intacte avec tous ses tresors, en 1922. Pour les Egyptiens, il y avait une vie apres la mort, une vie eternelle. Cest pour cela que le corps devait etre conserve dans le meilleur etat possible, cest ce quon appelle la momification. Cest aussi pour cela que lon retrouve aujourdhui de la nourriture, des armes ou des tresors dans les tombeaux. Ces objets accompagnaient le pharaon dans sa vie apres la mort.\n",
      "\n",
      "IDENTIFY: L'Egypte antique\n",
      "\n",
      "GUESS: C'est un ancien pays de l'Afrique \n",
      "\n",
      "SEEK: Qu'est ce que l'Egypte antique\n",
      "\n",
      "ASSESS: Qui\n",
      "\n",
      "assess_cues: {\"1\":\"L'Egypte antique est une ancienne civilisation de l'Afrique du Nord\",\"2\":\"La momification revient a conserver le corps dans une boite et le mettre dans une piece sans lumiere et sans air\",\"3\":\"Le premier Pharaon a vecu a 3000 av. J.-C.\"}\n",
      "\n",
      "Identify_validity: 1\n",
      "\n",
      "Guess_validity: 1\n",
      "\n",
      "Seek_validity: 1\n",
      "\n",
      "Assess_validity: <NA>\n",
      "\n",
      "mechanical_rating: <NA>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine texts and entries\n",
    "data['verbatim'] = data.apply(\n",
    "    lambda row: (\n",
    "        f\"key: {row['key']}\\n\\n\"\n",
    "        f\"reference: {row['reference']}\\n\\n\"\n",
    "        f\"IDENTIFY: {row['IDENTIFY']}\\n\\n\"\n",
    "        f\"GUESS: {row['GUESS']}\\n\\n\"\n",
    "        f\"SEEK: {row['SEEK']}\\n\\n\"\n",
    "        f\"ASSESS: {row['ASSESS']}\\n\\n\"\n",
    "        f\"assess_cues: {row['assess_cues']}\\n\\n\"\n",
    "        f\"Identify_validity: {row['Identify_validity']}\\n\\n\"\n",
    "        f\"Guess_validity: {row['Guess_validity']}\\n\\n\"\n",
    "        f\"Seek_validity: {row['Seek_validity']}\\n\\n\"\n",
    "        f\"Assess_validity: {row['Assess_validity']}\\n\\n\"\n",
    "        f\"mechanical_rating: {row['mechanical_rating']}\\n\\n\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Extract the list of verbatims\n",
    "verbatims = data['verbatim'].tolist()\n",
    "\n",
    "print(f\"Total number of verbatims: {len(verbatims)}\")\n",
    "print(f\"Verbatim example:\\n{verbatims[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training and Validation Data\n",
    "\n",
    "To evaluate the performance of our models, we need to split the data into training and validation sets. We'll use the training set to train the models and the validation set to evaluate their performance.\n",
    "\n",
    "### Steps:\n",
    "1. Identify labeled data (entries with annotations from all three raters)\n",
    "2. Create a subset of the labeled data for analysis\n",
    "3. Split the subset into training (70%) and validation (30%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled rows: 75\n",
      "Number of unlabeled rows: 846\n"
     ]
    }
   ],
   "source": [
    "# Identify the columns that represent your human ratings\n",
    "annotation_columns = ['Rater_Chloe', 'Rater_Oli', 'Rater_Gaia']\n",
    "\n",
    "# Filter labeled data (drop rows with NaN in any annotation column)\n",
    "labeled_data = data.dropna(subset=annotation_columns)\n",
    "\n",
    "# Filter unlabeled data\n",
    "unlabeled_data = data[~data.index.isin(labeled_data.index)]\n",
    "\n",
    "print(\"Number of labeled rows:\", len(labeled_data))\n",
    "print(\"Number of unlabeled rows:\", len(unlabeled_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 21\n",
      "Val size: 9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "subsample_size = 30\n",
    "\n",
    "# Step 1: Get a stratified subset of samples\n",
    "data_subset, _ = train_test_split(\n",
    "    labeled_data,\n",
    "    train_size=subsample_size,\n",
    "    # stratify=data['label'],  # Uncomment if you have a label column to stratify on\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Split subset into train/val\n",
    "train_data, val_data = train_test_split(\n",
    "    data_subset,\n",
    "    test_size=0.3,\n",
    "    # stratify=data_subset['label'],  # Uncomment if you have a label column to stratify on\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_data))\n",
    "print(\"Val size:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Templates\n",
    "\n",
    "Now we'll define the prompt templates that will be used to instruct the language models. These templates include:\n",
    "\n",
    "1. **Common Template**: The main instructions for evaluating the validity of a cycle\n",
    "2. **Response Template**: The format in which the model should provide its response\n",
    "\n",
    "The templates include detailed instructions on how to evaluate each step of the cycle and determine overall validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "annotation_columns = ['Rater_Chloe', 'Rater_Oli', 'Rater_Gaia']\n",
    "labels = [0,1]\n",
    "epsilon = 0.2\n",
    "\n",
    "# Define the common template for both scenarios\n",
    "common_template = \"\"\"\n",
    "You are an assistant that evaluates data entries.\n",
    "\n",
    "You are provided with data entries in the following format:\n",
    "\n",
    "The data has the following columns:\n",
    "- \"key\": Unique identifiant\n",
    "- \"reference\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
    "- \"IDENTIFY\": Response for the IDENTIFY step\n",
    "- \"GUESS\": Response for the GUESS step\n",
    "- \"SEEK\": Response for the SEEK step\n",
    "- \"ASSESS\": Response for the ASSESS step\n",
    "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
    "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
    "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
    "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
    "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
    "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
    "\n",
    "\n",
    "Here is an entry to evaluate:\n",
    "{verbatim_text}\n",
    "\n",
    "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
    "If it's empty, you'll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
    "\n",
    "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
    "\n",
    "- Identify Step: Does the Identify step indicate a topic of interest?\n",
    "- Guess Step: Does the Guess step suggest a possible explanation?\n",
    "- Seek Step: Is the Seek step formulated as a question?\n",
    "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
    "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
    "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
    "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
    "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
    "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? → If not, then no answer was actually found, and the cycle is not valid.\n",
    "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. → If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
    "\n",
    "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
    "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step's validity.\n",
    "\n",
    "If all these criteria are met, the cycle is valid.\n",
    "Validity is expressed as:\n",
    "1: Valid cycle\n",
    "0: Invalid cycle\n",
    "\n",
    "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
    "\"\"\"\n",
    "\n",
    "# Define the common response template for both scenarios\n",
    "common_response_template = \"\"\"\n",
    "Please follow the JSON format below:\n",
    "```json\n",
    "{{\n",
    "  \"Reasoning\": \"Your text here\",\n",
    "  \"Classification\": \"Your integer here\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Scenarios and GPU Monitoring\n",
    "\n",
    "We'll define two scenarios for our analysis:\n",
    "\n",
    "1. **Azure OpenAI with GPT-4o**: This scenario uses Azure's hosted GPT-4o model\n",
    "2. **vLLM with Llama-2-7b-chat**: This scenario uses vLLM to run the Llama 2 model locally on the supercomputer\n",
    "\n",
    "We'll also define a function to monitor GPU usage during execution, which is particularly useful for tracking resource utilization on the supercomputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "# Function to monitor GPU usage during execution\n",
    "def monitor_gpu():\n",
    "    !nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv\n",
    "    \n",
    "# Check GPU status before starting\n",
    "monitor_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "    # Azure OpenAI scenario\n",
    "    {\n",
    "        \"provider_llm1\": \"azure\",\n",
    "        \"model_name_llm1\": \"gpt-4o\",\n",
    "        \"temperature_llm1\": 0,\n",
    "\n",
    "        # For the \"improver\" LLM2\n",
    "        \"provider_llm2\": \"azure\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "\n",
    "        \"max_iterations\": 4,\n",
    "        \"n_completions\": 1,\n",
    "        \"prompt_name\": \"Azure-GPT4o\",\n",
    "\n",
    "        # Our initial prompt\n",
    "        \"template\": common_template,\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"json_output\": True,\n",
    "        \"selected_fields\": [\"Classification\"],\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\": common_response_template,\n",
    "    },\n",
    "    \n",
    "    # vLLM scenario with a larger model (adjust based on your supercomputer's capabilities)\n",
    "    {\n",
    "        \"provider_llm1\": \"vllm\",\n",
    "        \"model_name_llm1\": \"meta-llama/Llama-2-7b-chat-hf\",  # Or another model available on your supercomputer\n",
    "        \"temperature_llm1\": 0.1,\n",
    "        \n",
    "        # For the \"improver\" LLM2, still use Azure\n",
    "        \"provider_llm2\": \"azure\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "        \n",
    "        \"max_iterations\": 4,  # Can increase this since you have more compute\n",
    "        \"n_completions\": 1,\n",
    "        \"prompt_name\": \"vLLM-Llama2-7B\",\n",
    "        \n",
    "        # Same template as the Azure scenario\n",
    "        \"template\": common_template,\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"json_output\": True,\n",
    "        \"selected_fields\": [\"Classification\"],\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\": common_response_template,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Iterative Prompt Improvement\n",
    "\n",
    "Now we'll run the iterative prompt improvement process for each scenario. This process involves:\n",
    "\n",
    "1. Using the initial prompt to classify the training data\n",
    "2. Evaluating the performance on the validation data\n",
    "3. Improving the prompt based on the errors made\n",
    "4. Repeating the process for a specified number of iterations\n",
    "\n",
    "We'll run each scenario separately to better monitor progress and resource usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure OpenAI Scenario\n",
    "\n",
    "First, we'll run the Azure OpenAI scenario using GPT-4o. This will serve as our baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Azure OpenAI scenario...\n",
      "\n",
      "=== Iteration 1/4 ===\n",
      "\n",
      "=== Processing Verbatim 1/21 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "You are provided with data entries in the following format:\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"reference\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "key: bc22_8\n",
      "\n",
      "reference: La foret tropicale humide a un climat qui fournit beaucoup d'eau et une temperature elevee toute l'annee. Cela favorise la densite et la croissance permanente des plantes de ces forets.  En Asie, on appelle ce genre de foret 'la jungle'. Elle est parfois appelee foret dense, bien que cette expression signifie seulement que les arbres sont tres proches les uns des autres. Dans les forets tropicales, les arbres formant trois etages de hauteur differentes, sont tres serres et perdent leurs feuilles irregulierement.\n",
      "\n",
      "IDENTIFY: La plante des forets tropicales\n",
      "\n",
      "GUESS: Les forctes tropicale contiennent tous type de olante\n",
      "\n",
      "SEEK: Quelles plantes put on voir dans les forets tropicales\n",
      "\n",
      "ASSESS: Les foret tropicales sont toujours verte car il pleut tout le temps\n",
      "\n",
      "assess_cues: {\"4\":\"Les forets tropicales sont toujours vertes car il pleut tout le temps\",\"2\":\"La temperature dans les forets tropicales est entre 30 et 40C\",\"3\":\"La foret tropicale se trouve en Asie, en Amerique du Sud et en Afrique\"}\n",
      "\n",
      "Identify_validity: 1\n",
      "\n",
      "Guess_validity: 1\n",
      "\n",
      "Seek_validity: 1\n",
      "\n",
      "Assess_validity: 4\n",
      "\n",
      "mechanical_rating: 0\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it's empty, you'll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? → If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. → If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step's validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The mechanical_rating column has a numeric value of 0, which overrides any other logic in the codebook. Therefore, the cycle is classified as invalid without further evaluation of the individual steps.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Critical Error processing verbatim 1\n",
      "Error Type: AttributeError\n",
      "Error Message: 'dict' object has no attribute 'prompt_tokens'\n",
      "\n",
      "=== Processing Verbatim 2/21 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "You are provided with data entries in the following format:\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"reference\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "key: bc12_8\n",
      "\n",
      "reference: La foret tropicale humide a un climat qui fournit beaucoup d'eau et une temperature elevee toute l'annee. Cela favorise la densite et la croissance permanente des plantes de ces forets.  En Asie, on appelle ce genre de foret 'la jungle'. Elle est parfois appelee foret dense, bien que cette expression signifie seulement que les arbres sont tres proches les uns des autres. Dans les forets tropicales, les arbres formant trois etages de hauteur differentes, sont tres serres et perdent leurs feuilles irregulierement.\n",
      "\n",
      "IDENTIFY: La foret tropicale humide a un climat qui fournit beaucoup d'eau et une temperature elevee toute l'annee.\n",
      "\n",
      "GUESS: Les fortes tropicales contiennent tous types de plantes La temperature dans les forets tropicales est tres forte Les forets tropicales se trouvent dans differents endroits du monde\n",
      "\n",
      "SEEK: Dans la foret tropicaleil y a tout types de plantes\n",
      "\n",
      "ASSESS: La temperature dans les forets tropicales est entre 30 et 40C\n",
      "\n",
      "assess_cues: {\"4\":\"Les forets tropicales sont toujours vertes car il pleut tout le temps\",\"2\":\"La temperature dans les forets tropicales est entre 30 et 40C\",\"3\":\"La foret tropicale se trouve en Asie, en Amerique du Sud et en Afrique\"}\n",
      "\n",
      "Identify_validity: <NA>\n",
      "\n",
      "Guess_validity: <NA>\n",
      "\n",
      "Seek_validity: <NA>\n",
      "\n",
      "Assess_validity: 2\n",
      "\n",
      "mechanical_rating: <NA>\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it's empty, you'll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? → If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. → If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step's validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The mechanical_rating is empty, so we need to evaluate the cycle validity based on the provided criteria. \\n\\n1. Identify Step: The Identify step mentions 'La foret tropicale humide a un climat qui fournit beaucoup d'eau et une temperature elevee toute l'annee', which indicates a topic of interest related to the reference text. \\n\\n2. Guess Step: The Guess step suggests possible explanations related to the topic, such as the types of plants in tropical forests and their global distribution. \\n\\n3. Seek Step: The Seek step is not formulated as a question; it states 'Dans la foret tropicale il y a tout types de plantes', which is a declarative statement. \\n\\n4. Assess Step: The Assess step provides an answer 'La temperature dans les forets tropicales est entre 30 et 40C', which is found in the assess_cues. \\n\\n5. Consistency: The Identify, Guess, and Seek steps are related to the topic of tropical forests but not consistently to the same question. \\n\\n6. Reference Link: The steps are related to the topic of the reference text, which is about tropical forests. \\n\\n7. Seek Question Originality: The Seek step does not pose a question, but the statement about types of plants is not directly answered in the reference text. \\n\\n8. Resolving Answer: The Assess step does not resolve the Seek step since the Seek step is not a question. \\n\\n9. Valid Answer: The Assess step provides an answer found in the assess_cues, which is valid. \\n\\n10. Valid No: Not applicable since an answer was found. \\n\\nOverall, the cycle is invalid due to the Seek step not being a question and inconsistency in addressing the same question across steps.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Critical Error processing verbatim 2\n",
      "Error Type: AttributeError\n",
      "Error Message: 'dict' object has no attribute 'prompt_tokens'\n",
      "\n",
      "=== Processing Verbatim 3/21 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "You are provided with data entries in the following format:\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"reference\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "key: bc2_7\n",
      "\n",
      "reference: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite. Selon la legende, Rome aurait ete fondee par Romulus qui aurai donne son nom a la ville. Au depart, ce n'etait qu'un groupe de quelques villages, puis l'Empire romain va couvrir une grande partie de l'Europe et entourer toute la mer Mediterranee. La religion romaine comptait de nombreux dieux, inspires en partie des dieux grecs.\n",
      "\n",
      "IDENTIFY: C'est quoi la Rome?\n",
      "\n",
      "GUESS: C'est peut etre un peuple\n",
      "\n",
      "SEEK: C'est quoi la Rome antique ?\n",
      "\n",
      "ASSESS: Je n'ai pas pu trouver ma reponse\n",
      "\n",
      "assess_cues: {\"1\":\"Dans la legende, Romulus est le premier roi de Rome\",\"2\":\"L'Antiquite s'est etendue entre 3200 avant J.C, jusqu'a l'annee 476\",\"4\":\"La Mediterranee touche plusieurs continents\"}\n",
      "\n",
      "Identify_validity: <NA>\n",
      "\n",
      "Guess_validity: <NA>\n",
      "\n",
      "Seek_validity: <NA>\n",
      "\n",
      "Assess_validity: <NA>\n",
      "\n",
      "mechanical_rating: <NA>\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it's empty, you'll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? → If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. → If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step's validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The cycle is evaluated based on the provided criteria: 1) Identify Step: The Identify step 'C'est quoi la Rome?' indicates a topic of interest related to Rome. 2) Guess Step: The Guess step 'C'est peut etre un peuple' suggests a possible explanation, indicating that Rome might be a people. 3) Seek Step: The Seek step 'C'est quoi la Rome antique ?' is formulated as a question, asking specifically about ancient Rome. 4) Assess Step: The Assess step states 'Je n'ai pas pu trouver ma reponse', indicating that no answer was found, which is acceptable. 5) Consistency: The Identify, Guess, and Seek steps are related to the same question about Rome. 6) Reference Link: The steps are related to the topic of the reference text, which is about ancient Rome. 7) Seek Question Originality: The answer to the Seek question is not found in the reference text, as the text does not explicitly define 'Rome antique'. 8) Valid No: The Assess step indicates no answer was found, and the answer to the Seek question is not present in the assess_cues. All criteria are met, making the cycle valid.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Critical Error processing verbatim 3\n",
      "Error Type: AttributeError\n",
      "Error Message: 'dict' object has no attribute 'prompt_tokens'\n",
      "\n",
      "=== Processing Verbatim 4/21 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "You are provided with data entries in the following format:\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"reference\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "key: bc14_4\n",
      "\n",
      "reference: La tres grande majorite de lor disponible dans le monde est utilisee en bijouterie et pour sa fonction decorative : vetements, monuments ou uvres dart... Mais lor a dautres usages plus inattendus : les dentistes lutilisent pour fabriquer des couronnes dentaires par exemple. Presque tous les produits electroniques (telephone, ordinateur...) contiennent de lor en tres petite quantite car il permet de bien connecter les composantes. L'or est egalement utilise dans la fabrication des equipements spatiaux.\n",
      "\n",
      "IDENTIFY: Pourquoi cela permet de bien connecter les appareils electroniques\n",
      "\n",
      "GUESS: Car l'or est une masse electromagnetique qui permet de connecter ses objets electriques\n",
      "\n",
      "SEEK: Pourquoi Laure permet de connecter les objets electroniques\n",
      "\n",
      "ASSESS: Leur permet de bien connecter les composants internes des equipements spatiaux\n",
      "\n",
      "assess_cues: {\"4\":\"Les couronnes en or sont plus cheres\",\"2\":\"L'or permet de bien connecter les composants internes des equipement spatiaux\",\"3\":\"Le cuivre est un tres bon conducteur pas cher\"}\n",
      "\n",
      "Identify_validity: <NA>\n",
      "\n",
      "Guess_validity: <NA>\n",
      "\n",
      "Seek_validity: <NA>\n",
      "\n",
      "Assess_validity: 2\n",
      "\n",
      "mechanical_rating: <NA>\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it's empty, you'll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? → If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. → If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step's validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The cycle is evaluated based on the provided criteria. The Identify step indicates a topic of interest related to the conductivity of gold in electronic devices. The Guess step suggests a possible explanation, although it is not scientifically accurate, it does attempt to provide a reason. The Seek step is formulated as a question asking why gold is used to connect electronic components. The Assess step provides an answer that is found in the assess_cues, specifically 'L'or permet de bien connecter les composants internes des equipement spatiaux', which is consistent with the topic of conductivity. The Identify, Guess, and Seek steps are related to the same question about gold's conductivity and are linked to the reference text, which mentions gold's use in electronics. The answer to the Seek question is not found in the reference text, ensuring originality. The Assess step provides a resolving answer that matches the question in the Seek step. The Assess step validity is confirmed by the presence of the answer in the assess_cues. Therefore, all criteria for a valid cycle are met.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Critical Error processing verbatim 4\n",
      "Error Type: AttributeError\n",
      "Error Message: 'dict' object has no attribute 'prompt_tokens'\n",
      "\n",
      "=== Processing Verbatim 5/21 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "You are provided with data entries in the following format:\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"reference\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "key: bc12_4\n",
      "\n",
      "reference: La tres grande majorite de lor disponible dans le monde est utilisee en bijouterie et pour sa fonction decorative : vetements, monuments ou uvres dart... Mais lor a dautres usages plus inattendus : les dentistes lutilisent pour fabriquer des couronnes dentaires par exemple. Presque tous les produits electroniques (telephone, ordinateur...) contiennent de lor en tres petite quantite car il permet de bien connecter les composantes. L'or est egalement utilise dans la fabrication des equipements spatiaux.\n",
      "\n",
      "IDENTIFY: Laure est aussi utilise dans la fabrication des equipement spatiaux\n",
      "\n",
      "GUESS: Les telephones ordinateurs les appareils electroniques contiennent de l'or en tout petit quantite\n",
      "\n",
      "SEEK: ?\n",
      "\n",
      "ASSESS: L'infirmier deja connecte le composant interne desequipements spatiaux\n",
      "\n",
      "assess_cues: {\"4\":\"Les couronnes en or sont plus cheres\",\"2\":\"L'or permet de bien connecter les composants internes des equipement spatiaux\",\"3\":\"Le cuivre est un tres bon conducteur pas cher\"}\n",
      "\n",
      "Identify_validity: 2\n",
      "\n",
      "Guess_validity: <NA>\n",
      "\n",
      "Seek_validity: <NA>\n",
      "\n",
      "Assess_validity: 2\n",
      "\n",
      "mechanical_rating: <NA>\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it's empty, you'll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? → If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. → If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step's validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The mechanical_rating is empty, so we need to evaluate the cycle validity based on the provided criteria. The Identify step is valid as indicated by Identify_validity. The Guess step is not explicitly validated, but it suggests a possible explanation related to the topic of electronics containing gold. The Seek step is not formulated as a question, which is a requirement for validity. The Assess step is valid as indicated by Assess_validity, and it provides an answer related to the components of space equipment, which is consistent with the reference text. However, the Seek step is not a question, which makes the cycle invalid. Additionally, the consistency between Identify, Guess, and Seek steps is not fully established due to the lack of a question in the Seek step. Therefore, the cycle is invalid.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Critical Error processing verbatim 5\n",
      "Error Type: AttributeError\n",
      "Error Message: 'dict' object has no attribute 'prompt_tokens'\n",
      "\n",
      "=== Processing Verbatim 6/21 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "You are provided with data entries in the following format:\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"reference\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "key: bc22_7\n",
      "\n",
      "reference: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite. Selon la legende, Rome aurait ete fondee par Romulus qui aurai donne son nom a la ville. Au depart, ce n'etait qu'un groupe de quelques villages, puis l'Empire romain va couvrir une grande partie de l'Europe et entourer toute la mer Mediterranee. La religion romaine comptait de nombreux dieux, inspires en partie des dieux grecs.\n",
      "\n",
      "IDENTIFY: Romulus\n",
      "\n",
      "GUESS: Romulus est un roi de rome\n",
      "\n",
      "SEEK: Qui est Romulus ?\n",
      "\n",
      "ASSESS: Dans la legende Romulus est le premier roi de rome\n",
      "\n",
      "assess_cues: {\"1\":\"Dans la legende, Romulus est le premier roi de Rome\",\"2\":\"L'Antiquite s'est etendue entre 3200 avant J.C, jusqu'a l'annee 476\",\"4\":\"La Mediterranee touche plusieurs continents\"}\n",
      "\n",
      "Identify_validity: 1\n",
      "\n",
      "Guess_validity: 1\n",
      "\n",
      "Seek_validity: 1\n",
      "\n",
      "Assess_validity: 1\n",
      "\n",
      "mechanical_rating: 1\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it's empty, you'll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? → If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. → If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step's validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The mechanical_rating column has a numeric value of 1, which overrides any other logic in the codebook. Therefore, the cycle is automatically considered valid based on this rating.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Critical Error processing verbatim 6\n",
      "Error Type: AttributeError\n",
      "Error Message: 'dict' object has no attribute 'prompt_tokens'\n",
      "\n",
      "=== Processing Verbatim 7/21 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "You are provided with data entries in the following format:\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"reference\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "key: bc13_6\n",
      "\n",
      "reference: La religion de la Grece antique comprend plusieurs dieux. Les aventures de ces dieux forment la mythologie grecque, l'une des mythologies les plus developpees de l'histoire antique. D'apres les Grecs, les 12 principaux dieux vivent sur l'Olympe et ont une apparence et un comportement comparables a celui des humains. Les dieux les plus connus sont : Athena, deesse de la guerre, Zeus, roi des dieux et dieu de la foudre. Hera, deesse du mariage. Poseidon, dieu des oceans.\n",
      "\n",
      "IDENTIFY: Combien de dieux il y a dans grece antique\n",
      "\n",
      "GUESS: 10\n",
      "\n",
      "SEEK: Combien de dieux il y a dans grece antique\n",
      "\n",
      "ASSESS: Il y a pas la reponse a ma question\n",
      "\n",
      "assess_cues: {\"1\":\"Une mythologie est un ensemble de legendes liees a une civilisation\",\"2\":\"L'histoire comprend beaucoup de mythologies\",\"3\":\"L'Olympe est la plus haute montagne de Grece\"}\n",
      "\n",
      "Identify_validity: <NA>\n",
      "\n",
      "Guess_validity: <NA>\n",
      "\n",
      "Seek_validity: <NA>\n",
      "\n",
      "Assess_validity: <NA>\n",
      "\n",
      "mechanical_rating: <NA>\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it's empty, you'll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? → If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. → If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step's validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The cycle is evaluated based on the provided criteria: 1. Identify Step: The Identify step 'Combien de dieux il y a dans grece antique' indicates a topic of interest related to the number of gods in ancient Greece. 2. Guess Step: The Guess step '10' suggests a possible explanation or answer to the question. 3. Seek Step: The Seek step 'Combien de dieux il y a dans grece antique' is formulated as a question. 4. Assess Step: The Assess step 'Il y a pas la reponse a ma question' states that no answer was found, which is acceptable. 5. Consistency: The Identify, Guess, and Seek steps are related to the same question about the number of gods in ancient Greece. 6. Reference Link: The steps are related to the topic of the reference text, which discusses Greek gods. 7. Seek Question Originality: The answer to the Seek question is not found in the reference text, as the text mentions 12 gods but does not directly answer the question. 8. Resolving Answer: The Assess step states no answer was found, which is consistent with the lack of a direct answer in the reference text. 9. Valid No: The Assess step correctly identifies that the answer is not present in the assess_cues. Since all criteria are met, the cycle is valid.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Critical Error processing verbatim 7\n",
      "Error Type: AttributeError\n",
      "Error Message: 'dict' object has no attribute 'prompt_tokens'\n",
      "\n",
      "=== Processing Verbatim 8/21 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "You are provided with data entries in the following format:\n",
      "\n",
      "The data has the following columns:\n",
      "- \"key\": Unique identifiant\n",
      "- \"reference\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "- \"IDENTIFY\": Response for the IDENTIFY step\n",
      "- \"GUESS\": Response for the GUESS step\n",
      "- \"SEEK\": Response for the SEEK step\n",
      "- \"ASSESS\": Response for the ASSESS step\n",
      "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
      "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
      "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
      "\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "key: bc2_3\n",
      "\n",
      "reference: Toutankhamon etait un pharaon, un roi de l'Egypte antique. Il est tres connu aujourdhui parce que des archeologues ont retrouve son cercueil intacte avec tous ses tresors, en 1922. Pour les Egyptiens, il y avait une vie apres la mort, une vie eternelle. Cest pour cela que le corps devait etre conserve dans le meilleur etat possible, cest ce quon appelle la momification. Cest aussi pour cela que lon retrouve aujourdhui de la nourriture, des armes ou des tresors dans les tombeaux. Ces objets accompagnaient le pharaon dans sa vie apres la mort.\n",
      "\n",
      "IDENTIFY: Momification\n",
      "\n",
      "GUESS: La momification peut etre quand on se transforme en momie\n",
      "\n",
      "SEEK: Qu'est ce qui est le mot momification ?\n",
      "\n",
      "ASSESS: Je ne trouve pas ma reponse\n",
      "\n",
      "assess_cues: {\"1\":\"L'Egypte antique est une ancienne civilisation de l'Afrique du Nord\",\"2\":\"La momification revient a conserver le corps dans une boite et le mettre dans une piece sans lumiere et sans air\",\"3\":\"Le premier Pharaon a vecu a 3000 av. J.-C.\"}\n",
      "\n",
      "Identify_validity: 2\n",
      "\n",
      "Guess_validity: <NA>\n",
      "\n",
      "Seek_validity: 2\n",
      "\n",
      "Assess_validity: <NA>\n",
      "\n",
      "mechanical_rating: <NA>\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it's empty, you'll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? → If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. → If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step's validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Azure OpenAI scenario...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m azure_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m best_prompt_azure, best_kappa_val_azure, iteration_rows_azure \u001b[38;5;241m=\u001b[39m \u001b[43mrun_iterative_prompt_improvement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscenarios\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotation_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43malt_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples_to_give\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m azure_results\u001b[38;5;241m.\u001b[39mextend(iteration_rows_azure)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Check GPU status after Azure run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ocler\\Documents\\Académique\\Inria\\qualitative_analysis_project\\qualitative_analysis\\prompt_engineering.py:378\u001b[0m, in \u001b[0;36mrun_iterative_prompt_improvement\u001b[1;34m(scenario, train_data, val_data, annotation_columns, labels, alt_test, errors_examples, examples_to_give, epsilon, verbose)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# Evaluate on training set.\u001b[39;00m\n\u001b[0;32m    377\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 378\u001b[0m train_pred_df, train_cost_info, train_totals \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_general_verbatims\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbatims_subset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbatim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm1_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_llm1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature_llm1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselected_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselected_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_completions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_completions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    391\u001b[0m train_tokens \u001b[38;5;241m=\u001b[39m train_totals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_tokens_used\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ocler\\Documents\\Académique\\Inria\\qualitative_analysis_project\\qualitative_analysis\\notebooks_functions.py:658\u001b[0m, in \u001b[0;36mprocess_general_verbatims\u001b[1;34m(verbatims_subset, llm_client, model_name, prompt_template, prefix, temperature, json_output, selected_fields, n_completions, verbose)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;66;03m# Request n_completions from the LLM\u001b[39;00m\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_completions):\n\u001b[1;32m--> 658\u001b[0m         response_text, usage \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    666\u001b[0m         \u001b[38;5;66;03m# JSON mode\u001b[39;00m\n\u001b[0;32m    667\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m json_output:\n",
      "File \u001b[1;32mc:\\Users\\ocler\\Documents\\Académique\\Inria\\qualitative_analysis_project\\qualitative_analysis\\model_interaction.py:279\u001b[0m, in \u001b[0;36mAzureOpenAILLMClient.get_response\u001b[1;34m(self, prompt, model, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 279\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== LLM Response ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\openai\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\openai\\_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1276\u001b[0m     )\n\u001b[1;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\openai\\_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\openai\\_base_client.py:990\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 990\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    996\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\httpx\\_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\httpx\\_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\httpx\\_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    989\u001b[0m     hook(request)\n\u001b[1;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\httpx\\_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\httpx\\_transports\\default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    245\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    100\u001b[0m     (\n\u001b[0;32m    101\u001b[0m         http_version,\n\u001b[0;32m    102\u001b[0m         status,\n\u001b[0;32m    103\u001b[0m         reason_phrase,\n\u001b[0;32m    104\u001b[0m         headers,\n\u001b[0;32m    105\u001b[0m         trailing_data,\n\u001b[1;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\ssl.py:1295\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1294\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\ssl.py:1168\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Azure OpenAI scenario\n",
    "print(\"Running Azure OpenAI scenario...\")\n",
    "azure_results = []\n",
    "\n",
    "best_prompt_azure, best_kappa_val_azure, iteration_rows_azure = run_iterative_prompt_improvement(\n",
    "    scenario=scenarios[0],\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    alt_test=True,\n",
    "    errors_examples=0.5,\n",
    "    examples_to_give=4,\n",
    "    epsilon=epsilon,\n",
    "    verbose=verbose\n",
    ")\n",
    "azure_results.extend(iteration_rows_azure)\n",
    "\n",
    "# Check GPU status after Azure run\n",
    "monitor_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vLLM Scenario\n",
    "\n",
    "Now we'll run the vLLM scenario using Llama-2-7b-chat. This will leverage the supercomputer's GPU resources for local inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running vLLM scenario...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "vLLM is not available on this system. This could be due to installation issues or platform compatibility.\nvLLM may not work on Windows without WSL. Please consider using a different provider like 'azure', 'openai', or 'together'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning vLLM scenario...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m vllm_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m best_prompt_vllm, best_kappa_val_vllm, iteration_rows_vllm \u001b[38;5;241m=\u001b[39m \u001b[43mrun_iterative_prompt_improvement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscenarios\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotation_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43malt_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples_to_give\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m vllm_results\u001b[38;5;241m.\u001b[39mextend(iteration_rows_vllm)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Check GPU status after vLLM run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ocler\\Documents\\Académique\\Inria\\qualitative_analysis_project\\qualitative_analysis\\prompt_engineering.py:351\u001b[0m, in \u001b[0;36mrun_iterative_prompt_improvement\u001b[1;34m(scenario, train_data, val_data, annotation_columns, labels, alt_test, errors_examples, examples_to_give, epsilon, verbose)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide annotation columns for majority voting.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# Initialize LLM clients.\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m llm1_client \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprovider_1\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m llm2_client \u001b[38;5;241m=\u001b[39m get_llm_client(\n\u001b[0;32m    355\u001b[0m     provider\u001b[38;5;241m=\u001b[39mprovider_2, config\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mMODEL_CONFIG[provider_2]\n\u001b[0;32m    356\u001b[0m )\n\u001b[0;32m    358\u001b[0m current_prompt \u001b[38;5;241m=\u001b[39m initial_prompt\n",
      "File \u001b[1;32mc:\\Users\\ocler\\Documents\\Académique\\Inria\\qualitative_analysis_project\\qualitative_analysis\\model_interaction.py:617\u001b[0m, in \u001b[0;36mget_llm_client\u001b[1;34m(provider, config)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvllm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m VLLM_AVAILABLE:\n\u001b[1;32m--> 617\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    618\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM is not available on this system. This could be due to installation issues or platform compatibility.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    619\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM may not work on Windows without WSL. Please consider using a different provider like \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtogether\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    620\u001b[0m         )\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;66;03m# Extract required parameters\u001b[39;00m\n\u001b[0;32m    622\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mImportError\u001b[0m: vLLM is not available on this system. This could be due to installation issues or platform compatibility.\nvLLM may not work on Windows without WSL. Please consider using a different provider like 'azure', 'openai', or 'together'."
     ]
    }
   ],
   "source": [
    "# vLLM scenario\n",
    "print(\"Running vLLM scenario...\")\n",
    "vllm_results = []\n",
    "\n",
    "best_prompt_vllm, best_kappa_val_vllm, iteration_rows_vllm = run_iterative_prompt_improvement(\n",
    "    scenario=scenarios[1],\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    alt_test=True,\n",
    "    errors_examples=0.5,\n",
    "    examples_to_give=4,\n",
    "    epsilon=epsilon,\n",
    "    verbose=verbose\n",
    ")\n",
    "vllm_results.extend(iteration_rows_vllm)\n",
    "\n",
    "# Check GPU status after vLLM run\n",
    "monitor_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Analyze Results\n",
    "\n",
    "Now that we've run both scenarios, we'll combine the results and analyze them to compare the performance of Azure OpenAI and vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all results\n",
    "all_results = azure_results + vllm_results\n",
    "summary_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display settings for better visualization\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Display the summary dataframe\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Azure vs vLLM Performance\n",
    "\n",
    "Let's compare the performance of Azure OpenAI and vLLM by looking at the best results for each provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prompt_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Group by provider and get the best result for each\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m best_by_provider \u001b[38;5;241m=\u001b[39m \u001b[43msummary_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mloc[x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkappa_val\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39midxmax()])\n\u001b[0;32m      3\u001b[0m best_by_provider[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkappa_val\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malt_test_val\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miteration\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\pandas\\core\\frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9186\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9189\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'prompt_name'"
     ]
    }
   ],
   "source": [
    "# Group by provider and get the best result for each\n",
    "best_by_provider = summary_df.groupby('prompt_name').apply(lambda x: x.loc[x['kappa_val'].idxmax()])\n",
    "best_by_provider[['prompt_name', 'kappa_val', 'alt_test_val', 'iteration']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Performance Across Iterations\n",
    "\n",
    "Now let's visualize how the performance of each provider changes across iterations. This will help us understand the effectiveness of the iterative prompt improvement process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `iteration` for `x`. An entry with this name does not appear in `data`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Plot kappa values by iteration for each provider\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlineplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miteration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkappa_val\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKappa Values by Iteration and Provider\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\seaborn\\relational.py:485\u001b[0m, in \u001b[0;36mlineplot\u001b[1;34m(data, x, y, hue, size, style, units, weights, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlineplot\u001b[39m(\n\u001b[0;32m    472\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    473\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, units\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    481\u001b[0m \n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# Handle deprecation of ci parameter\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     errorbar \u001b[38;5;241m=\u001b[39m _deprecate_ci(errorbar, ci)\n\u001b[1;32m--> 485\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_LinePlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_boot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_boot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrorbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrorbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_style\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr_style\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_kws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr_kws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_hue(palette\u001b[38;5;241m=\u001b[39mpalette, order\u001b[38;5;241m=\u001b[39mhue_order, norm\u001b[38;5;241m=\u001b[39mhue_norm)\n\u001b[0;32m    496\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_size(sizes\u001b[38;5;241m=\u001b[39msizes, order\u001b[38;5;241m=\u001b[39msize_order, norm\u001b[38;5;241m=\u001b[39msize_norm)\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\seaborn\\relational.py:216\u001b[0m, in \u001b[0;36m_LinePlotter.__init__\u001b[1;34m(self, data, variables, estimator, n_boot, seed, errorbar, sort, orient, err_style, err_kws, legend)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    204\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, variables\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_size_range \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    213\u001b[0m         np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines.linewidth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    214\u001b[0m     )\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator \u001b[38;5;241m=\u001b[39m estimator\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrorbar \u001b[38;5;241m=\u001b[39m errorbar\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\seaborn\\_base.py:634\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\seaborn\\_base.py:679\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 679\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    680\u001b[0m     frame \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mframe\n\u001b[0;32m    681\u001b[0m     names \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mnames\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\seaborn\\_core\\data.py:58\u001b[0m, in \u001b[0;36mPlotData.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     53\u001b[0m     data: DataSource,\n\u001b[0;32m     54\u001b[0m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[0;32m     55\u001b[0m ):\n\u001b[0;32m     57\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data_source(data)\n\u001b[1;32m---> 58\u001b[0m     frame, names, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe \u001b[38;5;241m=\u001b[39m frame\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m names\n",
      "File \u001b[1;32mc:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\seaborn\\_core\\data.py:232\u001b[0m, in \u001b[0;36mPlotData._assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m         err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn entry with this name does not appear in `data`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m \n\u001b[0;32m    236\u001b[0m     \u001b[38;5;66;03m# Otherwise, assume the value somehow represents data\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# Ignore empty data structures\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret value `iteration` for `x`. An entry with this name does not appear in `data`."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot kappa values by iteration for each provider\n",
    "sns.lineplot(data=summary_df, x='iteration', y='kappa_val', hue='prompt_name', marker='o')\n",
    "plt.title('Kappa Values by Iteration and Provider')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Kappa Value')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Finally, let's save the results to a CSV file for further analysis or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to exploratory_data\\outputs\\vllm_azure_comparison_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV file\n",
    "output_dir = os.path.join(data_dir, 'outputs')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "summary_df.to_csv(os.path.join(output_dir, 'vllm_azure_comparison_results.csv'), index=False)\n",
    "\n",
    "print(f\"Results saved to {os.path.join(output_dir, 'vllm_azure_comparison_results.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
