{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kids Reflect vLLM Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will analyze the Kids Reflect dataset using both Azure OpenAI and vLLM for comparison. This notebook is specifically optimized for running on a supercomputer environment, taking advantage of multiple GPUs and high-performance computing resources.\n",
    "\n",
    "To help you navigate this notebook, here is a step-by-step outline of what we will do:\n",
    "\n",
    "1. **Configure vLLM for Supercomputer Environment**  \n",
    "   - Set environment variables to optimize vLLM for high-performance computing\n",
    "   - Verify GPU availability and configuration\n",
    "\n",
    "2. **Load and Preprocess the Dataset**  \n",
    "   - Load the Kids Reflect dataset\n",
    "   - Clean and normalize text columns\n",
    "   - Convert integer columns to the appropriate data type\n",
    "   - Create verbatim text for analysis\n",
    "\n",
    "3. **Prepare Training and Validation Data**  \n",
    "   - Filter labeled data\n",
    "   - Split data into training and validation sets\n",
    "\n",
    "4. **Define Prompt Templates and Scenarios**  \n",
    "   - Create templates for both Azure OpenAI and vLLM scenarios\n",
    "   - Configure model parameters for optimal performance\n",
    "\n",
    "5. **Run Iterative Prompt Improvement**  \n",
    "   - Execute each scenario separately to monitor progress\n",
    "   - Track GPU usage during execution\n",
    "\n",
    "6. **Analyze and Visualize Results**  \n",
    "   - Compare performance between Azure OpenAI and vLLM\n",
    "   - Visualize kappa values across iterations\n",
    "   - Save results for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure vLLM for Supercomputer Environment\n",
    "\n",
    "Before we begin, we need to configure vLLM to take full advantage of the supercomputer environment. This involves setting environment variables that control how vLLM utilizes the available GPU resources.\n",
    "\n",
    "### Key Configuration Parameters:\n",
    "\n",
    "- **VLLM_MODEL_PATH**: Path to the model or HuggingFace model ID\n",
    "- **VLLM_DTYPE**: Data type for model weights (float16 for efficiency)\n",
    "- **VLLM_GPU_MEMORY_UTILIZATION**: Target GPU memory utilization (0.95 or 95% for supercomputers)\n",
    "- **VLLM_TENSOR_PARALLEL_SIZE**: Number of GPUs to use for tensor parallelism (4 for multi-GPU setups)\n",
    "- **VLLM_MAX_MODEL_LEN**: Maximum sequence length (2048 tokens)\n",
    "- **VLLM_ENABLE_PREFIX_CACHING**: Enable prefix caching for better performance\n",
    "- **VLLM_WORKER_MULTIPROC_METHOD**: Worker multiprocessing method (spawn for better compatibility)\n",
    "\n",
    "These settings are optimized for high-performance computing environments with multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set vLLM environment variables for supercomputer\n",
    "%env VLLM_MODEL_PATH=TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "%env VLLM_DTYPE=float16\n",
    "%env VLLM_GPU_MEMORY_UTILIZATION=0.95\n",
    "%env VLLM_TENSOR_PARALLEL_SIZE=4\n",
    "%env VLLM_MAX_MODEL_LEN=2048\n",
    "%env VLLM_ENABLE_PREFIX_CACHING=true\n",
    "%env VLLM_WORKER_MULTIPROC_METHOD=spawn\n",
    "\n",
    "# Display current configuration\n",
    "!echo \"Current vLLM configuration:\"\n",
    "!echo \"VLLM_MODEL_PATH: $VLLM_MODEL_PATH\"\n",
    "!echo \"VLLM_GPU_MEMORY_UTILIZATION: $VLLM_GPU_MEMORY_UTILIZATION\"\n",
    "!echo \"VLLM_TENSOR_PARALLEL_SIZE: $VLLM_TENSOR_PARALLEL_SIZE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability\n",
    "\n",
    "Before proceeding, it's important to verify that GPUs are available and properly configured. This step helps identify any potential issues with GPU allocation or configuration before running the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup\n",
    "\n",
    "Now we'll import the necessary libraries and modules for our analysis. The qualitative_analysis package provides functions for data loading, preprocessing, and model interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from qualitative_analysis import (\n",
    "    clean_and_normalize,\n",
    "    load_data,\n",
    "    sanitize_dataframe,\n",
    ")\n",
    "from qualitative_analysis.prompt_engineering import run_iterative_prompt_improvement\n",
    "\n",
    "# Define data directory\n",
    "data_dir = 'exploratory_data'\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The Kids Reflect dataset contains entries from children who engaged in a four-step process to formulate divergent questions about a reference text. Each entry includes:\n",
    "\n",
    "- **Reference**: The text that children read beforehand\n",
    "- **IDENTIFY**: Where the child identifies a knowledge gap related to the reference text\n",
    "- **GUESS**: Where the child makes a guess about what the answer could be\n",
    "- **SEEK**: Where the child formulates a question to seek the answer\n",
    "- **ASSESS**: Where the child evaluates whether an answer was found\n",
    "\n",
    "The dataset also includes validity ratings for each step and overall mechanical ratings, as well as annotations from three human raters (Chloe, Oli, and Gaia).\n",
    "\n",
    "### Data Preprocessing Steps\n",
    "\n",
    "1. Load the dataset from the Excel file\n",
    "2. Clean and normalize text columns\n",
    "3. Convert integer columns to the appropriate data type\n",
    "4. Sanitize the DataFrame to handle any inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your dataset\n",
    "data_file_path = os.path.join(data_dir, 'Kids_Reflect_3anno.xlsx')\n",
    "\n",
    "# Load the data\n",
    "data = load_data(data_file_path, file_type='xlsx', delimiter=';')\n",
    "\n",
    "# 1) Now define the new column names for cleaning\n",
    "text_columns = [\"reference\", \"IDENTIFY\", \"GUESS\", \"SEEK\", \"ASSESS\", \"assess_cues\"]\n",
    "integer_columns = [\"Identify_validity\", \"Guess_validity\", \"Seek_validity\", \"Assess_validity\", \"mechanical_rating\", \"Rater_Chloe\", \"Rater_Oli\", \"Rater_Gaia\"]\n",
    "\n",
    "# 2) Clean and normalize the new columns\n",
    "for col in text_columns:\n",
    "    data[col] = clean_and_normalize(data[col])\n",
    "\n",
    "# 3) Convert selected columns to integers, preserving NaNs\n",
    "for col in integer_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors=\"coerce\").astype(\"Int64\")  # Uses nullable integer type\n",
    "\n",
    "# 4) Sanitize the DataFrame\n",
    "data = sanitize_dataframe(data)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Verbatim Text\n",
    "\n",
    "Now we'll combine the different columns into a single verbatim text for each entry. This format makes it easier for the language model to process the entire entry as a cohesive unit.\n",
    "\n",
    "The verbatim text includes:\n",
    "- The unique key identifier\n",
    "- The reference text\n",
    "- The IDENTIFY, GUESS, SEEK, and ASSESS steps\n",
    "- The validity ratings for each step\n",
    "- The mechanical rating (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine texts and entries\n",
    "data['verbatim'] = data.apply(\n",
    "    lambda row: (\n",
    "        f\"key: {row['key']}\\n\\n\"\n",
    "        f\"reference: {row['reference']}\\n\\n\"\n",
    "        f\"IDENTIFY: {row['IDENTIFY']}\\n\\n\"\n",
    "        f\"GUESS: {row['GUESS']}\\n\\n\"\n",
    "        f\"SEEK: {row['SEEK']}\\n\\n\"\n",
    "        f\"ASSESS: {row['ASSESS']}\\n\\n\"\n",
    "        f\"assess_cues: {row['assess_cues']}\\n\\n\"\n",
    "        f\"Identify_validity: {row['Identify_validity']}\\n\\n\"\n",
    "        f\"Guess_validity: {row['Guess_validity']}\\n\\n\"\n",
    "        f\"Seek_validity: {row['Seek_validity']}\\n\\n\"\n",
    "        f\"Assess_validity: {row['Assess_validity']}\\n\\n\"\n",
    "        f\"mechanical_rating: {row['mechanical_rating']}\\n\\n\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Extract the list of verbatims\n",
    "verbatims = data['verbatim'].tolist()\n",
    "\n",
    "print(f\"Total number of verbatims: {len(verbatims)}\")\n",
    "print(f\"Verbatim example:\\n{verbatims[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training and Validation Data\n",
    "\n",
    "To evaluate the performance of our models, we need to split the data into training and validation sets. We'll use the training set to train the models and the validation set to evaluate their performance.\n",
    "\n",
    "### Steps:\n",
    "1. Identify labeled data (entries with annotations from all three raters)\n",
    "2. Create a subset of the labeled data for analysis\n",
    "3. Split the subset into training (70%) and validation (30%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the columns that represent your human ratings\n",
    "annotation_columns = ['Rater_Chloe', 'Rater_Oli', 'Rater_Gaia']\n",
    "\n",
    "# Filter labeled data (drop rows with NaN in any annotation column)\n",
    "labeled_data = data.dropna(subset=annotation_columns)\n",
    "\n",
    "# Filter unlabeled data\n",
    "unlabeled_data = data[~data.index.isin(labeled_data.index)]\n",
    "\n",
    "print(\"Number of labeled rows:\", len(labeled_data))\n",
    "print(\"Number of unlabeled rows:\", len(unlabeled_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "subsample_size = 30\n",
    "\n",
    "# Step 1: Get a stratified subset of samples\n",
    "data_subset, _ = train_test_split(\n",
    "    labeled_data,\n",
    "    train_size=subsample_size,\n",
    "    # stratify=data['label'],  # Uncomment if you have a label column to stratify on\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Split subset into train/val\n",
    "train_data, val_data = train_test_split(\n",
    "    data_subset,\n",
    "    test_size=0.3,\n",
    "    # stratify=data_subset['label'],  # Uncomment if you have a label column to stratify on\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_data))\n",
    "print(\"Val size:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Templates\n",
    "\n",
    "Now we'll define the prompt templates that will be used to instruct the language models. These templates include:\n",
    "\n",
    "1. **Common Template**: The main instructions for evaluating the validity of a cycle\n",
    "2. **Response Template**: The format in which the model should provide its response\n",
    "\n",
    "The templates include detailed instructions on how to evaluate each step of the cycle and determine overall validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "annotation_columns = ['Rater_Chloe', 'Rater_Oli', 'Rater_Gaia']\n",
    "labels = [0,1]\n",
    "epsilon = 0.2\n",
    "\n",
    "# Define the common template for both scenarios\n",
    "common_template = \"\"\"\n",
    "You are an assistant that evaluates data entries.\n",
    "\n",
    "You are provided with data entries in the following format:\n",
    "\n",
    "The data has the following columns:\n",
    "- \"key\": Unique identifiant\n",
    "- \"reference\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
    "- \"IDENTIFY\": Response for the IDENTIFY step\n",
    "- \"GUESS\": Response for the GUESS step\n",
    "- \"SEEK\": Response for the SEEK step\n",
    "- \"ASSESS\": Response for the ASSESS step\n",
    "- \"assess_cues\": Possible answers that were proposed in the ASSESS step\n",
    "- \"Identify_validity\": If a number is already there (whatever the number), the step is valid\n",
    "- \"Guess_validity\": If a number is already there (whatever the number), the step is valid\n",
    "- \"Seek_validity\": If a number is already there (whatever the number), the step is valid\n",
    "- \"Assess_validity\": If a number is already there (whatever the number), the step is valid\n",
    "- \"mechanical_rating\": If a number is already there, you should use that as the final label (it over-rides any other logic in the codebook)\n",
    "\n",
    "\n",
    "Here is an entry to evaluate:\n",
    "{verbatim_text}\n",
    "\n",
    "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
    "If it's empty, you'll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
    "\n",
    "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
    "\n",
    "- Identify Step: Does the Identify step indicate a topic of interest?\n",
    "- Guess Step: Does the Guess step suggest a possible explanation?\n",
    "- Seek Step: Is the Seek step formulated as a question?\n",
    "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
    "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
    "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
    "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
    "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
    "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? → If not, then no answer was actually found, and the cycle is not valid.\n",
    "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. → If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
    "\n",
    "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
    "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step's validity.\n",
    "\n",
    "If all these criteria are met, the cycle is valid.\n",
    "Validity is expressed as:\n",
    "1: Valid cycle\n",
    "0: Invalid cycle\n",
    "\n",
    "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
    "\"\"\"\n",
    "\n",
    "# Define the common response template for both scenarios\n",
    "common_response_template = \"\"\"\n",
    "Please follow the JSON format below:\n",
    "```json\n",
    "{{\n",
    "  \"Reasoning\": \"Your text here\",\n",
    "  \"Classification\": \"Your integer here\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Scenarios and GPU Monitoring\n",
    "\n",
    "We'll define two scenarios for our analysis:\n",
    "\n",
    "1. **Azure OpenAI with GPT-4o**: This scenario uses Azure's hosted GPT-4o model\n",
    "2. **vLLM with Llama-2-7b-chat**: This scenario uses vLLM to run the Llama 2 model locally on the supercomputer\n",
    "\n",
    "We'll also define a function to monitor GPU usage during execution, which is particularly useful for tracking resource utilization on the supercomputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to monitor GPU usage during execution\n",
    "def monitor_gpu():\n",
    "    !nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv\n",
    "    \n",
    "# Check GPU status before starting\n",
    "monitor_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "    # Azure OpenAI scenario\n",
    "    {\n",
    "        \"provider_llm1\": \"azure\",\n",
    "        \"model_name_llm1\": \"gpt-4o\",\n",
    "        \"temperature_llm1\": 0,\n",
    "\n",
    "        # For the \"improver\" LLM2\n",
    "        \"provider_llm2\": \"azure\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "\n",
    "        \"max_iterations\": 4,\n",
    "        \"n_completions\": 1,\n",
    "        \"prompt_name\": \"Azure-GPT4o\",\n",
    "\n",
    "        # Our initial prompt\n",
    "        \"template\": common_template,\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"json_output\": True,\n",
    "        \"selected_fields\": [\"Classification\"],\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\": common_response_template,\n",
    "    },\n",
    "    \n",
    "    # vLLM scenario with a larger model (adjust based on your supercomputer's capabilities)\n",
    "    {\n",
    "        \"provider_llm1\": \"vllm\",\n",
    "        \"model_name_llm1\": \"meta-llama/Llama-2-7b-chat-hf\",  # Or another model available on your supercomputer\n",
    "        \"temperature_llm1\": 0.1,\n",
    "        \n",
    "        # For the \"improver\" LLM2, still use Azure\n",
    "        \"provider_llm2\": \"azure\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "        \n",
    "        \"max_iterations\": 4,  # Can increase this since you have more compute\n",
    "        \"n_completions\": 1,\n",
    "        \"prompt_name\": \"vLLM-Llama2-7B\",\n",
    "        \n",
    "        # Same template as the Azure scenario\n",
    "        \"template\": common_template,\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"json_output\": True,\n",
    "        \"selected_fields\": [\"Classification\"],\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\": common_response_template,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Iterative Prompt Improvement\n",
    "\n",
    "Now we'll run the iterative prompt improvement process for each scenario. This process involves:\n",
    "\n",
    "1. Using the initial prompt to classify the training data\n",
    "2. Evaluating the performance on the validation data\n",
    "3. Improving the prompt based on the errors made\n",
    "4. Repeating the process for a specified number of iterations\n",
    "\n",
    "We'll run each scenario separately to better monitor progress and resource usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure OpenAI Scenario\n",
    "\n",
    "First, we'll run the Azure OpenAI scenario using GPT-4o. This will serve as our baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure OpenAI scenario\n",
    "print(\"Running Azure OpenAI scenario...\")\n",
    "azure_results = []\n",
    "\n",
    "best_prompt_azure, best_kappa_val_azure, iteration_rows_azure = run_iterative_prompt_improvement(\n",
    "    scenario=scenarios[0],\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    alt_test=True,\n",
    "    errors_examples=0.5,\n",
    "    examples_to_give=4,\n",
    "    epsilon=epsilon,\n",
    "    verbose=verbose\n",
    ")\n",
    "azure_results.extend(iteration_rows_azure)\n",
    "\n",
    "# Check GPU status after Azure run\n",
    "monitor_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vLLM Scenario\n",
    "\n",
    "Now we'll run the vLLM scenario using Llama-2-7b-chat. This will leverage the supercomputer's GPU resources for local inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM scenario\n",
    "print(\"Running vLLM scenario...\")\n",
    "vllm_results = []\n",
    "\n",
    "best_prompt_vllm, best_kappa_val_vllm, iteration_rows_vllm = run_iterative_prompt_improvement(\n",
    "    scenario=scenarios[1],\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    alt_test=True,\n",
    "    errors_examples=0.5,\n",
    "    examples_to_give=4,\n",
    "    epsilon=epsilon,\n",
    "    verbose=verbose\n",
    ")\n",
    "vllm_results.extend(iteration_rows_vllm)\n",
    "\n",
    "# Check GPU status after vLLM run\n",
    "monitor_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Analyze Results\n",
    "\n",
    "Now that we've run both scenarios, we'll combine the results and analyze them to compare the performance of Azure OpenAI and vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = azure_results + vllm_results\n",
    "summary_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display settings for better visualization\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Display the summary dataframe\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Azure vs vLLM Performance\n",
    "\n",
    "Let's compare the performance of Azure OpenAI and vLLM by looking at the best results for each provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by provider and get the best result for each\n",
    "best_by_provider = summary_df.groupby('prompt_name').apply(lambda x: x.loc[x['kappa_val'].idxmax()])\n",
    "best_by_provider[['prompt_name', 'kappa_val', 'alt_test_val', 'iteration']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Performance Across Iterations\n",
    "\n",
    "Now let's visualize how the performance of each provider changes across iterations. This will help us understand the effectiveness of the iterative prompt improvement process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot kappa values by iteration for each provider\n",
    "sns.lineplot(data=summary_df, x='iteration', y='kappa_val', hue='prompt_name', marker='o')\n",
    "plt.title('Kappa Values by Iteration and Provider')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Kappa Value')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Finally, let's save the results to a CSV file for further analysis or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "output_dir = os.path.join(data_dir, 'outputs')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "summary_df.to_csv(os.path.join(output_dir, 'vllm_azure_comparison_results.csv'), index=False)\n",
    "\n",
    "print(f\"Results saved to {os.path.join(output_dir, 'vllm_azure_comparison_results.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
