{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex case classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will perform sequential binary classification analysis using LLMs. Binary classification refers to the task of categorizing data entries into one of two predefined categories. Here, we explore a specific case where binary classifications are conducted sequentially, with the final classification outcome depending on the results of prior classifications.\n",
    "\n",
    "To help you navigate this notebook, here is a step-by-step outline of what we will do:\n",
    "\n",
    "1. **Getting started**  \n",
    "   - Download and install the project and its dependencies, load import and your API key.\n",
    "\n",
    "2. **Load and preprocess the dataset**  \n",
    "   - Upload, explore and pre-process the dataset, with the sample dataset (recommended for a first use) or your own data.\n",
    "\n",
    "3. **Prompt construction and classification on manually annotated data**  \n",
    "\n",
    "4. **Evaluating Model Performance Against Human Annotations**  \n",
    "   - Compute metrics (e.g., **Cohen's Kappa**, **Alt-Test**, ...)\n",
    "\n",
    "5. **Final Step: Classify the Full Dataset**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Before we begin, let's set up the environment by cloning the project and installing the necessary dependencies.\n",
    "\n",
    "### Step 1: Clone the Project\n",
    "Run the following cell to download the project files.\n",
    "This will download the project folder into Colab and switch the working directory to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/OlivierLClerc/qualitative_analysis_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Install Required Libraries\n",
    "Now, install the project and its dependencies.\n",
    "\n",
    "‚ö†Ô∏è Note:\n",
    "\n",
    "- This will install all required libraries for the notebook to run.\n",
    "- If Colab suggests restarting the runtime, click \"Restart Runtime\" and re-run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd qualitative_analysis_project\n",
    "%pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load Your OpenAI API Key\n",
    "\n",
    "To use OpenAI models for analysis, you need to provide your **OpenAI API key**. This key allows secure access to the API.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "1. Click on the **üîë \"Key\" icon** on the left sidebar in Colab (**‚öôÔ∏è Settings** > **Secrets**).  \n",
    "2. Click **\"Add a new secret\"**.  \n",
    "3. Enter the following:  \n",
    "   - **Name** ‚Üí `OPENAI_API_KEY`  \n",
    "   - **Value** ‚Üí *Your OpenAI API Key* (Get it from [OpenAI](https://platform.openai.com/account/api-keys))  \n",
    "4. Click **\"Save\"**.  \n",
    "\n",
    "#### Troubleshooting\n",
    "\n",
    "- **API Key not found?**  \n",
    "  - Double-check that the secret name is exactly **`OPENAI_API_KEY`**.  \n",
    "  - If the issue persists, **refresh the page** and rerun the cell.  \n",
    "\n",
    "- **Is My Key Secure?**  \n",
    "  - Yes! Colab's **Secrets Manager** encrypts your key and keeps it safe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Retrieve API keys securely from Colab Secrets\n",
    "API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Check if the API key was loaded\n",
    "if API_KEY:\n",
    "    print(\"‚úÖ API Key loaded successfully!\")\n",
    "    os.environ['OPENAI_API_KEY'] = API_KEY\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è API Key not found. Please check the Secrets panel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import Project Modules\n",
    "\n",
    "Now that the project is installed, let's import the necessary modules and functions from the `qualitative_analysis` package. These tools will help us load data, process text, and perform binary classification analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: vLLM is not available. VLLMLLMClient will not be usable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ocler\\miniconda3\\envs\\gpt_rl\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qualitative_analysis import (\n",
    "    load_data,\n",
    "    clean_and_normalize,\n",
    "    sanitize_dataframe,\n",
    ")\n",
    "\n",
    "from qualitative_analysis.scenario_runner import run_scenarios\n",
    "from qualitative_analysis.evaluation import (\n",
    "    compute_kappa_metrics,\n",
    "    run_alt_test_on_results,\n",
    "    compute_classification_metrics_from_results\n",
    ")\n",
    "from qualitative_analysis.metrics.krippendorff import (\n",
    "    compute_krippendorff_non_inferiority,\n",
    "    print_non_inferiority_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>key</th>\n",
       "      <th>reference</th>\n",
       "      <th>IDENTIFY</th>\n",
       "      <th>GUESS</th>\n",
       "      <th>SEEK</th>\n",
       "      <th>ASSESS</th>\n",
       "      <th>identify_cues</th>\n",
       "      <th>guess_cues</th>\n",
       "      <th>...</th>\n",
       "      <th>Identify_validity</th>\n",
       "      <th>Guess_validity</th>\n",
       "      <th>Seek_validity</th>\n",
       "      <th>Assess_validity</th>\n",
       "      <th>mechanical_rating</th>\n",
       "      <th>Rater_Oli</th>\n",
       "      <th>Unvalid_Oli</th>\n",
       "      <th>Rater_Gaia</th>\n",
       "      <th>Unvalid_Gaia</th>\n",
       "      <th>Invalid_Gaia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aac13</td>\n",
       "      <td>3</td>\n",
       "      <td>aac13_3</td>\n",
       "      <td>Toutankhamon √©tait un pharaon, un roi de l'Egy...</td>\n",
       "      <td>L'Egypte antique</td>\n",
       "      <td>C'est un ancien pays de l'Afrique üí∏</td>\n",
       "      <td>Qu'est ce que l'√âgypte antique</td>\n",
       "      <td>Qui</td>\n",
       "      <td>{\"1\":\"L'Egypte antique\",\"2\":\"Le principe de mo...</td>\n",
       "      <td>{\"1\":\"C'est un ancien pays de l'Afrique\",\"2\":\"...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aac13</td>\n",
       "      <td>4</td>\n",
       "      <td>aac13_4</td>\n",
       "      <td>La tr√®s grande majorit√© de l‚Äôor disponible dan...</td>\n",
       "      <td>Les composants √©lectriques</td>\n",
       "      <td>Les composants √©lectroniques sont par exemple ...</td>\n",
       "      <td>Qu'est-ce qu'une composants √©lectroniques</td>\n",
       "      <td>Non</td>\n",
       "      <td>{\"1\":\"Utilit√© de l'or pour les couronnes denta...</td>\n",
       "      <td>{\"1\":\"Les couronnes en or sont plus solides et...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aac13</td>\n",
       "      <td>5</td>\n",
       "      <td>aac13_5</td>\n",
       "      <td>Des scientifiques ont r√©v√©l√© l‚Äôexistence de tr...</td>\n",
       "      <td>Des premiers humains l'Australie</td>\n",
       "      <td>C'est un pays du Sud</td>\n",
       "      <td>O√π se trouve l'Australie</td>\n",
       "      <td>Oui</td>\n",
       "      <td>{\"1\":\"La formation des traces dans les roches\"...</td>\n",
       "      <td>{\"1\":\"Ces traces se forment automatiquement qu...</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aac13</td>\n",
       "      <td>6</td>\n",
       "      <td>aac13_6</td>\n",
       "      <td>La religion de la Gr√®ce antique comprend plusi...</td>\n",
       "      <td>Une mythologie</td>\n",
       "      <td>L'olympe est un endroit en Gr√®ce</td>\n",
       "      <td>Qu'est ce qu'une mythologie</td>\n",
       "      <td>Non</td>\n",
       "      <td>{\"1\":\"Une mythologie\",\"2\":\"Les autres mytholog...</td>\n",
       "      <td>{\"1\":\"Une mythologie est un ensemble de contes...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aac24</td>\n",
       "      <td>3</td>\n",
       "      <td>aac24_3</td>\n",
       "      <td>Toutankhamon √©tait un pharaon, un roi de l'Egy...</td>\n",
       "      <td>Tout√¢nkhamon</td>\n",
       "      <td>Roi Pharaon</td>\n",
       "      <td>Quand les Pharaons sont-ils apparus</td>\n",
       "      <td>J'ai trouv√© ma r√©ponse</td>\n",
       "      <td>{\"1\":\"L'Egypte antique\",\"2\":\"Le principe de mo...</td>\n",
       "      <td>{\"1\":\"C'est un ancien pays de l'Afrique\",\"2\":\"...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  Iteration      key  \\\n",
       "0  aac13          3  aac13_3   \n",
       "1  aac13          4  aac13_4   \n",
       "2  aac13          5  aac13_5   \n",
       "3  aac13          6  aac13_6   \n",
       "4  aac24          3  aac24_3   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Toutankhamon √©tait un pharaon, un roi de l'Egy...   \n",
       "1  La tr√®s grande majorit√© de l‚Äôor disponible dan...   \n",
       "2  Des scientifiques ont r√©v√©l√© l‚Äôexistence de tr...   \n",
       "3  La religion de la Gr√®ce antique comprend plusi...   \n",
       "4  Toutankhamon √©tait un pharaon, un roi de l'Egy...   \n",
       "\n",
       "                           IDENTIFY  \\\n",
       "0                  L'Egypte antique   \n",
       "1        Les composants √©lectriques   \n",
       "2  Des premiers humains l'Australie   \n",
       "3                    Une mythologie   \n",
       "4                      Tout√¢nkhamon   \n",
       "\n",
       "                                               GUESS  \\\n",
       "0                C'est un ancien pays de l'Afrique üí∏   \n",
       "1  Les composants √©lectroniques sont par exemple ...   \n",
       "2                               C'est un pays du Sud   \n",
       "3                   L'olympe est un endroit en Gr√®ce   \n",
       "4                                        Roi Pharaon   \n",
       "\n",
       "                                        SEEK                  ASSESS  \\\n",
       "0             Qu'est ce que l'√âgypte antique                     Qui   \n",
       "1  Qu'est-ce qu'une composants √©lectroniques                     Non   \n",
       "2                   O√π se trouve l'Australie                     Oui   \n",
       "3                Qu'est ce qu'une mythologie                     Non   \n",
       "4        Quand les Pharaons sont-ils apparus  J'ai trouv√© ma r√©ponse   \n",
       "\n",
       "                                       identify_cues  \\\n",
       "0  {\"1\":\"L'Egypte antique\",\"2\":\"Le principe de mo...   \n",
       "1  {\"1\":\"Utilit√© de l'or pour les couronnes denta...   \n",
       "2  {\"1\":\"La formation des traces dans les roches\"...   \n",
       "3  {\"1\":\"Une mythologie\",\"2\":\"Les autres mytholog...   \n",
       "4  {\"1\":\"L'Egypte antique\",\"2\":\"Le principe de mo...   \n",
       "\n",
       "                                          guess_cues  ... Identify_validity  \\\n",
       "0  {\"1\":\"C'est un ancien pays de l'Afrique\",\"2\":\"...  ...               1.0   \n",
       "1  {\"1\":\"Les couronnes en or sont plus solides et...  ...               3.0   \n",
       "2  {\"1\":\"Ces traces se forment automatiquement qu...  ...               2.0   \n",
       "3  {\"1\":\"Une mythologie est un ensemble de contes...  ...               1.0   \n",
       "4  {\"1\":\"C'est un ancien pays de l'Afrique\",\"2\":\"...  ...               NaN   \n",
       "\n",
       "  Guess_validity Seek_validity  Assess_validity  mechanical_rating  Rater_Oli  \\\n",
       "0            1.0           1.0              NaN                NaN        NaN   \n",
       "1            NaN           NaN              NaN                NaN        NaN   \n",
       "2            3.0           3.0              NaN                0.0        NaN   \n",
       "3            3.0           1.0              NaN                0.0        NaN   \n",
       "4            NaN           3.0              NaN                NaN        NaN   \n",
       "\n",
       "   Unvalid_Oli  Rater_Gaia  Unvalid_Gaia  Invalid_Gaia  \n",
       "0        False         NaN         False         False  \n",
       "1        False         NaN         False         False  \n",
       "2        False         NaN         False         False  \n",
       "3        False         NaN         False         False  \n",
       "4        False         NaN         False         False  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define data directory\n",
    "data_dir = 'data/complex_user_case'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Define the path to your dataset\n",
    "data_file_path = os.path.join(data_dir, 'complex_data.xlsx')\n",
    "\n",
    "# Load the data\n",
    "data = load_data(data_file_path, file_type='xlsx', delimiter=';')\n",
    "\n",
    "# Preview the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    "The dataset provided for this notebook is an anonymized subset from the study available at [X]. In the original experiment, children were tasked with reading a reference text and engaging in four sequential interactions with an interactive app. The goal of these steps was to help the children formulate a divergent question. A question is considered divergent if its answer is not explicitly stated in the reference text.\n",
    "\n",
    "The four steps, include:\n",
    "1. **Identify**: The child identifies a knowledge gap related to the reference text.\n",
    "2. **Guess**: The child makes a guess about what the answer to the knowledge gap could be.\n",
    "3. **Seek**: The child formulates a question to seek the answer.\n",
    "4. **Assess**: The child evaluates whether the app provides an answer to their question.\n",
    "\n",
    "This process is called a **cycle**. An annotator evaluates the validity of a cycle by answering a series of binary Yes/No questions (binary classifications). A cycle is deemed valid if all binary questions can be answered by \"Yes\"; otherwise, it is considered invalid. For more details, see the codebook provided in the prompt cell.\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "The dataset includes the following key components:\n",
    "- **ref**: A column containing the text that children were asked to read beforehand.\n",
    "- **IDENTIFY**: A column containing the entries posed by the children during the Identify step.\n",
    "- **GUESS**: A column containing the entries posed by the children during the Guess step.\n",
    "- **SEEK**: A column containing the entries posed by the children during the Seek step.\n",
    "- **ASSESS**: A column containing the entries posed by the children during the Assess step.\n",
    "\n",
    "To classify a cycle, both the reference text and the entries from all four steps are required.\n",
    "\n",
    "Additionally, the dataset includes ratings from two human annotators. These ratings enable us to compute inter-annotator agreement metrics, such as Cohen's kappa, to assess the reliability of the annotations once the analysis is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing  (Optional, improve clarity and consistency of text data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Rename key columns**  \n",
    "   Give important columns more descriptive names  \n",
    "   (e.g. `ref` ‚Üí `reference`).\n",
    "\n",
    "2. **Clean textual data**  \n",
    "   For each text column, run `clean_and_normalize(series)` to  \n",
    "   - trim leading/trailing spaces  \n",
    "   - convert accented characters to plain ASCII (e.g. `'√©'` ‚Üí `'e'`).\n",
    "\n",
    "3. **Sanitize line breaks**  \n",
    "   Run `sanitize_dataframe(df)` to replace newline (`\\n`) and carriage‚Äëreturn (`\\r`) characters with a single space in every string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a) Define a mapping from old column names to new names\n",
    "rename_map = {\n",
    "    \"ref\": \"reference\",\n",
    "    \"IDENTIFY\": \"identify\",\n",
    "    \"GUESS\": \"guess\",\n",
    "    \"SEEK\": \"seek\",\n",
    "    \"ASSESS\": \"assess\"\n",
    "}\n",
    "\n",
    "# 1b) Rename the columns in the DataFrame\n",
    "data = data.rename(columns=rename_map)\n",
    "\n",
    "# 2) Now define the new column names for cleaning\n",
    "text_columns = [\"reference\", \"identify\", \"guess\", \"seek\", \"assess\"]\n",
    "\n",
    "# 3) Clean and normalize the new columns\n",
    "for col in text_columns:\n",
    "    data[col] = clean_and_normalize(data[col])\n",
    "\n",
    "# 4) Sanitize the DataFrame\n",
    "data = sanitize_dataframe(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine texts and questions\n",
    "\n",
    "To prepare the data for the LLM, we gather exactly the information a human annotator would need‚Äîplus the **ID** so we can merge results back into the original DataFrame.  \n",
    "The concatenated block of fields is called a **verbatim**.\n",
    "\n",
    "#### Create the `verbatim` field\n",
    "\n",
    "1. **Build verbatims**  \n",
    "   For every row we create a multi‚Äëline string containing:  \n",
    "   - the respondent **ID**  \n",
    "   - the cleaned **reference** text  \n",
    "   - the five cleaned prompt‚Äëresponse fields (**Identify**, **Guess**, **Seek**, **Assess**)  \n",
    "   Each section is separated by a blank line for readability, and the result is written to a new column named `verbatim`.\n",
    "\n",
    "2. **Sanity‚Äëcheck**  \n",
    "   - Print the total number of verbatims to ensure every row was processed.  \n",
    "   - Display the first verbatim as a spot‚Äëcheck of the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of verbatims: 921\n",
      "Verbatim example:\n",
      "Id: aac13_3\n",
      "\n",
      "Text: Toutankhamon etait un pharaon, un roi de l'Egypte antique. Il est tres connu aujourdhui parce que des archeologues ont retrouve son cercueil intacte avec tous ses tresors, en 1922. Pour les Egyptiens, il y avait une vie apres la mort, une vie eternelle. Cest pour cela que le corps devait etre conserve dans le meilleur etat possible, cest ce quon appelle la momification. Cest aussi pour cela que lon retrouve aujourdhui de la nourriture, des armes ou des tresors dans les tombeaux. Ces objets accompagnaient le pharaon dans sa vie apres la mort.\n",
      "\n",
      "Identify: L'Egypte antique\n",
      "\n",
      "Guess: C'est un ancien pays de l'Afrique \n",
      "\n",
      "Seek: Qu'est ce que l'Egypte antique\n",
      "\n",
      "Assess: Qui\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine texts and entries\n",
    "\n",
    "data['verbatim'] = data.apply(\n",
    "    lambda row: (\n",
    "        f\"Id: {row['key']}\\n\\n\"\n",
    "        f\"Text: {row['reference']}\\n\\n\"\n",
    "        f\"Identify: {row['identify']}\\n\\n\"\n",
    "        f\"Guess: {row['guess']}\\n\\n\"\n",
    "        f\"Seek: {row['seek']}\\n\\n\"\n",
    "        f\"Assess: {row['assess']}\\n\\n\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Extract the list of verbatims\n",
    "verbatims = data['verbatim'].tolist()\n",
    "\n",
    "print(f\"Total number of verbatims: {len(verbatims)}\")\n",
    "print(f\"Verbatim example:\\n{verbatims[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt construction and classification on manually annotated data\n",
    "\n",
    "This framework allows you to evaluate different configurations to determine which prompt, model, and parameters yield the most accurate classification. These configurations are stored in the scenarios list.\n",
    "\n",
    "The snippet defines two **classification scenarios** for evaluating participants‚Äô ‚ÄúIdentify‚ÄØ‚Üí‚ÄØGuess‚ÄØ‚Üí‚ÄØSeek‚ÄØ‚Üí‚ÄØAssess‚Äù reasoning cycles with a Large Language Model (LLM).\n",
    "\n",
    "Each scenario is a dictionary inside the `scenarios` list and can be seen as a self‚Äëcontained _experiment_: it specifies\n",
    "\n",
    "* which LLM to call (`provider_llm1`, `model_name_llm1`, `temperature_llm1`);\n",
    "* the **prompt template** that tells the LLM how to judge a single data row;\n",
    "* the expected JSON output (fields listed in `selected_fields`);\n",
    "* optional settings for **prompt‚Äërefinement** by a second LLM (`provider_llm2`, ‚Ä¶).\n",
    "\n",
    "Running the pipeline iterates over every scenario and evaluates every (or a subsample of) data rows, then writes the chosen output fields back to your dataframe or file.\n",
    "\n",
    "### LLM Settings\n",
    "\n",
    "- `provider_llm1`: The LLM provider used for classification (`azure`, `openai`, `anthropic`, `gemini`)\n",
    "- `model_name_llm1`: The model used for classification. This depends on the provider.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "- **For** `azure` ‚Üí `\"gpt-4o\"` or `\"gpt-4o-mini\"`\n",
    "- **For** `openai` ‚Üí `\"gpt-4o\"` or `\"gpt-4o-mini\"`\n",
    "- **For** `anthropic` ‚Üí `\"claude-3-7-sonnet-20250219\"`, `\"claude-3-5-haiku-20241022\"`\n",
    "- **For** `gemini` ‚Üí `\"gemini-2.0-flash-001\"`, `\"gemini-2.5-pro-preview-03-25\"`\n",
    "\n",
    "- `temperature_llm1`: Controls output variability. Set to `0` for deterministic responses. Higher values add randomness (not recommended for evaluation tasks).\n",
    "- `subsample_size`: Number of entries to evaluate. Set to `-1` to use the entire dataset.\n",
    "\n",
    "### Prompt Configuration\n",
    "\n",
    "- `prompt_name`: A short name identifying the scenario, used in performance tracking.\n",
    "- `template`: The full prompt used to guide the LLM. It could include:\n",
    "  - The **role** of the assistant\n",
    "  - A **description** of the input columns\n",
    "  - The **evaluation codebook** (la mani√®re dont les donn√©es doivent etre classifi√©es)\n",
    "  - Optionally, **examples**\n",
    "  - ‚ö†Ô∏è **Must contain** the `{verbatim_text}` placeholder for the entry being evaluated\n",
    "\n",
    "### Output\n",
    "\n",
    "- `selected_fields`: The fields to extract from the LLM‚Äôs output (e.g., `\"Classification\"`, `\"Reasoning\"`).  \n",
    "  You can modify this to include or exclude elements (like adding confidence scores, removing reasonning).\n",
    "- `prefix`: The key to look for in the LLM output that contains the classification label (e.g., `\"Classification\"`).\n",
    "Nous sp√©cifions donc cela pour que le parsing du verdict soit plus facile, pour r√©cuperer les labels de classification.\n",
    "- `label_type`: Data type of the classification label. Typically `\"int\"` for binary classification (`0` or `1`),  \n",
    "  but can be changed to `\"float\"` or `\"str\"` as needed.\n",
    "- `response_template`: The required format of the LLM output (e.g., JSON). This ensures correct parsing. It is recommended not to change this format request.\n",
    "- `json_output`: If `True`, the LLM must respond in JSON. Disabling this is not recommended. If you do, you will have to  \n",
    "  change the `response_template` accordingly.\n",
    "\n",
    "\n",
    "### Prompt Optimization (In developpement - better not to change anything)\n",
    "\n",
    "This section enables **automatic prompt refinement** using a second LLM. It attempts to generate an improved version of the prompt to reduce classification errors.\n",
    "\n",
    "- A second model (`llm2`) is used to review the prompt given to the first model (`llm1`) and suggest changes based on classification failures.\n",
    "- If the new prompt performs better (fewer classification errors), it replaces the original.\n",
    "\n",
    "**Warning**: This can lead to overfitting ‚Äî the new prompt may work well on the training data but generalize poorly.  \n",
    "It's highly recommended to **use a validation set** when using this feature.\n",
    "\n",
    "### Prompt Optimization\n",
    "\n",
    "- `provider_llm2`: LLM provider used for prompt improvement\n",
    "- `model_name_llm2`: Name of the refinement model\n",
    "- `temperature_llm2`: Temperature for the prompt-refiner LLM\n",
    "- `max_iterations`: How many times the prompt should be revised.\n",
    "For example, if you choose 3, each data entry will be classified three times: once with the original prompt, and twice with newly generated prompts.\n",
    "- `use_validation_set`: Whether to use a separate validation set to monitor prompt overfitting (Boolean)\n",
    "- `validation_size`: Number of samples in the validation set\n",
    "- `random_state`: Random seed for reproducible train/validation split\n",
    "\n",
    "### Majority vote\n",
    "\n",
    "- `n_completions`: Number of completions per entry. \n",
    "It is possible to generate multiple responses for each entry using the same LLM. This will produce several classification labels for the same data point.\n",
    "The final label is determined by majority vote. Generating multiple completions can improve robustness but also increases cost.\n",
    "\n",
    "### Example\n",
    "\n",
    "In the current example, we define two scenarios:\n",
    "\n",
    "**Scenario 1**: Includes examples in the prompt (*few-shot*)\n",
    "\n",
    "**Scenario 2**: Contains only the codebook and instructions (*zero-shot*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "    {\n",
    "        # LLM settings\n",
    "        \"provider_llm1\": \"openai\",\n",
    "        \"model_name_llm1\": \"gpt-4o\",\n",
    "        \"temperature_llm1\": 0,\n",
    "        \"prompt_name\": \"few_shot\",\n",
    "        \"subsample_size\": 5,  # Size of data subset to use\n",
    "\n",
    "        # Prompt configuration\n",
    "        \"template\": \"\"\"\n",
    "You are an assistant that evaluates data entries.\n",
    "\n",
    "The data has the following columns:\n",
    "- \"ID\": Unique identifiant of the participant\n",
    "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\\n\"\n",
    "- \"Identify\": Response for the IDENTIFY step\n",
    "- \"Guess\": Response for the GUESS step\n",
    "- \"Seek\": Response for the SEEK step\n",
    "- \"Assess\": Response for the ASSESS step\n",
    "\n",
    "Here is an entry to evaluate:\n",
    "{verbatim_text}\n",
    "\n",
    "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
    "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
    "\n",
    "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
    "\n",
    "- Identify Step: Does the Identify step indicate a topic of interest?\n",
    "- Guess Step: Does the Guess step suggest a possible explanation?\n",
    "- Seek Step: Is the Seek step formulated as a question?\n",
    "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
    "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
    "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
    "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
    "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
    "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
    "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
    "\n",
    "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
    "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
    "\n",
    "If all these criteria are met, the cycle is valid.\n",
    "Validity is expressed as:\n",
    "1: Valid cycle\n",
    "0: Invalid cycle\n",
    "\n",
    "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Example 1\n",
    "Key:\n",
    "AA25I4\n",
    "\n",
    "Reference:\n",
    "\"Rain forms when water evaporates into the atmosphere, condenses into droplets, and falls due to gravity.\"\n",
    "\n",
    "Cycle Steps:\n",
    "IDENTIFY: \"I don‚Äôt understand how rain forms.\"\n",
    "GUESS: \"Maybe rain condenses in the sky, forming droplets.\"\n",
    "SEEK: \"How does rain form?\"\n",
    "ASSESS: \"No\"\n",
    "Assess Cues:\n",
    "\n",
    "Validity Columns:\n",
    "Identify_validity: NA\n",
    "Guess_validity: 2\n",
    "Seek_validity: NA\n",
    "Assess_validity: NA\n",
    "Mechanical_rating: NA\n",
    "\n",
    "Reasoning\n",
    "Since the mechanical_rating column is empty, the validity must be determined using the codebook.\n",
    "\n",
    "Reasoning:\n",
    "Identify step: Does the Identify step indicate a topic of interest?\n",
    "Yes: The topic is the formation of rain.\n",
    "\n",
    "Guess step: A numeric value is present in the Guess_validity column, so no further validation is needed.\n",
    "Yes: It proposes condensation as the mechanism for rain formation.\n",
    "\n",
    "Seek step: Is the Seek step formulated as a question?\n",
    "Yes: It is explicitly phrased as a question with an interrogative structure.\n",
    "\n",
    "Assess step: Does it identify a possible answer or state that no answer was found (\"No\" is acceptable)?\n",
    "Yes: It states that the answer to the question was not found, which is a valid response in the Assess step.\n",
    "\n",
    "Consistency: Are the Identify, Guess, and Seek steps related to the same topic?\n",
    "Yes: They all pertain to the process of rain formation.\n",
    "\n",
    "Reference Link: Are the Identify, Guess, and Seek steps related to the reference text?\n",
    "Yes: The text discusses rain and explains its formation.\n",
    "\n",
    "Seek Question Originality: Is the answer to the Seek question absent (even vaguely) from the reference text?\n",
    "No: The answer is explicitly provided in the reference text.\n",
    "\n",
    "Resolving Answer:\n",
    "Not applicable (the answer was not found).\n",
    "\n",
    "Valid Answer:\n",
    "Not applicable (the answer was not found).\n",
    "\n",
    "Valid No: Is the answer to the SEEK question absent from the assess_cues?\n",
    "Yes: The answer to the SEEK question is not in assess_cues, so the \"No\" is valid.\n",
    "\n",
    "Conclusion\n",
    "The cycle is not valid because the answer to the SEEK question is explicitly present in the reference text.\n",
    "\n",
    "Validity:\n",
    "0\n",
    "\"\"\",\n",
    "        # Output\n",
    "        \"selected_fields\": [\"Classification\", \"Reasoning\"],\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\":\n",
    "        \"\"\"\n",
    "Please follow the JSON format below:\n",
    "```json\n",
    "{{\n",
    "  \"Reasoning\": \"Your text here\",\n",
    "  \"Classification\": \"Your integer here\"\n",
    "}}\n",
    "\"\"\",\n",
    "        \"json_output\": True,\n",
    "\n",
    "        # Prompt optimization\n",
    "        \"provider_llm2\": \"openai\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "        \"max_iterations\": 1,\n",
    "        \"use_validation_set\": False,\n",
    "        \"validation_size\": 10,\n",
    "        \"random_state\": 42,\n",
    "\n",
    "        # Majority vote\n",
    "        \"n_completions\": 1,\n",
    "\n",
    "    },\n",
    "    {\n",
    "        # LLM settings\n",
    "        \"provider_llm1\": \"openai\",\n",
    "        \"model_name_llm1\": \"gpt-4o\",\n",
    "        \"temperature_llm1\": 0,\n",
    "        \"prompt_name\": \"zero_shot\",\n",
    "        \"subsample_size\": 5,  # Size of data subset to use\n",
    "\n",
    "        # Prompt configuration\n",
    "        \"template\": \"\"\"\n",
    "You are an assistant that evaluates data entries.\n",
    "\n",
    "The data has the following columns:\n",
    "- \"ID\": Unique identifiant of the participant\n",
    "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\\n\"\n",
    "- \"Identify\": Response for the IDENTIFY step\n",
    "- \"Guess\": Response for the GUESS step\n",
    "- \"Seek\": Response for the SEEK step\n",
    "- \"Assess\": Response for the ASSESS step\n",
    "\n",
    "Here is an entry to evaluate:\n",
    "{verbatim_text}\n",
    "\n",
    "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
    "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
    "\n",
    "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
    "\n",
    "- Identify Step: Does the Identify step indicate a topic of interest?\n",
    "- Guess Step: Does the Guess step suggest a possible explanation?\n",
    "- Seek Step: Is the Seek step formulated as a question?\n",
    "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
    "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
    "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
    "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
    "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
    "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
    "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
    "\n",
    "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
    "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
    "\n",
    "If all these criteria are met, the cycle is valid.\n",
    "Validity is expressed as:\n",
    "1: Valid cycle\n",
    "0: Invalid cycle\n",
    "\n",
    "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
    "\"\"\",\n",
    "        # Output\n",
    "        \"selected_fields\": [\"Classification\", \"Reasoning\"],\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\":\n",
    "        \"\"\"\n",
    "Please follow the JSON format below:\n",
    "```json\n",
    "{{\n",
    "  \"Reasoning\": \"Your text here\",\n",
    "  \"Classification\": \"Your integer here\"\n",
    "}}\n",
    "\"\"\",\n",
    "        \"json_output\": True,\n",
    "\n",
    "        # Prompt optimization\n",
    "        \"provider_llm2\": \"openai\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "        \"max_iterations\": 1,\n",
    "        \"use_validation_set\": False,\n",
    "        \"validation_size\": 10,\n",
    "        \"random_state\": 42,\n",
    "\n",
    "        # Majority vote\n",
    "        \"n_completions\": 1,\n",
    "\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Annotated Subset\n",
    "\n",
    "Before launching the classification on the entire dataset, we first run it on the subset that has been manually annotated.  \n",
    "This step allows us to compute performance metrics (e.g., **accuracy**, **F1-score**) by comparing LLM predictions to human labels,  \n",
    "and therefore select which (if any) scenario can be used to classify the full, unlabeled dataset.\n",
    "\n",
    "#### Configuration Parameters\n",
    "\n",
    "- `annotation_columns`: The names of the columns containing human annotations.\n",
    "- `labels`: The possible label values (in this case, `[0, 1]` for binary classification).\n",
    "\n",
    "We filter out any rows with missing values in the annotation columns to ensure we're only evaluating on fully labeled data.\n",
    "\n",
    "#### Repeated Runs for Stability\n",
    "\n",
    "LLMs are **stochastic** by nature ‚Äî even with a temperature of `0`, outputs can vary.  \n",
    "To assess how consistent the model is, we introduce the `n_runs` parameter:\n",
    "\n",
    "- `n_runs`: The number of times the classification is repeated for each scenario on the annotated data.\n",
    "\n",
    "We recommend setting `n_runs = 3`, based on findings from **[Paper XX]** (insert reference),  \n",
    "which showed that **three repetitions strike a good balance between stability and cost**.  \n",
    "Running more times improves statistical reliability but increases costs proportionally.\n",
    "\n",
    "#### `n_runs` vs `n_completions`\n",
    "\n",
    "It‚Äôs important to distinguish between these two concepts:\n",
    "\n",
    "- **`n_completions`**:  \n",
    "  Controls how many responses are generated **within a single run** for each data point.  \n",
    "  The final label is determined by **majority vote** over those completions.  \n",
    "  **Example**:  \n",
    "  If `n_completions = 3` and the model returns `[0, 0, 1]`, the selected label will be `0`.\n",
    "\n",
    "- **`n_runs`**:  \n",
    "  Repeats the **entire classification process** multiple times across the same data.  \n",
    "  If you run the scenario three times and get `[0, 0, 1]` for a given entry,  \n",
    "  that variation will be captured when calculating metrics (e.g., **variance**, **disagreement rate**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 'few_shot' - Train size (all data): 5, No validation set\n",
      "\n",
      "=== Processing Verbatim 1/5 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"ID\": Unique identifiant of the participant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "\"\n",
      "- \"Identify\": Response for the IDENTIFY step\n",
      "- \"Guess\": Response for the GUESS step\n",
      "- \"Seek\": Response for the SEEK step\n",
      "- \"Assess\": Response for the ASSESS step\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc13_5\n",
      "\n",
      "Text: Des scientifiques ont revele lexistence de traces dinsectes, de plantes et danimaux en Australie. Ces traces sont les restes danimaux et de vegetaux morts qui ont laisse leur empreinte dans la roche : c'est ce qu'on appelle des fossiles. Ils vivaient il y a plus de 11 millions dannees, bien avant lapparition des premiers humains ! Ces empreintes passionnent les scientifiques car ils sont tres bien conserves, les specialistes peuvent observer des details tres interessants.\n",
      "\n",
      "Identify: C'est quoi un fossile\n",
      "\n",
      "Guess: Un animal mort\n",
      "\n",
      "Seek: C'est quoi un fossile\n",
      "\n",
      "Assess: Il y a pas la reponse a ma question\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example 1\n",
      "Key:\n",
      "AA25I4\n",
      "\n",
      "Reference:\n",
      "\"Rain forms when water evaporates into the atmosphere, condenses into droplets, and falls due to gravity.\"\n",
      "\n",
      "Cycle Steps:\n",
      "IDENTIFY: \"I don‚Äôt understand how rain forms.\"\n",
      "GUESS: \"Maybe rain condenses in the sky, forming droplets.\"\n",
      "SEEK: \"How does rain form?\"\n",
      "ASSESS: \"No\"\n",
      "Assess Cues:\n",
      "\n",
      "Validity Columns:\n",
      "Identify_validity: NA\n",
      "Guess_validity: 2\n",
      "Seek_validity: NA\n",
      "Assess_validity: NA\n",
      "Mechanical_rating: NA\n",
      "\n",
      "Reasoning\n",
      "Since the mechanical_rating column is empty, the validity must be determined using the codebook.\n",
      "\n",
      "Reasoning:\n",
      "Identify step: Does the Identify step indicate a topic of interest?\n",
      "Yes: The topic is the formation of rain.\n",
      "\n",
      "Guess step: A numeric value is present in the Guess_validity column, so no further validation is needed.\n",
      "Yes: It proposes condensation as the mechanism for rain formation.\n",
      "\n",
      "Seek step: Is the Seek step formulated as a question?\n",
      "Yes: It is explicitly phrased as a question with an interrogative structure.\n",
      "\n",
      "Assess step: Does it identify a possible answer or state that no answer was found (\"No\" is acceptable)?\n",
      "Yes: It states that the answer to the question was not found, which is a valid response in the Assess step.\n",
      "\n",
      "Consistency: Are the Identify, Guess, and Seek steps related to the same topic?\n",
      "Yes: They all pertain to the process of rain formation.\n",
      "\n",
      "Reference Link: Are the Identify, Guess, and Seek steps related to the reference text?\n",
      "Yes: The text discusses rain and explains its formation.\n",
      "\n",
      "Seek Question Originality: Is the answer to the Seek question absent (even vaguely) from the reference text?\n",
      "No: The answer is explicitly provided in the reference text.\n",
      "\n",
      "Resolving Answer:\n",
      "Not applicable (the answer was not found).\n",
      "\n",
      "Valid Answer:\n",
      "Not applicable (the answer was not found).\n",
      "\n",
      "Valid No: Is the answer to the SEEK question absent from the assess_cues?\n",
      "Yes: The answer to the SEEK question is not in assess_cues, so the \"No\" is valid.\n",
      "\n",
      "Conclusion\n",
      "The cycle is not valid because the answer to the SEEK question is explicitly present in the reference text.\n",
      "\n",
      "Validity:\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The mechanical_rating column is empty, so we must determine the validity using the codebook. \\n\\nIdentify Step: The Identify step asks 'C'est quoi un fossile', which indicates a topic of interest related to fossils. \\n\\nGuess Step: The Guess step suggests 'Un animal mort', which is a possible explanation for what a fossil might be. \\n\\nSeek Step: The Seek step is formulated as a question, 'C'est quoi un fossile', which is correctly structured as a question. \\n\\nAssess Step: The Assess step states 'Il y a pas la reponse a ma question', indicating that no answer was found, which is acceptable. \\n\\nConsistency: The Identify, Guess, and Seek steps are all related to the question of what a fossil is. \\n\\nReference Link: The Identify, Guess, and Seek steps are related to the topic of the reference text, which discusses fossils. \\n\\nSeek Question Originality: The answer to the Seek question 'C'est quoi un fossile' is not found in the reference text, as the text describes fossils but does not define them explicitly. \\n\\nResolving Answer: Not applicable, as no answer was found. \\n\\nValid Answer: Not applicable, as no answer was found. \\n\\nValid No: The answer to the SEEK question is not present in the assess_cues, so the 'No' is valid. \\n\\nAll criteria are met, so the cycle is valid.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1548\n",
      "Cost => 0.0063\n",
      "\n",
      "=== Processing Verbatim 2/5 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"ID\": Unique identifiant of the participant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "\"\n",
      "- \"Identify\": Response for the IDENTIFY step\n",
      "- \"Guess\": Response for the GUESS step\n",
      "- \"Seek\": Response for the SEEK step\n",
      "- \"Assess\": Response for the ASSESS step\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc2_8\n",
      "\n",
      "Text: La foret tropicale humide a un climat qui fournit beaucoup d'eau et une temperature elevee toute l'annee. Cela favorise la densite et la croissance permanente des plantes de ces forets.  En Asie, on appelle ce genre de foret 'la jungle'. Elle est parfois appelee foret dense, bien que cette expression signifie seulement que les arbres sont tres proches les uns des autres. Dans les forets tropicales, les arbres formant trois etages de hauteur differentes, sont tres serres et perdent leurs feuilles irregulierement.\n",
      "\n",
      "Identify: Quelle temperature ?\n",
      "\n",
      "Guess: Peut etre 100 degres\n",
      "\n",
      "Seek: Quelle nombre de degres ?\n",
      "\n",
      "Assess: La temperature dans les forets tropicales est entre 30 degres et 40 degres\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example 1\n",
      "Key:\n",
      "AA25I4\n",
      "\n",
      "Reference:\n",
      "\"Rain forms when water evaporates into the atmosphere, condenses into droplets, and falls due to gravity.\"\n",
      "\n",
      "Cycle Steps:\n",
      "IDENTIFY: \"I don‚Äôt understand how rain forms.\"\n",
      "GUESS: \"Maybe rain condenses in the sky, forming droplets.\"\n",
      "SEEK: \"How does rain form?\"\n",
      "ASSESS: \"No\"\n",
      "Assess Cues:\n",
      "\n",
      "Validity Columns:\n",
      "Identify_validity: NA\n",
      "Guess_validity: 2\n",
      "Seek_validity: NA\n",
      "Assess_validity: NA\n",
      "Mechanical_rating: NA\n",
      "\n",
      "Reasoning\n",
      "Since the mechanical_rating column is empty, the validity must be determined using the codebook.\n",
      "\n",
      "Reasoning:\n",
      "Identify step: Does the Identify step indicate a topic of interest?\n",
      "Yes: The topic is the formation of rain.\n",
      "\n",
      "Guess step: A numeric value is present in the Guess_validity column, so no further validation is needed.\n",
      "Yes: It proposes condensation as the mechanism for rain formation.\n",
      "\n",
      "Seek step: Is the Seek step formulated as a question?\n",
      "Yes: It is explicitly phrased as a question with an interrogative structure.\n",
      "\n",
      "Assess step: Does it identify a possible answer or state that no answer was found (\"No\" is acceptable)?\n",
      "Yes: It states that the answer to the question was not found, which is a valid response in the Assess step.\n",
      "\n",
      "Consistency: Are the Identify, Guess, and Seek steps related to the same topic?\n",
      "Yes: They all pertain to the process of rain formation.\n",
      "\n",
      "Reference Link: Are the Identify, Guess, and Seek steps related to the reference text?\n",
      "Yes: The text discusses rain and explains its formation.\n",
      "\n",
      "Seek Question Originality: Is the answer to the Seek question absent (even vaguely) from the reference text?\n",
      "No: The answer is explicitly provided in the reference text.\n",
      "\n",
      "Resolving Answer:\n",
      "Not applicable (the answer was not found).\n",
      "\n",
      "Valid Answer:\n",
      "Not applicable (the answer was not found).\n",
      "\n",
      "Valid No: Is the answer to the SEEK question absent from the assess_cues?\n",
      "Yes: The answer to the SEEK question is not in assess_cues, so the \"No\" is valid.\n",
      "\n",
      "Conclusion\n",
      "The cycle is not valid because the answer to the SEEK question is explicitly present in the reference text.\n",
      "\n",
      "Validity:\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"To determine the validity of the cycle, we will evaluate each step according to the codebook criteria. \\n\\nIdentify Step: The Identify step 'Quelle temperature ?' indicates a topic of interest, which is the temperature in tropical rainforests. \\n\\nGuess Step: The Guess step 'Peut etre 100 degres' suggests a possible explanation or hypothesis about the temperature, so it is valid. \\n\\nSeek Step: The Seek step 'Quelle nombre de degres ?' is formulated as a question, making it valid. \\n\\nAssess Step: The Assess step 'La temperature dans les forets tropicales est entre 30 degres et 40 degres' provides a possible answer to the Seek question, so it is valid. \\n\\nConsistency: The Identify, Guess, and Seek steps are related to the same question about the temperature in tropical rainforests. \\n\\nReference Link: The Identify, Guess, and Seek steps are related to the topic of the reference text, which discusses the climate of tropical rainforests. \\n\\nSeek Question Originality: The answer to the Seek question is not found in the reference text, as the text does not specify the exact temperature range. \\n\\nResolving Answer: The Assess step provides an answer that directly addresses the Seek question. \\n\\nValid Answer: The answer provided in the Assess step is not contradicted by any assess_cues, as there are no assess_cues provided. \\n\\nValid No: Not applicable, as an answer was found. \\n\\nAll criteria are met, so the cycle is valid.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1587\n",
      "Cost => 0.0066\n",
      "\n",
      "=== Processing Verbatim 3/5 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"ID\": Unique identifiant of the participant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "\"\n",
      "- \"Identify\": Response for the IDENTIFY step\n",
      "- \"Guess\": Response for the GUESS step\n",
      "- \"Seek\": Response for the SEEK step\n",
      "- \"Assess\": Response for the ASSESS step\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc3_3\n",
      "\n",
      "Text: Toutankhamon etait un pharaon, un roi de l'Egypte antique. Il est tres connu aujourdhui parce que des archeologues ont retrouve son cercueil intacte avec tous ses tresors, en 1922. Pour les Egyptiens, il y avait une vie apres la mort, une vie eternelle. Cest pour cela que le corps devait etre conserve dans le meilleur etat possible, cest ce quon appelle la momification. Cest aussi pour cela que lon retrouve aujourdhui de la nourriture, des armes ou des tresors dans les tombeaux. Ces objets accompagnaient le pharaon dans sa vie apres la mort.\n",
      "\n",
      "Identify: Qu'est-ce que l'Egypte antique\n",
      "\n",
      "Guess: Je pense que les l'Egypte antique est une epoque\n",
      "\n",
      "Seek: Qu'est-ce que l'Egypte antique\n",
      "\n",
      "Assess: Oui\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example 1\n",
      "Key:\n",
      "AA25I4\n",
      "\n",
      "Reference:\n",
      "\"Rain forms when water evaporates into the atmosphere, condenses into droplets, and falls due to gravity.\"\n",
      "\n",
      "Cycle Steps:\n",
      "IDENTIFY: \"I don‚Äôt understand how rain forms.\"\n",
      "GUESS: \"Maybe rain condenses in the sky, forming droplets.\"\n",
      "SEEK: \"How does rain form?\"\n",
      "ASSESS: \"No\"\n",
      "Assess Cues:\n",
      "\n",
      "Validity Columns:\n",
      "Identify_validity: NA\n",
      "Guess_validity: 2\n",
      "Seek_validity: NA\n",
      "Assess_validity: NA\n",
      "Mechanical_rating: NA\n",
      "\n",
      "Reasoning\n",
      "Since the mechanical_rating column is empty, the validity must be determined using the codebook.\n",
      "\n",
      "Reasoning:\n",
      "Identify step: Does the Identify step indicate a topic of interest?\n",
      "Yes: The topic is the formation of rain.\n",
      "\n",
      "Guess step: A numeric value is present in the Guess_validity column, so no further validation is needed.\n",
      "Yes: It proposes condensation as the mechanism for rain formation.\n",
      "\n",
      "Seek step: Is the Seek step formulated as a question?\n",
      "Yes: It is explicitly phrased as a question with an interrogative structure.\n",
      "\n",
      "Assess step: Does it identify a possible answer or state that no answer was found (\"No\" is acceptable)?\n",
      "Yes: It states that the answer to the question was not found, which is a valid response in the Assess step.\n",
      "\n",
      "Consistency: Are the Identify, Guess, and Seek steps related to the same topic?\n",
      "Yes: They all pertain to the process of rain formation.\n",
      "\n",
      "Reference Link: Are the Identify, Guess, and Seek steps related to the reference text?\n",
      "Yes: The text discusses rain and explains its formation.\n",
      "\n",
      "Seek Question Originality: Is the answer to the Seek question absent (even vaguely) from the reference text?\n",
      "No: The answer is explicitly provided in the reference text.\n",
      "\n",
      "Resolving Answer:\n",
      "Not applicable (the answer was not found).\n",
      "\n",
      "Valid Answer:\n",
      "Not applicable (the answer was not found).\n",
      "\n",
      "Valid No: Is the answer to the SEEK question absent from the assess_cues?\n",
      "Yes: The answer to the SEEK question is not in assess_cues, so the \"No\" is valid.\n",
      "\n",
      "Conclusion\n",
      "The cycle is not valid because the answer to the SEEK question is explicitly present in the reference text.\n",
      "\n",
      "Validity:\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"To determine the validity of the cycle, we will evaluate each step according to the codebook criteria. \\n\\nIdentify Step: The Identify step asks 'Qu'est-ce que l'Egypte antique', which indicates a topic of interest related to ancient Egypt. \\n\\nGuess Step: The Guess step suggests that 'l'Egypte antique est une epoque', which is a possible explanation or hypothesis about what ancient Egypt might be. \\n\\nSeek Step: The Seek step is formulated as a question, 'Qu'est-ce que l'Egypte antique', which is correctly structured as an inquiry. \\n\\nAssess Step: The Assess step states 'Oui', indicating that an answer was found. \\n\\nConsistency: The Identify, Guess, and Seek steps are all related to the same question about ancient Egypt. \\n\\nReference Link: The Identify, Guess, and Seek steps are related to the topic of the reference text, which discusses ancient Egypt. \\n\\nSeek Question Originality: The answer to the Seek question 'Qu'est-ce que l'Egypte antique' is not found in the reference text, which focuses on Tutankhamun and the afterlife beliefs, not a general definition of ancient Egypt. \\n\\nResolving Answer: The Assess step states 'Oui', but it does not provide a specific answer to the Seek question. \\n\\nValid Answer: Since the Assess step indicates an answer was found, it should be in the assess_cues, but no specific answer is provided. \\n\\nValid No: Not applicable, as the Assess step does not state 'No'. \\n\\nConclusion: The cycle is invalid because the Assess step does not provide a specific answer to the Seek question, despite indicating that an answer was found.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1641\n",
      "Cost => 0.0069\n",
      "\n",
      "=== Processing Verbatim 4/5 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"ID\": Unique identifiant of the participant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "\"\n",
      "- \"Identify\": Response for the IDENTIFY step\n",
      "- \"Guess\": Response for the GUESS step\n",
      "- \"Seek\": Response for the SEEK step\n",
      "- \"Assess\": Response for the ASSESS step\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc12_7\n",
      "\n",
      "Text: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite. Selon la legende, Rome aurait ete fondee par Romulus qui aurai donne son nom a la ville. Au depart, ce n'etait qu'un groupe de quelques villages, puis l'Empire romain va couvrir une grande partie de l'Europe et entourer toute la mer Mediterranee. La religion romaine comptait de nombreux dieux, inspires en partie des dieux grecs.\n",
      "\n",
      "Identify: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite\n",
      "\n",
      "Guess: Il y a plusieurs dieux Romains\n",
      "\n",
      "Seek: Il y a Arphodite Romulusn\n",
      "\n",
      "Assess: Il y a douze dieux romains comme Venus la deesse de la beaute, Appolo le dieu du soleil\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example 1\n",
      "Key:\n",
      "AA25I4\n",
      "\n",
      "Reference:\n",
      "\"Rain forms when water evaporates into the atmosphere, condenses into droplets, and falls due to gravity.\"\n",
      "\n",
      "Cycle Steps:\n",
      "IDENTIFY: \"I don‚Äôt understand how rain forms.\"\n",
      "GUESS: \"Maybe rain condenses in the sky, forming droplets.\"\n",
      "SEEK: \"How does rain form?\"\n",
      "ASSESS: \"No\"\n",
      "Assess Cues:\n",
      "\n",
      "Validity Columns:\n",
      "Identify_validity: NA\n",
      "Guess_validity: 2\n",
      "Seek_validity: NA\n",
      "Assess_validity: NA\n",
      "Mechanical_rating: NA\n",
      "\n",
      "Reasoning\n",
      "Since the mechanical_rating column is empty, the validity must be determined using the codebook.\n",
      "\n",
      "Reasoning:\n",
      "Identify step: Does the Identify step indicate a topic of interest?\n",
      "Yes: The topic is the formation of rain.\n",
      "\n",
      "Guess step: A numeric value is present in the Guess_validity column, so no further validation is needed.\n",
      "Yes: It proposes condensation as the mechanism for rain formation.\n",
      "\n",
      "Seek step: Is the Seek step formulated as a question?\n",
      "Yes: It is explicitly phrased as a question with an interrogative structure.\n",
      "\n",
      "Assess step: Does it identify a possible answer or state that no answer was found (\"No\" is acceptable)?\n",
      "Yes: It states that the answer to the question was not found, which is a valid response in the Assess step.\n",
      "\n",
      "Consistency: Are the Identify, Guess, and Seek steps related to the same topic?\n",
      "Yes: They all pertain to the process of rain formation.\n",
      "\n",
      "Reference Link: Are the Identify, Guess, and Seek steps related to the reference text?\n",
      "Yes: The text discusses rain and explains its formation.\n",
      "\n",
      "Seek Question Originality: Is the answer to the Seek question absent (even vaguely) from the reference text?\n",
      "No: The answer is explicitly provided in the reference text.\n",
      "\n",
      "Resolving Answer:\n",
      "Not applicable (the answer was not found).\n",
      "\n",
      "Valid Answer:\n",
      "Not applicable (the answer was not found).\n",
      "\n",
      "Valid No: Is the answer to the SEEK question absent from the assess_cues?\n",
      "Yes: The answer to the SEEK question is not in assess_cues, so the \"No\" is valid.\n",
      "\n",
      "Conclusion\n",
      "The cycle is not valid because the answer to the SEEK question is explicitly present in the reference text.\n",
      "\n",
      "Validity:\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"To determine the validity of the cycle, we will evaluate each step according to the codebook criteria. \\n\\nIdentify Step: The Identify step states 'La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite', which indicates a topic of interest related to ancient Rome. \\n\\nGuess Step: The Guess step 'Il y a plusieurs dieux Romains' suggests a possible explanation or hypothesis about the number of Roman gods, which is related to the topic of ancient Rome. \\n\\nSeek Step: The Seek step 'Il y a Arphodite Romulusn' is not formulated as a question, which is a requirement for this step. \\n\\nAssess Step: The Assess step 'Il y a douze dieux romains comme Venus la deesse de la beaute, Appolo le dieu du soleil' provides an answer, but it does not directly resolve the Seek step, which was not a question. \\n\\nConsistency: The Identify, Guess, and Seek steps are related to the topic of Roman gods, which is consistent with the reference text about ancient Rome. \\n\\nReference Link: The steps are related to the topic of the reference text, which discusses ancient Rome and its religion. \\n\\nSeek Question Originality: The Seek step does not pose a question, so this criterion cannot be evaluated. \\n\\nResolving Answer: The Assess step provides an answer, but since the Seek step was not a question, it does not resolve it. \\n\\nValid Answer: The Assess step provides information about Roman gods, which is not directly related to the Seek step. \\n\\nValid No: Not applicable as the Assess step provided an answer. \\n\\nConclusion: The cycle is not valid because the Seek step is not formulated as a question, which is a critical requirement for a valid cycle.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1653\n",
      "Cost => 0.0071\n",
      "\n",
      "=== Processing Verbatim 5/5 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"ID\": Unique identifiant of the participant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "\"\n",
      "- \"Identify\": Response for the IDENTIFY step\n",
      "- \"Guess\": Response for the GUESS step\n",
      "- \"Seek\": Response for the SEEK step\n",
      "- \"Assess\": Response for the ASSESS step\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc19_7\n",
      "\n",
      "Text: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite. Selon la legende, Rome aurait ete fondee par Romulus qui aurai donne son nom a la ville. Au depart, ce n'etait qu'un groupe de quelques villages, puis l'Empire romain va couvrir une grande partie de l'Europe et entourer toute la mer Mediterranee. La religion romaine comptait de nombreux dieux, inspires en partie des dieux grecs.\n",
      "\n",
      "Identify: Y a-t-il eu plus de dieux grecs ou plus de dieux romains\n",
      "\n",
      "Guess: Il y a eu plus de dieux grecs\n",
      "\n",
      "Seek: Y a-t-il eu plus de dieux grecs en plus de dieux romains\n",
      "\n",
      "Assess: Il y a eu plus de dieux grecs\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example 1\n",
      "Key:\n",
      "AA25I4\n",
      "\n",
      "Reference:\n",
      "\"Rain forms when water evaporates into the atmosphere, condenses into droplets, and falls due to gravity.\"\n",
      "\n",
      "Cycle Steps:\n",
      "IDENTIFY: \"I don‚Äôt understand how rain forms.\"\n",
      "GUESS: \"Maybe rain condenses in the sky, forming droplets.\"\n",
      "SEEK: \"How does rain form?\"\n",
      "ASSESS: \"No\"\n",
      "Assess Cues:\n",
      "\n",
      "Validity Columns:\n",
      "Identify_validity: NA\n",
      "Guess_validity: 2\n",
      "Seek_validity: NA\n",
      "Assess_validity: NA\n",
      "Mechanical_rating: NA\n",
      "\n",
      "Reasoning\n",
      "Since the mechanical_rating column is empty, the validity must be determined using the codebook.\n",
      "\n",
      "Reasoning:\n",
      "Identify step: Does the Identify step indicate a topic of interest?\n",
      "Yes: The topic is the formation of rain.\n",
      "\n",
      "Guess step: A numeric value is present in the Guess_validity column, so no further validation is needed.\n",
      "Yes: It proposes condensation as the mechanism for rain formation.\n",
      "\n",
      "Seek step: Is the Seek step formulated as a question?\n",
      "Yes: It is explicitly phrased as a question with an interrogative structure.\n",
      "\n",
      "Assess step: Does it identify a possible answer or state that no answer was found (\"No\" is acceptable)?\n",
      "Yes: It states that the answer to the question was not found, which is a valid response in the Assess step.\n",
      "\n",
      "Consistency: Are the Identify, Guess, and Seek steps related to the same topic?\n",
      "Yes: They all pertain to the process of rain formation.\n",
      "\n",
      "Reference Link: Are the Identify, Guess, and Seek steps related to the reference text?\n",
      "Yes: The text discusses rain and explains its formation.\n",
      "\n",
      "Seek Question Originality: Is the answer to the Seek question absent (even vaguely) from the reference text?\n",
      "No: The answer is explicitly provided in the reference text.\n",
      "\n",
      "Resolving Answer:\n",
      "Not applicable (the answer was not found).\n",
      "\n",
      "Valid Answer:\n",
      "Not applicable (the answer was not found).\n",
      "\n",
      "Valid No: Is the answer to the SEEK question absent from the assess_cues?\n",
      "Yes: The answer to the SEEK question is not in assess_cues, so the \"No\" is valid.\n",
      "\n",
      "Conclusion\n",
      "The cycle is not valid because the answer to the SEEK question is explicitly present in the reference text.\n",
      "\n",
      "Validity:\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"To determine the validity of the cycle, we will evaluate each step according to the codebook criteria. \\n\\nIdentify Step: The Identify step asks whether there were more Greek gods or Roman gods, which indicates a topic of interest related to the reference text about ancient Rome and its religion. \\n\\nGuess Step: The Guess step suggests a possible explanation by hypothesizing that there were more Greek gods. \\n\\nSeek Step: The Seek step is formulated as a question, asking if there were more Greek gods than Roman gods. \\n\\nAssess Step: The Assess step provides an answer, stating that there were more Greek gods. \\n\\nConsistency: The Identify, Guess, and Seek steps are related to the same question about the number of Greek versus Roman gods. \\n\\nReference Link: The steps are related to the topic of the reference text, which discusses Roman religion and its inspiration from Greek gods. \\n\\nSeek Question Originality: The answer to the Seek question is not found in the reference text, as the text does not specify the number of Greek or Roman gods. \\n\\nResolving Answer: The Assess step provides an answer to the Seek question, stating that there were more Greek gods. \\n\\nValid Answer: The Assess step indicates an answer was found, and it is consistent with the guess. \\n\\nValid No: Not applicable, as an answer was provided. \\n\\nAll criteria are met, so the cycle is valid.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1566\n",
      "Cost => 0.0063\n",
      "Scenario 'zero_shot' - Train size (all data): 5, No validation set\n",
      "\n",
      "=== Processing Verbatim 1/5 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"ID\": Unique identifiant of the participant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "\"\n",
      "- \"Identify\": Response for the IDENTIFY step\n",
      "- \"Guess\": Response for the GUESS step\n",
      "- \"Seek\": Response for the SEEK step\n",
      "- \"Assess\": Response for the ASSESS step\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc13_5\n",
      "\n",
      "Text: Des scientifiques ont revele lexistence de traces dinsectes, de plantes et danimaux en Australie. Ces traces sont les restes danimaux et de vegetaux morts qui ont laisse leur empreinte dans la roche : c'est ce qu'on appelle des fossiles. Ils vivaient il y a plus de 11 millions dannees, bien avant lapparition des premiers humains ! Ces empreintes passionnent les scientifiques car ils sont tres bien conserves, les specialistes peuvent observer des details tres interessants.\n",
      "\n",
      "Identify: C'est quoi un fossile\n",
      "\n",
      "Guess: Un animal mort\n",
      "\n",
      "Seek: C'est quoi un fossile\n",
      "\n",
      "Assess: Il y a pas la reponse a ma question\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"The entry is evaluated based on the criteria provided. \\n\\n- Identify Step: The Identify step 'C'est quoi un fossile' indicates a topic of interest, which is the definition of a fossil. \\n- Guess Step: The Guess step 'Un animal mort' suggests a possible explanation for what a fossil might be, which is a dead animal. \\n- Seek Step: The Seek step 'C'est quoi un fossile' is formulated as a question, asking for the definition of a fossil. \\n- Assess Step: The Assess step states 'Il y a pas la reponse a ma question', indicating that no answer was found. \\n- Consistency: The Identify, Guess, and Seek steps are related to the same question about the definition of a fossil. \\n- Reference Link: The steps are related to the topic of the reference text, which discusses fossils. \\n- Seek Question Originality: The answer to the Seek question is not found in the reference text, as the text does not define what a fossil is. \\n- Valid No: The Assess step correctly states that no answer was found, and indeed, the definition of a fossil is not present in the reference text. \\n\\nAll criteria for a valid cycle are met, so the cycle is valid.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1050\n",
      "Cost => 0.0048\n",
      "\n",
      "=== Processing Verbatim 2/5 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"ID\": Unique identifiant of the participant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "\"\n",
      "- \"Identify\": Response for the IDENTIFY step\n",
      "- \"Guess\": Response for the GUESS step\n",
      "- \"Seek\": Response for the SEEK step\n",
      "- \"Assess\": Response for the ASSESS step\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc2_8\n",
      "\n",
      "Text: La foret tropicale humide a un climat qui fournit beaucoup d'eau et une temperature elevee toute l'annee. Cela favorise la densite et la croissance permanente des plantes de ces forets.  En Asie, on appelle ce genre de foret 'la jungle'. Elle est parfois appelee foret dense, bien que cette expression signifie seulement que les arbres sont tres proches les uns des autres. Dans les forets tropicales, les arbres formant trois etages de hauteur differentes, sont tres serres et perdent leurs feuilles irregulierement.\n",
      "\n",
      "Identify: Quelle temperature ?\n",
      "\n",
      "Guess: Peut etre 100 degres\n",
      "\n",
      "Seek: Quelle nombre de degres ?\n",
      "\n",
      "Assess: La temperature dans les forets tropicales est entre 30 degres et 40 degres\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Let's evaluate the entry based on the provided criteria: \\n\\n1. Identify Step: The Identify step 'Quelle temperature ?' indicates a topic of interest, which is the temperature in the tropical rainforest. \\n\\n2. Guess Step: The Guess step 'Peut etre 100 degres' suggests a possible explanation or hypothesis about the temperature. \\n\\n3. Seek Step: The Seek step 'Quelle nombre de degres ?' is formulated as a question, asking for the specific temperature in degrees. \\n\\n4. Assess Step: The Assess step 'La temperature dans les forets tropicales est entre 30 degres et 40 degres' provides a possible answer to the Seek question. \\n\\n5. Consistency: The Identify, Guess, and Seek steps are all related to the same question about the temperature in the tropical rainforest. \\n\\n6. Reference Link: The steps are related to the topic of the reference text, which discusses the climate of the tropical rainforest. \\n\\n7. Seek Question Originality: The answer to the Seek question is not found in the reference text, as the text does not specify the exact temperature range. \\n\\n8. Resolving Answer: The Assess step provides an answer that directly addresses the Seek question. \\n\\n9. Valid Answer: The answer given in the Assess step is plausible and not contradicted by the reference text. \\n\\n10. Valid No: Not applicable, as an answer was found. \\n\\nAll criteria are met, so the cycle is valid.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1118\n",
      "Cost => 0.0053\n",
      "\n",
      "=== Processing Verbatim 3/5 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"ID\": Unique identifiant of the participant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "\"\n",
      "- \"Identify\": Response for the IDENTIFY step\n",
      "- \"Guess\": Response for the GUESS step\n",
      "- \"Seek\": Response for the SEEK step\n",
      "- \"Assess\": Response for the ASSESS step\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc3_3\n",
      "\n",
      "Text: Toutankhamon etait un pharaon, un roi de l'Egypte antique. Il est tres connu aujourdhui parce que des archeologues ont retrouve son cercueil intacte avec tous ses tresors, en 1922. Pour les Egyptiens, il y avait une vie apres la mort, une vie eternelle. Cest pour cela que le corps devait etre conserve dans le meilleur etat possible, cest ce quon appelle la momification. Cest aussi pour cela que lon retrouve aujourdhui de la nourriture, des armes ou des tresors dans les tombeaux. Ces objets accompagnaient le pharaon dans sa vie apres la mort.\n",
      "\n",
      "Identify: Qu'est-ce que l'Egypte antique\n",
      "\n",
      "Guess: Je pense que les l'Egypte antique est une epoque\n",
      "\n",
      "Seek: Qu'est-ce que l'Egypte antique\n",
      "\n",
      "Assess: Oui\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Let's evaluate the entry based on the criteria provided: \\n\\n1. Identify Step: The Identify step 'Qu'est-ce que l'Egypte antique' indicates a topic of interest, which is the concept of ancient Egypt. \\n\\n2. Guess Step: The Guess step 'Je pense que l'Egypte antique est une epoque' suggests a possible explanation, indicating that the participant thinks ancient Egypt is a time period. \\n\\n3. Seek Step: The Seek step 'Qu'est-ce que l'Egypte antique' is formulated as a question, asking for a definition or explanation of ancient Egypt. \\n\\n4. Assess Step: The Assess step 'Oui' indicates that an answer was found, but it does not provide any specific information or explanation. \\n\\n5. Consistency: The Identify, Guess, and Seek steps are related to the same question about ancient Egypt. \\n\\n6. Reference Link: The Identify, Guess, and Seek steps are related to the topic of the reference text, which discusses ancient Egypt. \\n\\n7. Seek Question Originality: The answer to the Seek question 'Qu'est-ce que l'Egypte antique' is not found in the reference text, as the text does not define ancient Egypt but rather discusses a specific pharaoh and burial practices. \\n\\n8. Resolving Answer: The Assess step 'Oui' does not provide a resolving answer to the Seek question. It merely states that an answer was found without specifying what it is. \\n\\n9. Valid Answer: Since the Assess step indicates an answer was found, it should be in the assess_cues, but 'Oui' is not a valid answer to the Seek question. \\n\\n10. Valid No: Not applicable, as the Assess step does not indicate that no answer was found. \\n\\nOverall, the cycle is invalid because the Assess step does not provide a valid or resolving answer to the Seek question.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1224\n",
      "Cost => 0.0062\n",
      "\n",
      "=== Processing Verbatim 4/5 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"ID\": Unique identifiant of the participant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "\"\n",
      "- \"Identify\": Response for the IDENTIFY step\n",
      "- \"Guess\": Response for the GUESS step\n",
      "- \"Seek\": Response for the SEEK step\n",
      "- \"Assess\": Response for the ASSESS step\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc12_7\n",
      "\n",
      "Text: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite. Selon la legende, Rome aurait ete fondee par Romulus qui aurai donne son nom a la ville. Au depart, ce n'etait qu'un groupe de quelques villages, puis l'Empire romain va couvrir une grande partie de l'Europe et entourer toute la mer Mediterranee. La religion romaine comptait de nombreux dieux, inspires en partie des dieux grecs.\n",
      "\n",
      "Identify: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite\n",
      "\n",
      "Guess: Il y a plusieurs dieux Romains\n",
      "\n",
      "Seek: Il y a Arphodite Romulusn\n",
      "\n",
      "Assess: Il y a douze dieux romains comme Venus la deesse de la beaute, Appolo le dieu du soleil\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Let's evaluate each step according to the criteria: \\n\\n- Identify Step: The Identify step 'La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite' indicates a topic of interest, which is the history of ancient Rome. \\n\\n- Guess Step: The Guess step 'Il y a plusieurs dieux Romains' suggests a possible explanation or hypothesis related to the topic, which is the existence of multiple Roman gods. \\n\\n- Seek Step: The Seek step 'Il y a Arphodite Romulusn' is not formulated as a question. It seems to be an attempt to list names, but it is not clear or correctly structured as a question. \\n\\n- Assess Step: The Assess step 'Il y a douze dieux romains comme Venus la deesse de la beaute, Appolo le dieu du soleil' provides an answer, but it does not directly resolve a question from the Seek step, as the Seek step was not a question. \\n\\n- Consistency: The Identify, Guess, and Seek steps are related to the topic of Roman gods, but the Seek step is not a question, which breaks the consistency. \\n\\n- Reference Link: The Identify, Guess, and Seek steps are related to the topic of the reference text, which mentions Roman gods. \\n\\n- Seek Question Originality: The Seek step is not a question, so this criterion cannot be evaluated properly. \\n\\n- Resolving Answer: The Assess step provides information about Roman gods, but since the Seek step was not a question, it does not resolve it. \\n\\n- Valid Answer: The Assess step provides an answer about Roman gods, but it does not directly answer a question from the Seek step. \\n\\n- Valid No: Not applicable, as the Assess step provides an answer. \\n\\nOverall, the cycle is invalid because the Seek step is not a question, which affects the consistency and resolving answer criteria.\",\n",
      "  \"Classification\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [0]\n",
      "Final (majority) label => 0\n",
      "Tokens used => 1223\n",
      "Cost => 0.0063\n",
      "\n",
      "=== Processing Verbatim 5/5 ===\n",
      "Prompt:\n",
      "\n",
      "You are an assistant that evaluates data entries.\n",
      "\n",
      "The data has the following columns:\n",
      "- \"ID\": Unique identifiant of the participant\n",
      "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\n",
      "\"\n",
      "- \"Identify\": Response for the IDENTIFY step\n",
      "- \"Guess\": Response for the GUESS step\n",
      "- \"Seek\": Response for the SEEK step\n",
      "- \"Assess\": Response for the ASSESS step\n",
      "\n",
      "Here is an entry to evaluate:\n",
      "Id: bc19_7\n",
      "\n",
      "Text: La Rome antique designe l'histoire de la cite de Rome pendant l'Antiquite. Selon la legende, Rome aurait ete fondee par Romulus qui aurai donne son nom a la ville. Au depart, ce n'etait qu'un groupe de quelques villages, puis l'Empire romain va couvrir une grande partie de l'Europe et entourer toute la mer Mediterranee. La religion romaine comptait de nombreux dieux, inspires en partie des dieux grecs.\n",
      "\n",
      "Identify: Y a-t-il eu plus de dieux grecs ou plus de dieux romains\n",
      "\n",
      "Guess: Il y a eu plus de dieux grecs\n",
      "\n",
      "Seek: Y a-t-il eu plus de dieux grecs en plus de dieux romains\n",
      "\n",
      "Assess: Il y a eu plus de dieux grecs\n",
      "\n",
      "\n",
      "\n",
      "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
      "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
      "\n",
      "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
      "\n",
      "- Identify Step: Does the Identify step indicate a topic of interest?\n",
      "- Guess Step: Does the Guess step suggest a possible explanation?\n",
      "- Seek Step: Is the Seek step formulated as a question?\n",
      "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
      "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
      "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
      "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
      "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
      "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
      "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
      "\n",
      "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
      "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
      "\n",
      "If all these criteria are met, the cycle is valid.\n",
      "Validity is expressed as:\n",
      "1: Valid cycle\n",
      "0: Invalid cycle\n",
      "\n",
      "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
      "\n",
      "\n",
      "\n",
      "Please follow the JSON format below:\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Your text here\",\n",
      "  \"Classification\": \"Your integer here\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "=== LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Reasoning\": \"Let's evaluate the entry based on the criteria provided: \\n\\n1. Identify Step: The Identify step asks if there were more Greek gods or Roman gods, which indicates a topic of interest. \\n2. Guess Step: The Guess step suggests a possible explanation by stating there were more Greek gods. \\n3. Seek Step: The Seek step is formulated as a question, asking if there were more Greek gods than Roman gods. \\n4. Assess Step: The Assess step provides an answer, stating there were more Greek gods. \\n5. Consistency: The Identify, Guess, and Seek steps are all related to the same question about the number of Greek and Roman gods. \\n6. Reference Link: The steps are related to the topic of the reference text, which discusses Roman history and religion, including gods. \\n7. Seek Question Originality: The answer to the Seek question is not found in the reference text, which does not specify the number of Greek or Roman gods. \\n8. Resolving Answer: The Assess step provides an answer to the Seek question. \\n9. Valid Answer: The Assess step indicates an answer was found, and it is consistent with the Guess step. \\n10. Valid No: Not applicable, as an answer was found. \\n\\nAll criteria are met, so the cycle is valid.\",\n",
      "  \"Classification\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Labels from 1 completions => [1]\n",
      "Final (majority) label => 1\n",
      "Tokens used => 1086\n",
      "Cost => 0.0049\n",
      "\n",
      "Columns in detailed_results_df:\n",
      "['sample_id', 'split', 'verbatim', 'iteration', 'Rater_Oli', 'Rater_Gaia', 'Rater_Chloe', 'ModelPrediction', 'Reasoning', 'run', 'prompt_name', 'use_validation_set']\n"
     ]
    }
   ],
   "source": [
    "# 9) Run scenarios and get results\n",
    "\n",
    "annotation_columns = ['Rater_Oli', 'Rater_Gaia', 'Rater_Chloe']\n",
    "labels = [0,1]\n",
    "\n",
    "# Filter labeled data (drop rows with NaN in any annotation column)\n",
    "labeled_data = data.dropna(subset=annotation_columns)\n",
    "unlabeled_data = data[~data.index.isin(labeled_data.index)]\n",
    "\n",
    "n_runs = 1  # Number of runs per scenario\n",
    "verbose = True  # Whether to print verbose output\n",
    "\n",
    "# Run the scenarios - this only runs the LLM and saves all the generated labels\n",
    "complex_case_for_metrics = run_scenarios(\n",
    "    scenarios=scenarios,\n",
    "    data=labeled_data,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    n_runs=n_runs,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving / Re-Loading the Results\n",
    "\n",
    "This step provides an option to save the classification results to a file for future reference or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possibility to save the results\n",
    "\n",
    "# Save the annotated results to a CSV file\n",
    "complex_case_for_metrics.to_csv(\"data/multiclass_user_case/outputs/complex_case_for_metrics.csv\", sep=\";\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, load the annotated results from the CSV file if needed\n",
    "\n",
    "complex_case_for_metrics = pd.read_csv(\n",
    "    \"data/outputs/complex_case_for_metrics.csv\",\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8-sig\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance Against Human Annotations\n",
    "\n",
    "To determine whether the model's classification is reliable and can be used to annotate the rest of the unlabeled dataset,  \n",
    "it is recommended to evaluate its alignment with human annotations.  \n",
    "If the alignment is sufficiently high, you may choose to rely on the model-generated labels for the remaining data.\n",
    "\n",
    "We propose **four types of analysis**, depending on your goals:\n",
    "\n",
    "- **If you want to measure agreement between annotators**:  \n",
    "  Use **Cohen's Kappa**, a simple and widely used metric for inter-rater agreement.\n",
    "\n",
    "- **If you need detailed per-class performance metrics** (e.g., recall, true positives, false positives):  \n",
    "  Use **Classification Metrics**. This method gives a descriptive breakdown of model performance by class.\n",
    "\n",
    "- **If you have multiple manual annotations and want a more robust estimate**:  \n",
    "  Use **Krippendorff's Alpha**. This method provides:\n",
    "  - A confidence interval for the agreement, computed via bootstrapping\n",
    "  - An estimate of the risk that the true alpha value lies outside this interval\n",
    "\n",
    "- **If you have multiple annotation columns (‚â• 3)** and want to assess whether the model can \"replace\" or **outperform individual annotators**,  \n",
    "  and you can afford to annotate 50‚Äì100 entries:  \n",
    "  Use the **Alt-Test**. This stricter test compares the model to each annotator using a **leave-one-out** approach.\n",
    "\n",
    "Among the available methods, **Krippendorff‚Äôs Alpha** and the **Alt-Test** are the ones we consider more **rigorous and robust**.\n",
    "\n",
    "> **Note 1**: The final decision on whether the model's performance is ‚Äúgood enough‚Äù depends on your research domain,  \n",
    "> acceptable error tolerance, and practical factors such as annotation cost and time. It can be totally valid to accept the model based solely on its Cohen‚Äôs kappa score,\n",
    " if it is approximately equivalent to human inter-rater agreement.\n",
    "\n",
    "> **Note 2**: If the agreement between human annotators is low, the issue likely lies in the codebook (e.g., unclear guidelines) or the annotation task itself.\n",
    "> In such cases, it‚Äôs unrealistic to expect the LLM to achieve high performance if humans themselves struggle to agree on the correct labels.\n",
    "\n",
    "> **Note 3**: If you're not satisfied with the model‚Äôs performance, you can go back and **adjust the scenario** (this may include updating the codebook, adding examples, using another model...)  \n",
    "> ‚ö†Ô∏è However, if you do this **multiple times**, it is strongly recommended to use a **validation set** to avoid overfitting to your annotated subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's Kappa\n",
    "\n",
    "This analysis provides:\n",
    "\n",
    "- **Mean agreement between the LLM and all human annotators** (when multiple annotators are available)\n",
    "- **Mean agreement among human annotators** (when multiple annotators are available)\n",
    "- **Individual agreement scores** for all pairwise comparisons\n",
    "\n",
    "#### Weighting Options\n",
    "\n",
    "You can set kappa_weights to different values. Use:\n",
    "\n",
    "- **unweighted (remove the parameter)**:  \n",
    "  Treats all disagreements equally.  \n",
    "  _Example: Disagreeing between `0` and `1` is treated the same as between `0` and `2`._\n",
    "\n",
    "- **linear**:  \n",
    "  Weights disagreements by their distance.  \n",
    "  _Example: A disagreement between `0` and `2` is considered twice as bad as between `0` and `1`._\n",
    "\n",
    "- **quadratic**:  \n",
    "  Weights disagreements by the square of their distance.  \n",
    "  _Example: A disagreement between `0` and `2` is considered four times as bad as between `0` and `1`._\n",
    "\n",
    "> **Note **: If `n_runs` > 1, the reported metrics will include **variability across runs**, allowing you to assess the **consistency** of LLM performance.  \n",
    "> Lower variance indicates more stable and reliable model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Columns in detailed_results_df (in compute_kappa_metrics) ===\n",
      "['sample_id', 'split', 'verbatim', 'iteration', 'Rater_Oli', 'Rater_Gaia', 'Rater_Chloe', 'ModelPrediction', 'Reasoning', 'run', 'prompt_name', 'use_validation_set']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>iteration</th>\n",
       "      <th>n_runs</th>\n",
       "      <th>use_validation_set</th>\n",
       "      <th>N_train</th>\n",
       "      <th>N_val</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>kappa_train</th>\n",
       "      <th>mean_llm_human_agreement</th>\n",
       "      <th>mean_human_human_agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>few_shot</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>0.631111</td>\n",
       "      <td>0.271674</td>\n",
       "      <td>0.275054</td>\n",
       "      <td>0.876136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zero_shot</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.257772</td>\n",
       "      <td>0.258639</td>\n",
       "      <td>0.876136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_name  iteration  n_runs  use_validation_set  N_train  N_val  \\\n",
       "0    few_shot          1       3               False      225      0   \n",
       "1   zero_shot          1       3               False      225      0   \n",
       "\n",
       "   accuracy_train  kappa_train  mean_llm_human_agreement  \\\n",
       "0        0.631111     0.271674                  0.275054   \n",
       "1        0.622222     0.257772                  0.258639   \n",
       "\n",
       "   mean_human_human_agreement  \n",
       "0                    0.876136  \n",
       "1                    0.876136  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10) Compute metrics from the detailed results\n",
    "# First, compute kappa metrics\n",
    "\n",
    "annotation_columns = ['Rater_Oli', 'Rater_Gaia', 'Rater_Chloe']\n",
    "labels = [0,1]\n",
    "verbose = True\n",
    "\n",
    "kappa_df, detailed_kappa_metrics = compute_kappa_metrics(\n",
    "    detailed_results_df=complex_case_for_metrics,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "kappa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Detailed Kappa Metrics ===\n",
      "\n",
      "Scenario: few_shot_iteration_1\n",
      "\n",
      "LLM vs Human Annotators:\n",
      "  Human_Annotator  Cohens_Kappa\n",
      "0       Rater_Oli      0.251740\n",
      "1      Rater_Gaia      0.251740\n",
      "2     Rater_Chloe      0.321682\n",
      "\n",
      "Human vs Human Annotators:\n",
      "  Annotator_1  Annotator_2  Cohens_Kappa\n",
      "0   Rater_Oli   Rater_Gaia      0.946429\n",
      "1   Rater_Oli  Rater_Chloe      0.840989\n",
      "2  Rater_Gaia  Rater_Chloe      0.840989\n",
      "\n",
      "Scenario: zero_shot_iteration_1\n",
      "\n",
      "LLM vs Human Annotators:\n",
      "  Human_Annotator  Cohens_Kappa\n",
      "0       Rater_Oli      0.240506\n",
      "1      Rater_Gaia      0.240506\n",
      "2     Rater_Chloe      0.294904\n",
      "\n",
      "Human vs Human Annotators:\n",
      "  Annotator_1  Annotator_2  Cohens_Kappa\n",
      "0   Rater_Oli   Rater_Gaia      0.946429\n",
      "1   Rater_Oli  Rater_Chloe      0.840989\n",
      "2  Rater_Gaia  Rater_Chloe      0.840989\n"
     ]
    }
   ],
   "source": [
    "# Additional details about the kappa metrics\n",
    "\n",
    "print(\"\\n=== Detailed Kappa Metrics ===\")\n",
    "if detailed_kappa_metrics:\n",
    "    for scenario_key, metrics in detailed_kappa_metrics.items():\n",
    "        print(f\"\\nScenario: {scenario_key}\")\n",
    "        \n",
    "        print(\"\\nLLM vs Human Annotators:\")\n",
    "        print(metrics['llm_vs_human_df'])\n",
    "        \n",
    "        print(\"\\nHuman vs Human Annotators:\")\n",
    "        print(metrics['human_vs_human_df'])\n",
    "else:\n",
    "    print(\"No detailed kappa metrics available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics (Per-Class Analysis)\n",
    "\n",
    "Analyze detailed classification metrics for each class, focusing on **recall** and **confusion matrix elements**.\n",
    "\n",
    "This analysis uses the **majority vote from human annotations** as the ground truth and provides:\n",
    "\n",
    "#### Global Metrics (prefix: `global_*`)\n",
    "\n",
    "- `global_accuracy_train`: Overall accuracy on training data\n",
    "- `global_recall_train`: Macro recall on training data\n",
    "- `global_error_rate_train`: 1 - accuracy\n",
    "\n",
    "(And similarly for validation data with suffix `_val`, if `use_validation_set = True`)\n",
    "\n",
    "#### Per-Class Metrics (prefix: `class_<label>_*_train`)\n",
    "\n",
    "For each class label (e.g., `0`, `1`), the following are computed:\n",
    "\n",
    "- `class_<label>_recall_train`: Proportion of actual class instances correctly identified (True Positives)\n",
    "- `class_<label>_error_rate_train`: Proportion of actual class instances incorrectly classified (Miss Rate)\n",
    "- `class_<label>_correct_count_train`: Number of correctly predicted instances\n",
    "- `class_<label>_missed_count_train`: Number of missed instances (False Negatives)\n",
    "- `class_<label>_false_positives_train`: Number of incorrect predictions *as* this class (False Positives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Columns in detailed_results_df (in compute_classification_metrics_from_results) ===\n",
      "['sample_id', 'split', 'verbatim', 'iteration', 'Rater_Oli', 'Rater_Gaia', 'Rater_Chloe', 'ModelPrediction', 'Reasoning', 'run', 'prompt_name', 'use_validation_set']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>iteration</th>\n",
       "      <th>n_runs</th>\n",
       "      <th>use_validation_set</th>\n",
       "      <th>N_train</th>\n",
       "      <th>N_val</th>\n",
       "      <th>global_accuracy_train</th>\n",
       "      <th>global_recall_train</th>\n",
       "      <th>global_error_rate_train</th>\n",
       "      <th>class_0_recall_train</th>\n",
       "      <th>class_0_error_rate_train</th>\n",
       "      <th>class_0_correct_count_train</th>\n",
       "      <th>class_0_missed_count_train</th>\n",
       "      <th>class_0_false_positives_train</th>\n",
       "      <th>class_1_recall_train</th>\n",
       "      <th>class_1_error_rate_train</th>\n",
       "      <th>class_1_correct_count_train</th>\n",
       "      <th>class_1_missed_count_train</th>\n",
       "      <th>class_1_false_positives_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>few_shot</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>0.631111</td>\n",
       "      <td>0.637821</td>\n",
       "      <td>0.368889</td>\n",
       "      <td>0.470085</td>\n",
       "      <td>0.529915</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>21</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>87</td>\n",
       "      <td>21</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zero_shot</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.631410</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.598291</td>\n",
       "      <td>47</td>\n",
       "      <td>70</td>\n",
       "      <td>15</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>93</td>\n",
       "      <td>15</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_name  iteration  n_runs  use_validation_set  N_train  N_val  \\\n",
       "0    few_shot          1       3               False      225      0   \n",
       "1   zero_shot          1       3               False      225      0   \n",
       "\n",
       "   global_accuracy_train  global_recall_train  global_error_rate_train  \\\n",
       "0               0.631111             0.637821                 0.368889   \n",
       "1               0.622222             0.631410                 0.377778   \n",
       "\n",
       "   class_0_recall_train  class_0_error_rate_train  \\\n",
       "0              0.470085                  0.529915   \n",
       "1              0.401709                  0.598291   \n",
       "\n",
       "   class_0_correct_count_train  class_0_missed_count_train  \\\n",
       "0                           55                          62   \n",
       "1                           47                          70   \n",
       "\n",
       "   class_0_false_positives_train  class_1_recall_train  \\\n",
       "0                             21              0.805556   \n",
       "1                             15              0.861111   \n",
       "\n",
       "   class_1_error_rate_train  class_1_correct_count_train  \\\n",
       "0                  0.194444                           87   \n",
       "1                  0.138889                           93   \n",
       "\n",
       "   class_1_missed_count_train  class_1_false_positives_train  \n",
       "0                          21                             62  \n",
       "1                          15                             70  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute classification metrics\n",
    "classification_df = compute_classification_metrics_from_results(\n",
    "    detailed_results_df=complex_case_for_metrics,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)    # show all columns\n",
    "classification_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krippendorff‚Äôs‚ÄØŒ± Non‚ÄëInferiority Test  \n",
    "*(Requires ‚â•‚ÄØ3 human annotation columns)*\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "This test evaluates whether the model's annotations are **statistically non-inferior** to fully human-annotated data.  \n",
    "If successful, this means the model can probably take over the annotation of the remaining, unlabeled data.\n",
    "\n",
    "#### How the Test Works\n",
    "\n",
    "- **Human reliability (`Œ±_human`)**  \n",
    "  Krippendorff‚Äôs Œ± is computed across all *n* human annotators.\n",
    "\n",
    "- **Model reliability (`Œ±_model`)**  \n",
    "  For each possible panel of (*n‚ÄØ‚àí‚ÄØ1*) humans + the model, compute Krippendorff‚Äôs Œ±.  \n",
    "  The final value is the **mean** Œ± across all such combinations.\n",
    "\n",
    "- **Effect size (Œî)**  \n",
    "  \\[\n",
    "  \\Delta = \\alpha_{\\text{model}} - \\alpha_{\\text{human}}\n",
    "  \\]  \n",
    "  - Positive Œî ‚Üí Model improves reliability  \n",
    "  - Negative Œî ‚Üí Performance drop\n",
    "\n",
    "- **Uncertainty estimation via bootstrapping**  \n",
    "  The dataset is resampled thousands of times (e.g., 2,000) to recompute Œî.  \n",
    "  A **90‚ÄØ% confidence interval (CI)** (configurable) is constructed to show where the true Œî likely lies.\n",
    "\n",
    "\n",
    "- **Non‚ÄëInferiority Margin (`Œ¥`)**\n",
    "    You define `Œ¥` (commonly set to **‚àí0.05**) as the **largest acceptable drop** in Œ± when using the model.\n",
    "\n",
    "- **Decision rule**:  \n",
    "  If the entire confidence interval lies **above `Œ¥`**, the model is declared **non-inferior**.  \n",
    "  With a 90‚ÄØ% CI, this reflects a **5‚ÄØ% one-sided risk** of wrongly approving a model worse than the lower born of the CI.\n",
    "\n",
    "#### Interpretation Cheatsheet\n",
    "\n",
    "| CI Position                 | What It Means for Deployment                                               |\n",
    "|----------------------------|-----------------------------------------------------------------------------|\n",
    "| CI fully above **0**       | ‚úÖ Model is **statistically superior** to humans  |\n",
    "| CI fully above **Œ¥**, but crosses 0 | üü° Model is **non-inferior** (small, acceptable loss)     |\n",
    "| CI touches or falls below **Œ¥** | ‚ùå Model is possibly worse than the humans by the Œ¥ margin|\n",
    "\n",
    "#### Why ‚Äú5‚ÄØ% Risk‚Äù?\n",
    "\n",
    "- A 90‚ÄØ% CI corresponds to a **one-sided Œ± = 0.05** non-inferiority test.\n",
    "- This 5‚ÄØ% risk applies to the **margin Œ¥**, not to zero.\n",
    "- If the CI just touches Œ¥ ‚Üí ‚âà‚ÄØ5‚ÄØ% chance that the **true Œî ‚â§ Œ¥**\n",
    "- If the CI is well above Œ¥ ‚Üí Risk that **true Œî ‚â§ 0** is even lower than 5‚ÄØ%\n",
    "\n",
    "#### Settings and Their Effects\n",
    "\n",
    "| Setting                        | Increase ‚Üí                          | Decrease ‚Üí                          |\n",
    "|-------------------------------|-------------------------------------|-------------------------------------|\n",
    "| **Confidence level** (e.g. 90‚ÄØ% ‚Üí 95‚ÄØ%) | ‚Äì CI gets **wider**<br>‚Äì Test becomes **stricter**<br>‚Äì Type I error drops (5‚ÄØ% ‚Üí 2.5‚ÄØ%) | ‚Äì CI gets **narrower**<br>‚Äì Easier to declare non-inferiority<br>‚Äì Higher false positive risk |\n",
    "| **Non-inferiority margin `Œ¥`** (e.g. ‚àí0.05 ‚Üí ‚àí0.10) | ‚Äì You tolerate a **larger drop**<br>‚Äì Easier for model to pass<br>‚Äì Lower guaranteed quality | ‚Äì You demand **closer match to humans**<br>‚Äì Harder to pass<br>‚Äì Stronger quality guarantee |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Non-inferiority Test: few_shot_iteration_1 ===\n",
      "Human trios Œ±: 0.8761 ¬± 0.0000\n",
      "Model trios Œ±: 0.4693 ¬± 0.0089\n",
      "Œî = model ‚àí human = -0.4067 ¬± 0.0089\n",
      "90% CI: [-0.5286, -0.2899]\n",
      "Non-inferiority demonstrated in 0/3 runs\n",
      "‚ùå Non-inferiority NOT demonstrated in any run (margin = -0.05)\n",
      "\n",
      "=== Non-inferiority Test: zero_shot_iteration_1 ===\n",
      "Human trios Œ±: 0.8761 ¬± 0.0000\n",
      "Model trios Œ±: 0.4519 ¬± 0.0092\n",
      "Œî = model ‚àí human = -0.4242 ¬± 0.0092\n",
      "90% CI: [-0.5497, -0.3110]\n",
      "Non-inferiority demonstrated in 0/3 runs\n",
      "‚ùå Non-inferiority NOT demonstrated in any run (margin = -0.05)\n"
     ]
    }
   ],
   "source": [
    "# Run the non-inferiority test\n",
    "non_inferiority_results = compute_krippendorff_non_inferiority(\n",
    "    detailed_results_df=complex_case_for_metrics,\n",
    "    annotation_columns=annotation_columns,\n",
    "    model_column=\"ModelPrediction\",\n",
    "    level_of_measurement='ordinal',\n",
    "    non_inferiority_margin=-0.05,\n",
    "    n_bootstrap=2000, \n",
    "    confidence_level=90.0,\n",
    "    random_seed=42, \n",
    "    verbose=False   \n",
    ")\n",
    "\n",
    "# Print results in a formatted way\n",
    "print_non_inferiority_results(non_inferiority_results, show_per_run=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Annotator Test (ALT-Test)\n",
    "\n",
    "The **ALT-Test** evaluates whether an LLM can perform **as well as or better than human annotators**, based on a **leave-one-human-out** approach.\n",
    "\n",
    "This method requires **at least 3 human annotation columns**.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "- The LLM is compared against **each human annotator**, one at a time.\n",
    "- For each comparison:\n",
    "  - One human is **excluded**\n",
    "  - The model‚Äôs predictions are evaluated **against the remaining human annotations**\n",
    "  - This simulates a realistic setting where the LLM replaces a single annotator and is judged by agreement with the rest\n",
    "\n",
    "#### Key Metrics in Output\n",
    "\n",
    "- **`winning_rate_train`**: Proportion of annotators for which the LLM performs as well or better (after adjusting for Œµ)\n",
    "- **`passed_alt_test_train`**: `True` if the LLM passes the test (i.e., `winning_rate ‚â• 0.5`)\n",
    "- **`avg_adv_prob_train`**: Average advantage probability, how likely the model is better across comparisons\n",
    "- **`p_values_train`**: List of p-values for each comparison\n",
    "\n",
    "#### Interpreting `Œµ` (Epsilon)\n",
    "\n",
    "- `Œµ` accounts for the **cost/effort/time trade-off** between using an LLM and a human annotator.\n",
    "- Higher `Œµ` gives the model more leeway, useful when **human annotations are costly**.\n",
    "- Recommendations from the original paper:\n",
    "  - `Œµ = 0.2` ‚Üí when humans are **experts**\n",
    "  - `Œµ = 0.1` ‚Üí when humans are **crowdworkers**\n",
    "\n",
    "> If `winning_rate ‚â• 0.5`, the LLM is considered **statistically competitive with human annotators** for this dataset and scenario (the LLM is \"better\" than half the humans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Columns in detailed_results_df (in run_alt_test_on_results) ===\n",
      "['sample_id', 'split', 'verbatim', 'iteration', 'Rater_Oli', 'Rater_Gaia', 'Rater_Chloe', 'ModelPrediction', 'Reasoning', 'run', 'prompt_name', 'use_validation_set']\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 75 valid labels\n",
      "  Rater_Oli: 75 valid labels\n",
      "  Rater_Gaia: 75 valid labels\n",
      "  Rater_Chloe: 75 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_Gaia: int64\n",
      "  Rater_Chloe: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_Gaia type after conversion: <class 'numpy.int32'>\n",
      "Rater_Chloe type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.9677 => rejectH0=False | rho_f=0.680, rho_h=0.987\n",
      "Rater_Gaia: p=0.9677 => rejectH0=False | rho_f=0.680, rho_h=0.987\n",
      "Rater_Chloe: p=0.9677 => rejectH0=False | rho_f=0.680, rho_h=0.987\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.000\n",
      "Average Advantage Probability (rho) = 0.680\n",
      "Passed Alt-Test? => False\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 75 valid labels\n",
      "  Rater_Oli: 75 valid labels\n",
      "  Rater_Gaia: 75 valid labels\n",
      "  Rater_Chloe: 75 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_Gaia: int64\n",
      "  Rater_Chloe: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_Gaia type after conversion: <class 'numpy.int32'>\n",
      "Rater_Chloe type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.9494 => rejectH0=False | rho_f=0.693, rho_h=0.987\n",
      "Rater_Gaia: p=0.9494 => rejectH0=False | rho_f=0.693, rho_h=0.987\n",
      "Rater_Chloe: p=0.9494 => rejectH0=False | rho_f=0.693, rho_h=0.987\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.000\n",
      "Average Advantage Probability (rho) = 0.693\n",
      "Passed Alt-Test? => False\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 75 valid labels\n",
      "  Rater_Oli: 75 valid labels\n",
      "  Rater_Gaia: 75 valid labels\n",
      "  Rater_Chloe: 75 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_Gaia: int64\n",
      "  Rater_Chloe: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_Gaia type after conversion: <class 'numpy.int32'>\n",
      "Rater_Chloe type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.9677 => rejectH0=False | rho_f=0.680, rho_h=0.987\n",
      "Rater_Gaia: p=0.9677 => rejectH0=False | rho_f=0.680, rho_h=0.987\n",
      "Rater_Chloe: p=0.9677 => rejectH0=False | rho_f=0.680, rho_h=0.987\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.000\n",
      "Average Advantage Probability (rho) = 0.680\n",
      "Passed Alt-Test? => False\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 75 valid labels\n",
      "  Rater_Oli: 75 valid labels\n",
      "  Rater_Gaia: 75 valid labels\n",
      "  Rater_Chloe: 75 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_Gaia: int64\n",
      "  Rater_Chloe: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_Gaia type after conversion: <class 'numpy.int32'>\n",
      "Rater_Chloe type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.9799 => rejectH0=False | rho_f=0.667, rho_h=0.987\n",
      "Rater_Gaia: p=0.9799 => rejectH0=False | rho_f=0.667, rho_h=0.987\n",
      "Rater_Chloe: p=0.9799 => rejectH0=False | rho_f=0.667, rho_h=0.987\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.000\n",
      "Average Advantage Probability (rho) = 0.667\n",
      "Passed Alt-Test? => False\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 75 valid labels\n",
      "  Rater_Oli: 75 valid labels\n",
      "  Rater_Gaia: 75 valid labels\n",
      "  Rater_Chloe: 75 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_Gaia: int64\n",
      "  Rater_Chloe: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_Gaia type after conversion: <class 'numpy.int32'>\n",
      "Rater_Chloe type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.9677 => rejectH0=False | rho_f=0.680, rho_h=0.987\n",
      "Rater_Gaia: p=0.9677 => rejectH0=False | rho_f=0.680, rho_h=0.987\n",
      "Rater_Chloe: p=0.9677 => rejectH0=False | rho_f=0.680, rho_h=0.987\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.000\n",
      "Average Advantage Probability (rho) = 0.680\n",
      "Passed Alt-Test? => False\n",
      "=== ALT Test: Label Debugging ===\n",
      "Label counts for each rater:\n",
      "  ModelPrediction: 75 valid labels\n",
      "  Rater_Oli: 75 valid labels\n",
      "  Rater_Gaia: 75 valid labels\n",
      "  Rater_Chloe: 75 valid labels\n",
      "\n",
      "Label types for each rater:\n",
      "  ModelPrediction: int64\n",
      "  Rater_Oli: int64\n",
      "  Rater_Gaia: int64\n",
      "  Rater_Chloe: int64\n",
      "\n",
      "Mixed types across raters: False\n",
      "========================================\n",
      "\n",
      "=== Converting labels to consistent types ===\n",
      "Using label_type: int\n",
      "Model predictions type after conversion: <class 'numpy.int32'>\n",
      "Rater_Oli type after conversion: <class 'numpy.int32'>\n",
      "Rater_Gaia type after conversion: <class 'numpy.int32'>\n",
      "Rater_Chloe type after conversion: <class 'numpy.int32'>\n",
      "=== Alt-Test: summary ===\n",
      "P-values for each comparison:\n",
      "Rater_Oli: p=0.9677 => rejectH0=False | rho_f=0.680, rho_h=0.987\n",
      "Rater_Gaia: p=0.9677 => rejectH0=False | rho_f=0.680, rho_h=0.987\n",
      "Rater_Chloe: p=0.9677 => rejectH0=False | rho_f=0.680, rho_h=0.987\n",
      "\n",
      "Summary statistics:\n",
      "Winning Rate (omega) = 0.000\n",
      "Average Advantage Probability (rho) = 0.680\n",
      "Passed Alt-Test? => False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>N_train</th>\n",
       "      <th>winning_rate_train</th>\n",
       "      <th>passed_alt_test_train</th>\n",
       "      <th>avg_adv_prob_train</th>\n",
       "      <th>p_values_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>few_shot</td>\n",
       "      <td>225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.684444</td>\n",
       "      <td>[0.9615964156138078, 0.9615964156138078, 0.9615964156138078]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>zero_shot</td>\n",
       "      <td>225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.675556</td>\n",
       "      <td>[0.971760465805921, 0.971760465805921, 0.971760465805921]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_name  N_train  winning_rate_train  passed_alt_test_train  \\\n",
       "6    few_shot      225                 0.0                  False   \n",
       "7   zero_shot      225                 0.0                  False   \n",
       "\n",
       "   avg_adv_prob_train  \\\n",
       "6            0.684444   \n",
       "7            0.675556   \n",
       "\n",
       "                                                 p_values_train  \n",
       "6  [0.9615964156138078, 0.9615964156138078, 0.9615964156138078]  \n",
       "7     [0.971760465805921, 0.971760465805921, 0.971760465805921]  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run ALT test\n",
    "epsilon = 0.2  # Epsilon parameter for ALT test\n",
    "alt_test_df = run_alt_test_on_results(\n",
    "    detailed_results_df=complex_case_for_metrics,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    epsilon=epsilon,\n",
    "    alpha=0.05,\n",
    "    verbose=verbose\n",
    ")\n",
    "alt_test_df = alt_test_df.drop(\n",
    "    columns=[\"iteration\", \"run\", \"use_validation_set\", \"N_val\", \"n_runs\"]\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)   # show full content in each cell\n",
    "alt_test_df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step: Classify the Full Dataset\n",
    "\n",
    "If you are satisfied with the evaluation metrics, you can now use the **best-performing scenario** to classify the **entire unlabeled dataset**.\n",
    "\n",
    "Simply **copy the chosen scenario** and run the classification.\n",
    "\n",
    "> This time, only **one run is needed**, since you're not computing evaluation metrics (there are no human labels to compare against).\n",
    "\n",
    "If you're **not satisfied with the results**, feel free to continue exploring and testing **different scenarios**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = [\n",
    "    {\n",
    "        # LLM settings\n",
    "        \"provider_llm1\": \"azure\",\n",
    "        \"model_name_llm1\": \"gpt-4o\",\n",
    "        \"temperature_llm1\": 0,\n",
    "        \"prompt_name\": \"few_shot\",\n",
    "        \"subsample_size\": -1,  # Size of data subset to use\n",
    "\n",
    "        # Prompt configuration\n",
    "        \"template\": \"\"\"\n",
    "You are an assistant that evaluates data entries.\n",
    "\n",
    "The data has the following columns:\n",
    "- \"ID\": Unique identifiant of the participant\n",
    "- \"Text\": The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\\n\"\n",
    "- \"Identify\": Response for the IDENTIFY step\n",
    "- \"Guess\": Response for the GUESS step\n",
    "- \"Seek\": Response for the SEEK step\n",
    "- \"Assess\": Response for the ASSESS step\n",
    "\n",
    "Here is an entry to evaluate:\n",
    "{verbatim_text}\n",
    "\n",
    "If a numeric value is present in the mechanical_rating column, copy it as the correct label.\n",
    "If it‚Äôs empty, you‚Äôll decide an overall cycle validity (0 or 1) based on the following codebook:\n",
    "\n",
    "A cycle is considered valid if you can answer \"yes\" to all the following questions:\n",
    "\n",
    "- Identify Step: Does the Identify step indicate a topic of interest?\n",
    "- Guess Step: Does the Guess step suggest a possible explanation?\n",
    "- Seek Step: Is the Seek step formulated as a question?\n",
    "- Assess Step: Does it identify a possible answer or state that no answer where found (\"no\" is ok) ?\n",
    "- Consistency: Are the Identify, Guess, and Seek steps related to the same question?\n",
    "- Reference Link: Are the Identify, Guess, and Seek steps related to the topic of the reference text?\n",
    "- Seek Question Originality: Is the answer to the Seek question not found (even vaguely) in the reference text?\n",
    "- Resolving Answer: If the Assess step state an answer, does it answer to the question in the Seek step ?\n",
    "- Valid Answer: If the ASSESS step indicates an answer was found, is the answer indeed in the assess_cues? ‚Üí If not, then no answer was actually found, and the cycle is not valid.\n",
    "- Valid No: If the ASSESS step indicates no answer was found, confirm that the answer to the SEEK question is not actually present in the assess_cues. ‚Üí If the participant claims no answer was found, but it is in fact in assess_cues, the cycle is not valid.\n",
    "\n",
    "Identify_validity, Guess_validity, Seek_validity, Assess_validity:\n",
    "If one of those column already shows a numeric value (whatever the value), accept the step for this question without re-checking that step‚Äôs validity.\n",
    "\n",
    "If all these criteria are met, the cycle is valid.\n",
    "Validity is expressed as:\n",
    "1: Valid cycle\n",
    "0: Invalid cycle\n",
    "\n",
    "Minor spelling, grammatical, or phrasing errors should not be penalized as long as the intent of the entry is clear and aligns with the inclusion criteria. Focus on the content and purpose of the entry rather than linguistic perfection.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Example 1\n",
    "Key:\n",
    "AA25I4\n",
    "\n",
    "Reference:\n",
    "\"Rain forms when water evaporates into the atmosphere, condenses into droplets, and falls due to gravity.\"\n",
    "\n",
    "Cycle Steps:\n",
    "IDENTIFY: \"I don‚Äôt understand how rain forms.\"\n",
    "GUESS: \"Maybe rain condenses in the sky, forming droplets.\"\n",
    "SEEK: \"How does rain form?\"\n",
    "ASSESS: \"No\"\n",
    "Assess Cues:\n",
    "\n",
    "Validity Columns:\n",
    "Identify_validity: NA\n",
    "Guess_validity: 2\n",
    "Seek_validity: NA\n",
    "Assess_validity: NA\n",
    "Mechanical_rating: NA\n",
    "\n",
    "Reasoning\n",
    "Since the mechanical_rating column is empty, the validity must be determined using the codebook.\n",
    "\n",
    "Reasoning:\n",
    "Identify step: Does the Identify step indicate a topic of interest?\n",
    "Yes: The topic is the formation of rain.\n",
    "\n",
    "Guess step: A numeric value is present in the Guess_validity column, so no further validation is needed.\n",
    "Yes: It proposes condensation as the mechanism for rain formation.\n",
    "\n",
    "Seek step: Is the Seek step formulated as a question?\n",
    "Yes: It is explicitly phrased as a question with an interrogative structure.\n",
    "\n",
    "Assess step: Does it identify a possible answer or state that no answer was found (\"No\" is acceptable)?\n",
    "Yes: It states that the answer to the question was not found, which is a valid response in the Assess step.\n",
    "\n",
    "Consistency: Are the Identify, Guess, and Seek steps related to the same topic?\n",
    "Yes: They all pertain to the process of rain formation.\n",
    "\n",
    "Reference Link: Are the Identify, Guess, and Seek steps related to the reference text?\n",
    "Yes: The text discusses rain and explains its formation.\n",
    "\n",
    "Seek Question Originality: Is the answer to the Seek question absent (even vaguely) from the reference text?\n",
    "No: The answer is explicitly provided in the reference text.\n",
    "\n",
    "Resolving Answer:\n",
    "Not applicable (the answer was not found).\n",
    "\n",
    "Valid Answer:\n",
    "Not applicable (the answer was not found).\n",
    "\n",
    "Valid No: Is the answer to the SEEK question absent from the assess_cues?\n",
    "Yes: The answer to the SEEK question is not in assess_cues, so the \"No\" is valid.\n",
    "\n",
    "Conclusion\n",
    "The cycle is not valid because the answer to the SEEK question is explicitly present in the reference text.\n",
    "\n",
    "Validity:\n",
    "0\n",
    "\"\"\",\n",
    "        # Output\n",
    "        \"selected_fields\": [\"Classification\", \"Reasoning\"],\n",
    "        \"prefix\": \"Classification\",\n",
    "        \"label_type\": \"int\",\n",
    "        \"response_template\":\n",
    "        \"\"\"\n",
    "Please follow the JSON format below:\n",
    "```json\n",
    "{{\n",
    "  \"Reasoning\": \"Your text here\",\n",
    "  \"Classification\": \"Your integer here\"\n",
    "}}\n",
    "\"\"\",\n",
    "        \"json_output\": True,\n",
    "\n",
    "        # Prompt optimization\n",
    "        \"provider_llm2\": \"azure\",\n",
    "        \"model_name_llm2\": \"gpt-4o\",\n",
    "        \"temperature_llm2\": 0.7,\n",
    "        \"max_iterations\": 1,\n",
    "        \"use_validation_set\": False,\n",
    "        \"validation_size\": 10,\n",
    "        \"random_state\": 42,\n",
    "\n",
    "        # Majority vote\n",
    "        \"n_completions\": 1,\n",
    "\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run the scenario\n",
    "complex_case_fully_annotated = run_scenarios(\n",
    "    scenarios=scenario,\n",
    "    data=data,\n",
    "    annotation_columns=annotation_columns,\n",
    "    labels=labels,\n",
    "    n_runs=n_runs,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_case_fully_annotated.to_csv(\"data/outputs/complex_case_fully_annotated.csv\", sep=\";\", index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
