{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Import functions from qualitative_analysis\n",
    "from qualitative_analysis.data_processing import load_data, clean_and_normalize, sanitize_dataframe\n",
    "from qualitative_analysis.model_interaction import get_llm_client\n",
    "from qualitative_analysis.evaluation import compute_cohens_kappa\n",
    "from qualitative_analysis.utils import save_results_to_csv, load_results_from_csv\n",
    "import qualitative_analysis.config as config\n",
    "from qualitative_analysis.prompt_construction import construct_prompt\n",
    "from qualitative_analysis.prompt_construction import build_data_format_description\n",
    "from qualitative_analysis.response_parsing import extract_code_from_response\n",
    "from qualitative_analysis.cost_estimation import openai_api_calculate_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = 'data'\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ref</th>\n",
       "      <th>txt1.ctrl1</th>\n",
       "      <th>txt1.det</th>\n",
       "      <th>txt1.exp</th>\n",
       "      <th>txt1.ctrl2</th>\n",
       "      <th>corr_cycle1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BC23</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>Le manteau,le centre de la Terre</td>\n",
       "      <td>Je pense que le manteau et le centre de la Ter...</td>\n",
       "      <td>Le manteau et le centre de la Terre ne font il...</td>\n",
       "      <td>Je n'ai pas pu trouver la réponse à  ma question.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BC14</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>Température du magma</td>\n",
       "      <td>La température du magma s'élève à  plus de 100...</td>\n",
       "      <td>à combien de  °cle magma est?</td>\n",
       "      <td>La température du magma atteint les 1000 °C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BC5</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>Pourqoi on l'appelle le manteau.</td>\n",
       "      <td>le magma et une pierre.</td>\n",
       "      <td>quel et la temperature magma</td>\n",
       "      <td>1 000c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BC22</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>La température du magma</td>\n",
       "      <td>La température du magma dépasse les 500 °C</td>\n",
       "      <td>Quel est la température du magma ?</td>\n",
       "      <td>La température du magma atteint les 1000 °C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BC20</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>Combien de couches y-t-il ?</td>\n",
       "      <td>Il existe d'autre couches dans la terre à  par...</td>\n",
       "      <td>Quelles sont les autres couches et combien son...</td>\n",
       "      <td>La terre contient 7 couches  dont le noyau.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                                                ref  \\\n",
       "0  BC23  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "1  BC14  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "2   BC5  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "3  BC22  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "4  BC20  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "\n",
       "                         txt1.ctrl1  \\\n",
       "0  Le manteau,le centre de la Terre   \n",
       "1              Température du magma   \n",
       "2  Pourqoi on l'appelle le manteau.   \n",
       "3           La température du magma   \n",
       "4       Combien de couches y-t-il ?   \n",
       "\n",
       "                                            txt1.det  \\\n",
       "0  Je pense que le manteau et le centre de la Ter...   \n",
       "1  La température du magma s'élève à  plus de 100...   \n",
       "2                            le magma et une pierre.   \n",
       "3         La température du magma dépasse les 500 °C   \n",
       "4  Il existe d'autre couches dans la terre à  par...   \n",
       "\n",
       "                                            txt1.exp  \\\n",
       "0  Le manteau et le centre de la Terre ne font il...   \n",
       "1                      à combien de  °cle magma est?   \n",
       "2                       quel et la temperature magma   \n",
       "3                 Quel est la température du magma ?   \n",
       "4  Quelles sont les autres couches et combien son...   \n",
       "\n",
       "                                          txt1.ctrl2  corr_cycle1  \n",
       "0  Je n'ai pas pu trouver la réponse à  ma question.            1  \n",
       "1        La température du magma atteint les 1000 °C            1  \n",
       "2                                             1 000c            0  \n",
       "3        La température du magma atteint les 1000 °C            1  \n",
       "4        La terre contient 7 couches  dont le noyau.            1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to your dataset\n",
    "data_file_path = os.path.join(data_dir, 'binary_sample.csv')\n",
    "\n",
    "# Load the data\n",
    "data = load_data(data_file_path, file_type='csv', delimiter=';')\n",
    "\n",
    "# Preview the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ocler\\Documents\\Académique\\Inria\\qualitative_analysis_project\\qualitative_analysis\\data_processing.py:139: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df.applymap(\n"
     ]
    }
   ],
   "source": [
    "# 1) Define a mapping from old column names to new names\n",
    "rename_map = {\n",
    "    \"ref\": \"reference\",\n",
    "    \"txt1.ctrl1\": \"Identify\",\n",
    "    \"txt1.det\": \"Guess\",\n",
    "    \"txt1.exp\": \"Seek\",\n",
    "    \"txt1.ctrl2\": \"Assess\"\n",
    "}\n",
    "\n",
    "# 2) Rename the columns in the DataFrame\n",
    "data = data.rename(columns=rename_map)\n",
    "\n",
    "# 3) Now define the new column names for cleaning\n",
    "text_columns = [\"reference\", \"Identify\", \"Guess\", \"Seek\", \"Assess\"]\n",
    "\n",
    "# 4) Clean and normalize the new columns\n",
    "for col in text_columns:\n",
    "    data[col] = clean_and_normalize(data[col])\n",
    "\n",
    "# 5) Sanitize the DataFrame\n",
    "data = sanitize_dataframe(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of verbatims: 45\n",
      "Verbatim example:\n",
      "Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: Le manteau,le centre de la Terre\n",
      "Guess: Je pense que le manteau et le centre de la Terre sont les màames choses\n",
      "Seek: Le manteau et le centre de la Terre ne font ils qu'un?\n",
      "Assess: Je n'ai pas pu trouver la réponse à  ma question.\n"
     ]
    }
   ],
   "source": [
    "# Combine texts and entries\n",
    "\n",
    "data['verbatim'] = data.apply(\n",
    "    lambda row: (\n",
    "        f\"Reference: {row['reference']}\\n\\n\"\n",
    "        f\"Identify: {row['Identify']}\\n\"\n",
    "        f\"Guess: {row['Guess']}\\n\"\n",
    "        f\"Seek: {row['Seek']}\\n\"\n",
    "        f\"Assess: {row['Assess']}\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Extract the list of verbatims\n",
    "verbatims = data['verbatim'].tolist()\n",
    "\n",
    "print(f\"Total number of verbatims: {len(verbatims)}\")\n",
    "print(f\"Verbatim example:\\n{verbatims[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the provider and model\n",
    "provider = 'azure'\n",
    "model_name = 'gpt-4o-mini'\n",
    "\n",
    "# Initialize the client\n",
    "llm_client = get_llm_client(provider=provider, config=config.MODEL_CONFIG[provider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing Verbatim 1/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 846 (prompt) + 37 (completion) = 883 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 848 (prompt) + 40 (completion) = 888 total\n",
      "Cost for Theme 'Guess Step': $0.0002\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 839 (prompt) + 31 (completion) = 870 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 854 (prompt) + 26 (completion) = 880 total\n",
      "Cost for Theme 'Assess Step': $0.0001\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 867 (prompt) + 50 (completion) = 917 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 828 (prompt) + 2 (completion) = 830 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 867 (prompt) + 37 (completion) = 904 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 2/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 839 (prompt) + 34 (completion) = 873 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 834 (prompt) + 30 (completion) = 864 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 833 (prompt) + 29 (completion) = 862 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 858 (prompt) + 34 (completion) = 892 total\n",
      "Cost for Theme 'Assess Step': $0.0001\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 840 (prompt) + 27 (completion) = 867 total\n",
      "Cost for Theme 'Consistency': $0.0001\n",
      "Extracted Score for 'Consistency': 1\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 824 (prompt) + 2 (completion) = 826 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 880 (prompt) + 55 (completion) = 935 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 3/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 794 (prompt) + 47 (completion) = 841 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 0\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 788 (prompt) + 42 (completion) = 830 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 771 (prompt) + 25 (completion) = 796 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 819 (prompt) + 53 (completion) = 872 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 794 (prompt) + 39 (completion) = 833 total\n",
      "Cost for Theme 'Consistency': $0.0001\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 806 (prompt) + 42 (completion) = 848 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 810 (prompt) + 42 (completion) = 852 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0001\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 4/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 832 (prompt) + 33 (completion) = 865 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 828 (prompt) + 30 (completion) = 858 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 828 (prompt) + 31 (completion) = 859 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 848 (prompt) + 30 (completion) = 878 total\n",
      "Cost for Theme 'Assess Step': $0.0001\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 810 (prompt) + 2 (completion) = 812 total\n",
      "Cost for Theme 'Consistency': $0.0001\n",
      "Extracted Score for 'Consistency': 1\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 818 (prompt) + 2 (completion) = 820 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 862 (prompt) + 42 (completion) = 904 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 5/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 786 (prompt) + 2 (completion) = 788 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 822 (prompt) + 40 (completion) = 862 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 809 (prompt) + 28 (completion) = 837 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 848 (prompt) + 46 (completion) = 894 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 837 (prompt) + 45 (completion) = 882 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 847 (prompt) + 48 (completion) = 895 total\n",
      "Cost for Theme 'Reference Link': $0.0002\n",
      "Extracted Score for 'Reference Link': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 838 (prompt) + 34 (completion) = 872 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0001\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 6/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 821 (prompt) + 42 (completion) = 863 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 0\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 824 (prompt) + 46 (completion) = 870 total\n",
      "Cost for Theme 'Guess Step': $0.0002\n",
      "Extracted Score for 'Guess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 801 (prompt) + 24 (completion) = 825 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 858 (prompt) + 60 (completion) = 918 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 844 (prompt) + 56 (completion) = 900 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 830 (prompt) + 35 (completion) = 865 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 840 (prompt) + 40 (completion) = 880 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0001\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 7/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 803 (prompt) + 34 (completion) = 837 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 811 (prompt) + 43 (completion) = 854 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 794 (prompt) + 26 (completion) = 820 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 825 (prompt) + 38 (completion) = 863 total\n",
      "Cost for Theme 'Assess Step': $0.0001\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 837 (prompt) + 60 (completion) = 897 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 788 (prompt) + 2 (completion) = 790 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 833 (prompt) + 44 (completion) = 877 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 8/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 841 (prompt) + 32 (completion) = 873 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 845 (prompt) + 37 (completion) = 882 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 835 (prompt) + 27 (completion) = 862 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 871 (prompt) + 43 (completion) = 914 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 880 (prompt) + 63 (completion) = 943 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 828 (prompt) + 2 (completion) = 830 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 874 (prompt) + 45 (completion) = 919 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 9/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 834 (prompt) + 43 (completion) = 877 total\n",
      "Cost for Theme 'Identify Step': $0.0002\n",
      "Extracted Score for 'Identify Step': 0\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 828 (prompt) + 39 (completion) = 867 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 824 (prompt) + 34 (completion) = 858 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 0\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 846 (prompt) + 36 (completion) = 882 total\n",
      "Cost for Theme 'Assess Step': $0.0001\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 835 (prompt) + 36 (completion) = 871 total\n",
      "Cost for Theme 'Consistency': $0.0001\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 858 (prompt) + 50 (completion) = 908 total\n",
      "Cost for Theme 'Reference Link': $0.0002\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 862 (prompt) + 50 (completion) = 912 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 10/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 778 (prompt) + 2 (completion) = 780 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 804 (prompt) + 30 (completion) = 834 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 800 (prompt) + 26 (completion) = 826 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 828 (prompt) + 34 (completion) = 862 total\n",
      "Cost for Theme 'Assess Step': $0.0001\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 818 (prompt) + 35 (completion) = 853 total\n",
      "Cost for Theme 'Consistency': $0.0001\n",
      "Extracted Score for 'Consistency': 1\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 794 (prompt) + 2 (completion) = 796 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 829 (prompt) + 34 (completion) = 863 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0001\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 11/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 815 (prompt) + 32 (completion) = 847 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 814 (prompt) + 32 (completion) = 846 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 804 (prompt) + 23 (completion) = 827 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 839 (prompt) + 37 (completion) = 876 total\n",
      "Cost for Theme 'Assess Step': $0.0001\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 837 (prompt) + 45 (completion) = 882 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 802 (prompt) + 2 (completion) = 804 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 846 (prompt) + 42 (completion) = 888 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 12/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 805 (prompt) + 42 (completion) = 847 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 0\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 816 (prompt) + 54 (completion) = 870 total\n",
      "Cost for Theme 'Guess Step': $0.0002\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 790 (prompt) + 28 (completion) = 818 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 833 (prompt) + 51 (completion) = 884 total\n",
      "Cost for Theme 'Assess Step': $0.0002\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 826 (prompt) + 55 (completion) = 881 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 819 (prompt) + 40 (completion) = 859 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 826 (prompt) + 42 (completion) = 868 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0001\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 13/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 762 (prompt) + 31 (completion) = 793 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 0\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 772 (prompt) + 42 (completion) = 814 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 755 (prompt) + 25 (completion) = 780 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 0\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 786 (prompt) + 36 (completion) = 822 total\n",
      "Cost for Theme 'Assess Step': $0.0001\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 771 (prompt) + 32 (completion) = 803 total\n",
      "Cost for Theme 'Consistency': $0.0001\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 789 (prompt) + 42 (completion) = 831 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 804 (prompt) + 52 (completion) = 856 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 14/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 811 (prompt) + 32 (completion) = 843 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 815 (prompt) + 37 (completion) = 852 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 804 (prompt) + 26 (completion) = 830 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 833 (prompt) + 35 (completion) = 868 total\n",
      "Cost for Theme 'Assess Step': $0.0001\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 847 (prompt) + 59 (completion) = 906 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 798 (prompt) + 2 (completion) = 800 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 860 (prompt) + 60 (completion) = 920 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0002\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 15/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 752 (prompt) + 2 (completion) = 754 total\n",
      "Cost for Theme 'Identify Step': $0.0001\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 785 (prompt) + 37 (completion) = 822 total\n",
      "Cost for Theme 'Guess Step': $0.0001\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 783 (prompt) + 35 (completion) = 818 total\n",
      "Cost for Theme 'Seek Step': $0.0001\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 807 (prompt) + 39 (completion) = 846 total\n",
      "Cost for Theme 'Assess Step': $0.0001\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 822 (prompt) + 64 (completion) = 886 total\n",
      "Cost for Theme 'Consistency': $0.0002\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 797 (prompt) + 32 (completion) = 829 total\n",
      "Cost for Theme 'Reference Link': $0.0001\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 811 (prompt) + 41 (completion) = 852 total\n",
      "Cost for Theme 'Seek Question Originality': $0.0001\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "\n",
      "=== Processing Complete ===\n",
      "Total Tokens Used: 89982\n",
      "Total Cost for Processing: $0.0151\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: Le manteau,le centre de la Terre\n",
      "Guess: Je pense que le manteau et le centre de la Terre sont les màames choses\n",
      "Seek: Le manteau et le centre de la Terre ne font ils qu'un?\n",
      "Assess: Je n'ai pas pu trouver la réponse à  ma question., Tokens Used: 6172, Cost: $0.0010\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: Température du magma\n",
      "Guess: La température du magma s'élève à  plus de 1000 °c\n",
      "Seek: à combien de  °cle magma est?\n",
      "Assess: La température du magma atteint les 1000 °C, Tokens Used: 6119, Cost: $0.0010\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: Pourqoi on l'appelle le manteau.\n",
      "Guess: le magma et une pierre.\n",
      "Seek: quel et la temperature magma\n",
      "Assess: 1 000c, Tokens Used: 5872, Cost: $0.0010\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: La température du magma\n",
      "Guess: La température du magma dépasse les 500 °C\n",
      "Seek: Quel est la température du magma ?\n",
      "Assess: La température du magma atteint les 1000 °C, Tokens Used: 5996, Cost: $0.0010\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: Combien de couches y-t-il ?\n",
      "Guess: Il existe d'autre couches dans la terre à  part le manteau .\n",
      "Seek: Quelles sont les autres couches et combien sont-elles ?\n",
      "Assess: La terre contient 7 couches  dont le noyau., Tokens Used: 6030, Cost: $0.0010\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: Pourquoi ont apelle le liquide le magma.\n",
      "Guess: Moi je crois que le magma est t' une ville d'Afrique.\n",
      "Seek: Quelle est la température du m agma.\n",
      "Assess: Je n'et pas trouvé., Tokens Used: 6121, Cost: $0.0011\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: Les Couches de terre\n",
      "Guess: Il existe d 'autres couches dans le manteau .\n",
      "Seek: Quelle sont les autres couches de la terre\n",
      "Assess: La terre contient 7 chouches dont le noyau., Tokens Used: 5938, Cost: $0.0010\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: Le magma est le responsable de l'explosion car c'est le coeur du volcan.\n",
      "Guess: IL existe d'autres couches dans la terre à  par le manteau.\n",
      "Seek: Que se passe-t-il pendant l'éruption du volcan\n",
      "Assess: IL y a d'autres couches dans la terre dont le noyau, Tokens Used: 6223, Cost: $0.0010\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: Si le<<manteau>>existé pas qu'est qui ce passeré\n",
      "Guess: Le manteau est chaud ou froid?\n",
      "Seek: Si on y va on peut mourir\n",
      "Assess: Je n'est pas la raiponce de la question!!!!!!!!!!!!!!!!!!!?!!!!!, Tokens Used: 6175, Cost: $0.0011\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: température du magma\n",
      "Guess: la temperature du magma depasse les 500 C\n",
      "Seek: quelle est la temperature du magma\n",
      "Assess: La température du magma atteint led 1000 degres, Tokens Used: 5814, Cost: $0.0009\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: le magma\n",
      "Guess: La température du magma dépasse les 500c\n",
      "Seek: Quelle est la température du magma?\n",
      "Assess: La température du magma atteint les 1000c, Tokens Used: 5970, Cost: $0.0010\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: Les roches sont liquide\n",
      "Guess: Durant l'éruption le volcan fait sortir du magma\n",
      "Seek: Pourquoi le liquide s' appelle le magma\n",
      "Assess: Combien de couche contient le noyau, Tokens Used: 6027, Cost: $0.0010\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: j'ai pas trouver\n",
      "Guess: ...\n",
      "Seek: J'AI AUCUNNE IDEà\n",
      "Assess: ..., Tokens Used: 5699, Cost: $0.0010\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: comment le magma explosent si c'est une substance liquide?\n",
      "Guess: La température du magma dépasse les 500 \n",
      "Seek: comment le magma explosent ?\n",
      "Assess: je n'est pas trouver, Tokens Used: 6019, Cost: $0.0010\n",
      "Verbatim: Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: C'est quoi le magma\n",
      "Guess: Je pense que le magma est de la lave\n",
      "Seek: Pourquoi appel-ton le magma\n",
      "Assess: Je n'ai pas trouvé, Tokens Used: 5807, Cost: $0.0010\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Le manteau,le centre de la Terre\\nGuess: Je pense que le manteau et le centre de la Terre sont les màames choses\\nSeek: Le manteau et le centre de la Terre ne font ils qu'un?\\nAssess: Je n'ai pas pu trouver la réponse à  ma question.\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Température du magma\\nGuess: La température du magma s'élève à  plus de 1000 °c\\nSeek: à combien de  °cle magma est?\\nAssess: La température du magma atteint les 1000 °C\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Pourqoi on l'appelle le manteau.\\nGuess: le magma et une pierre.\\nSeek: quel et la temperature magma\\nAssess: 1 000c\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: La température du magma\\nGuess: La température du magma dépasse les 500 °C\\nSeek: Quel est la température du magma ?\\nAssess: La température du magma atteint les 1000 °C\", 'Overall_Validity': 1}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Combien de couches y-t-il ?\\nGuess: Il existe d'autre couches dans la terre à  part le manteau .\\nSeek: Quelles sont les autres couches et combien sont-elles ?\\nAssess: La terre contient 7 couches  dont le noyau.\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Pourquoi ont apelle le liquide le magma.\\nGuess: Moi je crois que le magma est t' une ville d'Afrique.\\nSeek: Quelle est la température du m agma.\\nAssess: Je n'et pas trouvé.\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Les Couches de terre\\nGuess: Il existe d 'autres couches dans le manteau .\\nSeek: Quelle sont les autres couches de la terre\\nAssess: La terre contient 7 chouches dont le noyau.\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Le magma est le responsable de l'explosion car c'est le coeur du volcan.\\nGuess: IL existe d'autres couches dans la terre à  par le manteau.\\nSeek: Que se passe-t-il pendant l'éruption du volcan\\nAssess: IL y a d'autres couches dans la terre dont le noyau\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Si le<<manteau>>existé pas qu'est qui ce passeré\\nGuess: Le manteau est chaud ou froid?\\nSeek: Si on y va on peut mourir\\nAssess: Je n'est pas la raiponce de la question!!!!!!!!!!!!!!!!!!!?!!!!!\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: température du magma\\nGuess: la temperature du magma depasse les 500 C\\nSeek: quelle est la temperature du magma\\nAssess: La température du magma atteint led 1000 degres\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: le magma\\nGuess: La température du magma dépasse les 500c\\nSeek: Quelle est la température du magma?\\nAssess: La température du magma atteint les 1000c\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Les roches sont liquide\\nGuess: Durant l'éruption le volcan fait sortir du magma\\nSeek: Pourquoi le liquide s' appelle le magma\\nAssess: Combien de couche contient le noyau\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: j'ai pas trouver\\nGuess: ...\\nSeek: J'AI AUCUNNE IDEà\\x89\\nAssess: ...\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: comment le magma explosent si c'est une substance liquide?\\nGuess: La température du magma dépasse les 500 \\x84\\x83\\nSeek: comment le magma explosent ?\\nAssess: je n'est pas trouver\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: C'est quoi le magma\\nGuess: Je pense que le magma est de la lave\\nSeek: Pourquoi appel-ton le magma\\nAssess: Je n'ai pas trouvé\", 'Overall_Validity': 0}\n"
     ]
    }
   ],
   "source": [
    "# Define column descriptions\n",
    "column_descriptions = {\n",
    "    \"ID\": \"Unique identifier for each entry\",\n",
    "    \"reference\": \"The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\",\n",
    "    \"Identify\": \"Response for the IDENTIFY step\",\n",
    "    \"Guess\": \"Response for the GUESS step\",\n",
    "    \"Seek\": \"Response for the SEEK step\",\n",
    "    \"Assess\": \"Response for the ASSESS step\"\n",
    "}\n",
    "\n",
    "# Define criteria as themes\n",
    "codebook = {\n",
    "    \"Identify Step\": \"Does the Identify step indicate a topic of interest?\",\n",
    "    \"Guess Step\": \"Does the Guess step suggest a possible explanation?\",\n",
    "    \"Seek Step\": \"Is the Seek step formulated as a question?\",\n",
    "    \"Assess Step\": \"Does it identify a possible answer or state that no answer was found ('no' is ok)?\",\n",
    "    \"Consistency\": \"Are the Identify, Guess, and Seek steps related to the same question?\",\n",
    "    \"Reference Link\": \"Are the Identify, Guess, and Seek steps related to the topic of the reference text?\",\n",
    "    \"Seek Question Originality\": \"Is the answer to the Seek question not found (even vaguely) in the reference text?\",\n",
    "}\n",
    "\n",
    "# Build data format description\n",
    "data_format_description = build_data_format_description(column_descriptions)\n",
    "\n",
    "# Add classification flags\n",
    "binary_classification = True  # Set to False for multi-class classification\n",
    "post_reasoning = True  # Set to False for single-step classification\n",
    "\n",
    "# Define queries to be used globally\n",
    "binary_query = \"Reply with '1' if the entry meets the criterion or '0' otherwise.\"\n",
    "multiclass_query = \"Reply with a number corresponding to the category.\"\n",
    "post_reasoning_query = \"First, generate a one-sentence reasoning about the classification.\"\n",
    "\n",
    "def construct_prompt(verbatim, theme=None, theme_description=None, binary_classification=True, codebook=None, post_reasoning=False):\n",
    "    \"\"\"\n",
    "    Constructs a prompt for evaluating a specific criterion (theme) for a given verbatim or for multi-class classification.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant tasked with evaluating the following entry.\n",
    "\n",
    "You are provided with data entries in the following format:\n",
    "\n",
    "{data_format_description}\n",
    "\"\"\"\n",
    "\n",
    "    if binary_classification:\n",
    "        assert theme and theme_description, \"Theme and its description must be provided for binary classification.\"\n",
    "        prompt += f\"\"\"\n",
    "**Entry:**\n",
    "{verbatim}\n",
    "\n",
    "**Criterion:**\n",
    "{theme} - {theme_description}\n",
    "{binary_query}\n",
    "\"\"\"\n",
    "    else:\n",
    "        assert codebook is not None, \"Codebook is required for multi-class classification.\"\n",
    "        prompt += \"\"\"\n",
    "The data consists of entries that need to be classified into one of the following categories:\n",
    "------------------------------\n",
    "\"\"\"\n",
    "        for theme_i, (theme, theme_description) in enumerate(codebook.items()):\n",
    "            prompt += f\"{theme_i} - {theme}: {theme_description}\\n\"\n",
    "        prompt += f\"\"\"------------------------------\n",
    "        **Entry Description:**\n",
    "        {verbatim}\n",
    "        {multiclass_query}\n",
    "        \"\"\"\n",
    "\n",
    "    if post_reasoning:\n",
    "        return prompt, post_reasoning_query\n",
    "\n",
    "    return prompt, None\n",
    "\n",
    "def generate_with_reasoning(llm_client, model_name, prompt, reasoning_query=None, temperature=0.0001, verbose=False):\n",
    "    \"\"\"\n",
    "    Handles both single-step and two-step LLM calls based on whether reasoning_query is provided.\n",
    "    Returns the response text and the usage object.\n",
    "    \"\"\"\n",
    "    # First call for reasoning (if applicable)\n",
    "    if reasoning_query:\n",
    "        reasoning_prompt = f\"{prompt}\\n\\n{reasoning_query}\"\n",
    "        response_text_1, usage_1 = llm_client.get_response(\n",
    "            prompt=reasoning_prompt,\n",
    "            model=model_name,\n",
    "            max_tokens=500,\n",
    "            temperature=temperature,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        # Second call for classification\n",
    "        classification_prompt = f\"{prompt}\\n\\nReasoning:\\n{response_text_1}\\n\\nProvide the final classification.\"\n",
    "        response_text_2, usage_2 = llm_client.get_response(\n",
    "            prompt=classification_prompt,\n",
    "            model=model_name,\n",
    "            max_tokens=500,\n",
    "            temperature=temperature,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        # Combine usage\n",
    "        usage_1.prompt_tokens += usage_2.prompt_tokens\n",
    "        usage_1.completion_tokens += usage_2.completion_tokens\n",
    "        usage_1.total_tokens += usage_2.total_tokens\n",
    "        return response_text_2, usage_1\n",
    "\n",
    "    # Single-step classification\n",
    "    response_text, usage = llm_client.get_response(\n",
    "        prompt=prompt,\n",
    "        model=model_name,\n",
    "        max_tokens=500,\n",
    "        temperature=temperature,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    return response_text, usage\n",
    "\n",
    "# Select subset of verbatims for testing\n",
    "verbatims_subset = verbatims[:15]\n",
    "\n",
    "# Initialize results, verbatim_costs, token counters and total_costs\n",
    "results = []\n",
    "verbatim_costs = []  # Track costs per verbatim\n",
    "total_tokens_used = 0\n",
    "total_cost = 0\n",
    "\n",
    "for idx, verbatim in enumerate(verbatims_subset):\n",
    "    print(f\"=== Processing Verbatim {idx + 1}/{len(verbatims_subset)} ===\")\n",
    "    verbatim_tokens_used = 0  # Track tokens for this verbatim\n",
    "    verbatim_cost = 0  # Track cost for this verbatim\n",
    "\n",
    "    if binary_classification:\n",
    "        for theme, theme_description in codebook.items():\n",
    "            print(f\"\\n--- Evaluating Theme: {theme} ---\")\n",
    "\n",
    "            # Build the prompt\n",
    "            prompt, reasoning_query_current = construct_prompt(\n",
    "                verbatim, \n",
    "                theme, \n",
    "                theme_description, \n",
    "                binary_classification=True, \n",
    "                post_reasoning=post_reasoning\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                response_content, usage = generate_with_reasoning(\n",
    "                    llm_client=llm_client,\n",
    "                    model_name=model_name,\n",
    "                    prompt=prompt,\n",
    "                    reasoning_query=reasoning_query_current,\n",
    "                    temperature=0.0001,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                # Track token usage\n",
    "                if usage:\n",
    "                    prompt_tokens = usage.prompt_tokens\n",
    "                    completion_tokens = usage.completion_tokens\n",
    "                    total_tokens = usage.total_tokens\n",
    "\n",
    "                    # Calculate the cost for this request\n",
    "                    cost = openai_api_calculate_cost(usage, model=model_name)\n",
    "                    total_tokens_used += total_tokens\n",
    "                    total_cost += cost\n",
    "                    verbatim_tokens_used += total_tokens\n",
    "                    verbatim_cost += cost\n",
    "\n",
    "                    # Print detailed token usage and cost\n",
    "                    print(f\"Tokens Used: {prompt_tokens} (prompt) + {completion_tokens} (completion) = {total_tokens} total\")\n",
    "                    print(f\"Cost for Theme '{theme}': ${cost:.4f}\")\n",
    "\n",
    "                # Parse response\n",
    "                score = extract_code_from_response(response_content)\n",
    "                if score in [0, 1]:\n",
    "                    results.append({\n",
    "                        'Verbatim': verbatim,\n",
    "                        'Theme': theme,\n",
    "                        'Score': score\n",
    "                    })\n",
    "                    print(f\"Extracted Score for '{theme}': {score}\")\n",
    "                else:\n",
    "                    print(f\"Failed to parse a valid score for '{theme}'\")\n",
    "                    results.append({\n",
    "                        'Verbatim': verbatim,\n",
    "                        'Theme': theme,\n",
    "                        'Score': None\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing Verbatim {idx + 1} for Theme '{theme}': {e}\")\n",
    "                results.append({\n",
    "                    'Verbatim': verbatim,\n",
    "                    'Theme': theme,\n",
    "                    'Score': None\n",
    "                })\n",
    "    else:\n",
    "        # Multi-class classification\n",
    "        prompt, reasoning_query_current = construct_prompt(\n",
    "            verbatim, \n",
    "            binary_classification=False, \n",
    "            codebook=codebook, \n",
    "            post_reasoning=post_reasoning\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response_content, usage = generate_with_reasoning(\n",
    "                llm_client=llm_client,\n",
    "                model_name=model_name,\n",
    "                prompt=prompt,\n",
    "                reasoning_query=reasoning_query_current,\n",
    "                temperature=0.0001,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Track token usage\n",
    "            if usage:\n",
    "                prompt_tokens = usage.prompt_tokens\n",
    "                completion_tokens = usage.completion_tokens\n",
    "                total_tokens = usage.total_tokens\n",
    "\n",
    "                # Calculate the cost for this request\n",
    "                cost = openai_api_calculate_cost(usage, model=model_name)\n",
    "                total_tokens_used += total_tokens\n",
    "                total_cost += cost\n",
    "                verbatim_tokens_used += total_tokens\n",
    "                verbatim_cost += cost\n",
    "\n",
    "                # Print detailed token usage and cost\n",
    "                print(f\"Tokens Used: {prompt_tokens} (prompt) + {completion_tokens} (completion) = {total_tokens} total\")\n",
    "                print(f\"Cost for Verbatim '{idx + 1}': ${cost:.4f}\")\n",
    "\n",
    "            # Parse response\n",
    "            score = extract_code_from_response(response_content)\n",
    "            results.append({\n",
    "                'Verbatim': verbatim,\n",
    "                'Label': score\n",
    "            })\n",
    "            print(f\"Extracted Multi-Class Label: {score}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Verbatim {idx + 1} for Multi-Class Classification: {e}\")\n",
    "            results.append({\n",
    "                'Verbatim': verbatim,\n",
    "                'Label': None\n",
    "            })\n",
    "\n",
    "    # Store verbatim-level cost\n",
    "    verbatim_costs.append({'Verbatim': verbatim, 'Tokens Used': verbatim_tokens_used, 'Cost': verbatim_cost})\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n=== Processing Complete ===\")\n",
    "print(f\"Total Tokens Used: {total_tokens_used}\")\n",
    "print(f\"Total Cost for Processing: ${total_cost:.4f}\")\n",
    "\n",
    "# # Detailed Token and Cost Breakdown\n",
    "# for cost_entry in verbatim_costs:\n",
    "#     print(f\"Verbatim: {cost_entry['Verbatim']}, Tokens Used: {cost_entry['Tokens Used']}, Cost: ${cost_entry['Cost']:.4f}\")\n",
    "\n",
    "# Organize scores by verbatim for binary classification\n",
    "if binary_classification:\n",
    "    verbatim_scores = defaultdict(dict)\n",
    "    for entry in results:\n",
    "        verbatim = entry['Verbatim']\n",
    "        theme = entry['Theme']\n",
    "        score = entry['Score']\n",
    "        verbatim_scores[verbatim][theme] = score\n",
    "\n",
    "    # Determine overall validity\n",
    "    final_results = []\n",
    "    for verbatim, scores in verbatim_scores.items():\n",
    "        overall_validity = 1  # Assume valid\n",
    "        for theme, score in scores.items():\n",
    "            if score != 1:\n",
    "                overall_validity = 0\n",
    "                break\n",
    "        final_results.append({\n",
    "            'Verbatim': verbatim,\n",
    "            'Overall_Validity': overall_validity\n",
    "        })\n",
    "\n",
    "    # Optionally, print the final results\n",
    "    for result in final_results:\n",
    "        print(result)\n",
    "else:\n",
    "    # Print multi-class classification results\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: data\\outputs\\experiment_gpt-4o-mini_20241218_192402.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# Define the save path\n",
    "outputs_dir = os.path.join(data_dir, 'outputs')\n",
    "os.makedirs(outputs_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "save_path = os.path.join(outputs_dir, f\"experiment_{model_name.replace('/', '_')}_{timestamp}.csv\")\n",
    "\n",
    "# Save results\n",
    "save_results_to_csv(\n",
    "    coding=results_df.to_dict('records'),\n",
    "    save_path=save_path,\n",
    "    fieldnames=['Verbatim', 'Overall_Validity'],\n",
    "    verbatims=None  # Verbatims are included in the results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results loaded from: data\\outputs\\experiment_gpt-4o-mini_20241218_192402.csv\n"
     ]
    }
   ],
   "source": [
    "# Load results\n",
    "loaded_results = load_results_from_csv(save_path)\n",
    "# The function returns (verbatims, coding)\n",
    "verbatims_loaded, coding_loaded = loaded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Score between human annotations and model: 0.12\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have human annotations in the data\n",
    "human_annotations = data['corr_cycle1'].tolist()  # Replace with actual column name\n",
    "model_coding = results_df['Overall_Validity'].tolist()\n",
    "\n",
    "human_annotations_short = human_annotations[:15]\n",
    "\n",
    "# Compute Cohen's Kappa\n",
    "kappa = compute_cohens_kappa(\n",
    "    human_annotations_short,\n",
    "    model_coding,\n",
    "    labels=[0, 1],\n",
    "    weights='linear'\n",
    ")\n",
    "\n",
    "print(f\"Cohen's Kappa Score between human annotations and model: {kappa:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(human_annotations_short, model_coding, labels=[0, 1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
