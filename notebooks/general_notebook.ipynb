{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Import functions from qualitative_analysis\n",
    "from qualitative_analysis.data_processing import load_data, clean_and_normalize, sanitize_dataframe\n",
    "from qualitative_analysis.model_interaction import get_llm_client\n",
    "from qualitative_analysis.evaluation import compute_cohens_kappa\n",
    "from qualitative_analysis.utils import save_results_to_csv, load_results_from_csv\n",
    "import qualitative_analysis.config as config\n",
    "from qualitative_analysis.prompt_construction import construct_prompt\n",
    "from qualitative_analysis.prompt_construction import build_data_format_description\n",
    "from qualitative_analysis.response_parsing import extract_code_from_response\n",
    "from qualitative_analysis.cost_estimation import openai_api_calculate_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = 'C:/Users/ocler/Documents/Académique/Inria/Kids_Reflect/Data/'\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ref</th>\n",
       "      <th>txt1.ctrl1</th>\n",
       "      <th>txt1.det</th>\n",
       "      <th>txt1.exp</th>\n",
       "      <th>txt1.ctrl2</th>\n",
       "      <th>corr_cycle1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BC23</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>Le manteau,le centre de la Terre</td>\n",
       "      <td>Je pense que le manteau et le centre de la Ter...</td>\n",
       "      <td>Le manteau et le centre de la Terre ne font il...</td>\n",
       "      <td>Je n'ai pas pu trouver la réponse à  ma question.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BC14</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>Température du magma</td>\n",
       "      <td>La température du magma s'élève à  plus de 100...</td>\n",
       "      <td>à combien de  °cle magma est?</td>\n",
       "      <td>La température du magma atteint les 1000 °C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BC5</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>Pourqoi on l'appelle le manteau.</td>\n",
       "      <td>le magma et une pierre.</td>\n",
       "      <td>quel et la temperature magma</td>\n",
       "      <td>1 000c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BC22</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>La température du magma</td>\n",
       "      <td>La température du magma dépasse les 500 °C</td>\n",
       "      <td>Quel est la température du magma ?</td>\n",
       "      <td>La température du magma atteint les 1000 °C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BC20</td>\n",
       "      <td>À des dizaines de kilomètres sous nos pieds, l...</td>\n",
       "      <td>Combien de couches y-t-il ?</td>\n",
       "      <td>Il existe d'autre couches dans la terre à  par...</td>\n",
       "      <td>Quelles sont les autres couches et combien son...</td>\n",
       "      <td>La terre contient 7 couches  dont le noyau.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                                                ref  \\\n",
       "0  BC23  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "1  BC14  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "2   BC5  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "3  BC22  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "4  BC20  À des dizaines de kilomètres sous nos pieds, l...   \n",
       "\n",
       "                         txt1.ctrl1  \\\n",
       "0  Le manteau,le centre de la Terre   \n",
       "1              Température du magma   \n",
       "2  Pourqoi on l'appelle le manteau.   \n",
       "3           La température du magma   \n",
       "4       Combien de couches y-t-il ?   \n",
       "\n",
       "                                            txt1.det  \\\n",
       "0  Je pense que le manteau et le centre de la Ter...   \n",
       "1  La température du magma s'élève à  plus de 100...   \n",
       "2                            le magma et une pierre.   \n",
       "3         La température du magma dépasse les 500 °C   \n",
       "4  Il existe d'autre couches dans la terre à  par...   \n",
       "\n",
       "                                            txt1.exp  \\\n",
       "0  Le manteau et le centre de la Terre ne font il...   \n",
       "1                      à combien de  °cle magma est?   \n",
       "2                       quel et la temperature magma   \n",
       "3                 Quel est la température du magma ?   \n",
       "4  Quelles sont les autres couches et combien son...   \n",
       "\n",
       "                                          txt1.ctrl2  corr_cycle1  \n",
       "0  Je n'ai pas pu trouver la réponse à  ma question.            1  \n",
       "1        La température du magma atteint les 1000 °C            1  \n",
       "2                                             1 000c            0  \n",
       "3        La température du magma atteint les 1000 °C            1  \n",
       "4        La terre contient 7 couches  dont le noyau.            1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to your dataset\n",
    "data_file_path = os.path.join(data_dir, 'Kids_reflect_sub_4steps.csv')\n",
    "\n",
    "# Load the data\n",
    "data = load_data(data_file_path, file_type='csv', delimiter=';')\n",
    "\n",
    "# Preview the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ocler\\Documents\\Académique\\Inria\\qualitative_analysis_project\\qualitative_analysis\\data_processing.py:139: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df.applymap(\n"
     ]
    }
   ],
   "source": [
    "# 1) Define a mapping from old column names to new names\n",
    "rename_map = {\n",
    "    \"ref\": \"reference\",\n",
    "    \"txt1.ctrl1\": \"Identify\",\n",
    "    \"txt1.det\": \"Guess\",\n",
    "    \"txt1.exp\": \"Seek\",\n",
    "    \"txt1.ctrl2\": \"Assess\"\n",
    "}\n",
    "\n",
    "# 2) Rename the columns in the DataFrame\n",
    "data = data.rename(columns=rename_map)\n",
    "\n",
    "# 3) Now define the new column names for cleaning\n",
    "text_columns = [\"reference\", \"Identify\", \"Guess\", \"Seek\", \"Assess\"]\n",
    "\n",
    "# 4) Clean and normalize the new columns\n",
    "for col in text_columns:\n",
    "    data[col] = clean_and_normalize(data[col])\n",
    "\n",
    "# 5) Sanitize the DataFrame\n",
    "data = sanitize_dataframe(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of verbatims: 45\n",
      "Verbatim example:\n",
      "Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche quon appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : cest ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit quun volcan est endormi sil ny a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\n",
      "\n",
      "Identify: Le manteau,le centre de la Terre\n",
      "Guess: Je pense que le manteau et le centre de la Terre sont les màames choses\n",
      "Seek: Le manteau et le centre de la Terre ne font ils qu'un?\n",
      "Assess: Je n'ai pas pu trouver la réponse à  ma question.\n"
     ]
    }
   ],
   "source": [
    "# Combine texts and entries\n",
    "\n",
    "data['verbatim'] = data.apply(\n",
    "    lambda row: (\n",
    "        f\"Reference: {row['reference']}\\n\\n\"\n",
    "        f\"Identify: {row['Identify']}\\n\"\n",
    "        f\"Guess: {row['Guess']}\\n\"\n",
    "        f\"Seek: {row['Seek']}\\n\"\n",
    "        f\"Assess: {row['Assess']}\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Extract the list of verbatims\n",
    "verbatims = data['verbatim'].tolist()\n",
    "\n",
    "print(f\"Total number of verbatims: {len(verbatims)}\")\n",
    "print(f\"Verbatim example:\\n{verbatims[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the provider and model\n",
    "provider = 'azure'\n",
    "model_name = 'gpt-4o-mini'\n",
    "\n",
    "# Initialize the client\n",
    "llm_client = get_llm_client(provider=provider, config=config.MODEL_CONFIG[provider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing Verbatim 1/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 297 (prompt) + 1 (completion) = 298 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 296 (prompt) + 1 (completion) = 297 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 296 (prompt) + 1 (completion) = 297 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 306 (prompt) + 1 (completion) = 307 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 301 (prompt) + 1 (completion) = 302 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 1\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 305 (prompt) + 1 (completion) = 306 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 307 (prompt) + 1 (completion) = 308 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 2/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 295 (prompt) + 1 (completion) = 296 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 294 (prompt) + 1 (completion) = 295 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 294 (prompt) + 1 (completion) = 295 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 304 (prompt) + 1 (completion) = 305 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 299 (prompt) + 1 (completion) = 300 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 1\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 303 (prompt) + 1 (completion) = 304 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 305 (prompt) + 1 (completion) = 306 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 3/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 266 (prompt) + 1 (completion) = 267 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 265 (prompt) + 1 (completion) = 266 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 265 (prompt) + 1 (completion) = 266 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 275 (prompt) + 1 (completion) = 276 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 270 (prompt) + 1 (completion) = 271 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 274 (prompt) + 1 (completion) = 275 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 276 (prompt) + 1 (completion) = 277 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 4/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 292 (prompt) + 1 (completion) = 293 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 291 (prompt) + 1 (completion) = 292 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 291 (prompt) + 1 (completion) = 292 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 301 (prompt) + 1 (completion) = 302 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 296 (prompt) + 1 (completion) = 297 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 1\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 300 (prompt) + 1 (completion) = 301 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 302 (prompt) + 1 (completion) = 303 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 0\n",
      "=== Processing Verbatim 5/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 284 (prompt) + 1 (completion) = 285 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 283 (prompt) + 1 (completion) = 284 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 283 (prompt) + 1 (completion) = 284 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 293 (prompt) + 1 (completion) = 294 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 288 (prompt) + 1 (completion) = 289 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 292 (prompt) + 1 (completion) = 293 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 294 (prompt) + 1 (completion) = 295 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 6/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 282 (prompt) + 1 (completion) = 283 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 0\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 281 (prompt) + 1 (completion) = 282 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 281 (prompt) + 1 (completion) = 282 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 291 (prompt) + 1 (completion) = 292 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 286 (prompt) + 1 (completion) = 287 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 290 (prompt) + 1 (completion) = 291 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 292 (prompt) + 1 (completion) = 293 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 7/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 277 (prompt) + 1 (completion) = 278 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 276 (prompt) + 1 (completion) = 277 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 276 (prompt) + 1 (completion) = 277 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 286 (prompt) + 1 (completion) = 287 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 281 (prompt) + 1 (completion) = 282 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 285 (prompt) + 1 (completion) = 286 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 287 (prompt) + 1 (completion) = 288 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 8/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 297 (prompt) + 1 (completion) = 298 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 296 (prompt) + 1 (completion) = 297 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 296 (prompt) + 1 (completion) = 297 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 306 (prompt) + 1 (completion) = 307 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 301 (prompt) + 1 (completion) = 302 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 305 (prompt) + 1 (completion) = 306 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 307 (prompt) + 1 (completion) = 308 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 9/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 288 (prompt) + 1 (completion) = 289 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 287 (prompt) + 1 (completion) = 288 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 287 (prompt) + 1 (completion) = 288 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 297 (prompt) + 1 (completion) = 298 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 292 (prompt) + 1 (completion) = 293 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 296 (prompt) + 1 (completion) = 297 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 298 (prompt) + 1 (completion) = 299 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 10/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 280 (prompt) + 1 (completion) = 281 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 279 (prompt) + 1 (completion) = 280 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 279 (prompt) + 1 (completion) = 280 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 289 (prompt) + 1 (completion) = 290 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 284 (prompt) + 1 (completion) = 285 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 1\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 288 (prompt) + 1 (completion) = 289 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 290 (prompt) + 1 (completion) = 291 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 11/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 284 (prompt) + 1 (completion) = 285 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 283 (prompt) + 1 (completion) = 284 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 283 (prompt) + 1 (completion) = 284 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 293 (prompt) + 1 (completion) = 294 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 288 (prompt) + 1 (completion) = 289 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 1\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 292 (prompt) + 1 (completion) = 293 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 294 (prompt) + 1 (completion) = 295 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 12/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 274 (prompt) + 1 (completion) = 275 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 273 (prompt) + 1 (completion) = 274 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 273 (prompt) + 1 (completion) = 274 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 283 (prompt) + 1 (completion) = 284 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 278 (prompt) + 1 (completion) = 279 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 282 (prompt) + 1 (completion) = 283 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 284 (prompt) + 1 (completion) = 285 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 13/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 258 (prompt) + 1 (completion) = 259 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 0\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 257 (prompt) + 1 (completion) = 258 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 257 (prompt) + 1 (completion) = 258 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 0\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 267 (prompt) + 1 (completion) = 268 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 262 (prompt) + 1 (completion) = 263 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 266 (prompt) + 1 (completion) = 267 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 268 (prompt) + 1 (completion) = 269 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 14/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 282 (prompt) + 1 (completion) = 283 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 281 (prompt) + 1 (completion) = 282 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 0\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 281 (prompt) + 1 (completion) = 282 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 291 (prompt) + 1 (completion) = 292 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 286 (prompt) + 1 (completion) = 287 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 0\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 290 (prompt) + 1 (completion) = 291 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 292 (prompt) + 1 (completion) = 293 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "=== Processing Verbatim 15/15 ===\n",
      "\n",
      "--- Evaluating Theme: Identify Step ---\n",
      "Tokens Used: 267 (prompt) + 1 (completion) = 268 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Identify Step': 1\n",
      "\n",
      "--- Evaluating Theme: Guess Step ---\n",
      "Tokens Used: 266 (prompt) + 1 (completion) = 267 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Guess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Step ---\n",
      "Tokens Used: 266 (prompt) + 1 (completion) = 267 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Step': 1\n",
      "\n",
      "--- Evaluating Theme: Assess Step ---\n",
      "Tokens Used: 276 (prompt) + 1 (completion) = 277 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Assess Step': 1\n",
      "\n",
      "--- Evaluating Theme: Consistency ---\n",
      "Tokens Used: 271 (prompt) + 1 (completion) = 272 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Consistency': 1\n",
      "\n",
      "--- Evaluating Theme: Reference Link ---\n",
      "Tokens Used: 275 (prompt) + 1 (completion) = 276 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Reference Link': 1\n",
      "\n",
      "--- Evaluating Theme: Seek Question Originality ---\n",
      "Tokens Used: 277 (prompt) + 1 (completion) = 278 total\n",
      "Cost for this call: $0.0000\n",
      "Extracted Score for 'Seek Question Originality': 1\n",
      "\n",
      "=== Processing Complete ===\n",
      "Total Tokens Used: 30101\n",
      "Total Cost for Processing: $0.0046\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Le manteau,le centre de la Terre\\nGuess: Je pense que le manteau et le centre de la Terre sont les màames choses\\nSeek: Le manteau et le centre de la Terre ne font ils qu'un?\\nAssess: Je n'ai pas pu trouver la réponse à  ma question.\", 'Overall_Validity': 1}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Température du magma\\nGuess: La température du magma s'élève à  plus de 1000 °c\\nSeek: à combien de  °cle magma est?\\nAssess: La température du magma atteint les 1000 °C\", 'Overall_Validity': 1}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Pourqoi on l'appelle le manteau.\\nGuess: le magma et une pierre.\\nSeek: quel et la temperature magma\\nAssess: 1 000c\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: La température du magma\\nGuess: La température du magma dépasse les 500 °C\\nSeek: Quel est la température du magma ?\\nAssess: La température du magma atteint les 1000 °C\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Combien de couches y-t-il ?\\nGuess: Il existe d'autre couches dans la terre à  part le manteau .\\nSeek: Quelles sont les autres couches et combien sont-elles ?\\nAssess: La terre contient 7 couches  dont le noyau.\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Pourquoi ont apelle le liquide le magma.\\nGuess: Moi je crois que le magma est t' une ville d'Afrique.\\nSeek: Quelle est la température du m agma.\\nAssess: Je n'et pas trouvé.\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Les Couches de terre\\nGuess: Il existe d 'autres couches dans le manteau .\\nSeek: Quelle sont les autres couches de la terre\\nAssess: La terre contient 7 chouches dont le noyau.\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Le magma est le responsable de l'explosion car c'est le coeur du volcan.\\nGuess: IL existe d'autres couches dans la terre à  par le manteau.\\nSeek: Que se passe-t-il pendant l'éruption du volcan\\nAssess: IL y a d'autres couches dans la terre dont le noyau\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Si le<<manteau>>existé pas qu'est qui ce passeré\\nGuess: Le manteau est chaud ou froid?\\nSeek: Si on y va on peut mourir\\nAssess: Je n'est pas la raiponce de la question!!!!!!!!!!!!!!!!!!!?!!!!!\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: température du magma\\nGuess: la temperature du magma depasse les 500 C\\nSeek: quelle est la temperature du magma\\nAssess: La température du magma atteint led 1000 degres\", 'Overall_Validity': 1}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: le magma\\nGuess: La température du magma dépasse les 500c\\nSeek: Quelle est la température du magma?\\nAssess: La température du magma atteint les 1000c\", 'Overall_Validity': 1}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: Les roches sont liquide\\nGuess: Durant l'éruption le volcan fait sortir du magma\\nSeek: Pourquoi le liquide s' appelle le magma\\nAssess: Combien de couche contient le noyau\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: j'ai pas trouver\\nGuess: ...\\nSeek: J'AI AUCUNNE IDEà\\x89\\nAssess: ...\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: comment le magma explosent si c'est une substance liquide?\\nGuess: La température du magma dépasse les 500 \\x84\\x83\\nSeek: comment le magma explosent ?\\nAssess: je n'est pas trouver\", 'Overall_Validity': 0}\n",
      "{'Verbatim': \"Reference: À des dizaines de kilomètres sous nos pieds, la terre contient une couche qu\\x92on appelle le manteau. Il fait tellement chaud sur cette couche que les roches deviennent liquides : c\\x92est ce qu'on appelle le magma. Le magma est responsable de l'explosion des volcans: c'est ce que les scientifiques appellent aussi une éruption d'un volcan. On dit qu\\x92un volcan est endormi s\\x92il n\\x92y a eu aucune éruption dans les 10 000 dernières années. Au delà de 10 000 ans, on peut dire que le volcan est éteint.\\n\\nIdentify: C'est quoi le magma\\nGuess: Je pense que le magma est de la lave\\nSeek: Pourquoi appel-ton le magma\\nAssess: Je n'ai pas trouvé\", 'Overall_Validity': 1}\n"
     ]
    }
   ],
   "source": [
    "# Define column descriptions\n",
    "column_descriptions = {\n",
    "    \"ID\": \"Unique identifier for each entry\",\n",
    "    \"reference\": \"The reference text that participants must read beforehand. Their responses for the different steps must be semantically related to this text (same topic), but the answer to the question they are asking should not be found in the text.\",\n",
    "    \"Identify\": \"Response for the IDENTIFY step\",\n",
    "    \"Guess\": \"Response for the GUESS step\",\n",
    "    \"Seek\": \"Response for the SEEK step\",\n",
    "    \"Assess\": \"Response for the ASSESS step\"\n",
    "}\n",
    "\n",
    "# Define criteria as themes\n",
    "codebook = {\n",
    "    \"Identify Step\": \"Does the Identify step indicate a topic of interest?\",\n",
    "    \"Guess Step\": \"Does the Guess step suggest a possible explanation?\",\n",
    "    \"Seek Step\": \"Is the Seek step formulated as a question?\",\n",
    "    \"Assess Step\": \"Does it identify a possible answer or state that no answer was found ('no' is ok)?\",\n",
    "    \"Consistency\": \"Are the Identify, Guess, and Seek steps related to the same question?\",\n",
    "    \"Reference Link\": \"Are the Identify, Guess, and Seek steps related to the topic of the reference text?\",\n",
    "    \"Seek Question Originality\": \"Is the answer to the Seek question not found (even vaguely) in the reference text?\",\n",
    "}\n",
    "\n",
    "# Build data format description\n",
    "data_format_description = build_data_format_description(column_descriptions)\n",
    "\n",
    "# Add classification flags\n",
    "binary_classification = True  # Set to False for multi-class classification\n",
    "post_reasoning = True  # Set to False for single-step classification\n",
    "\n",
    "# Define queries to be used globally\n",
    "binary_query = \"Reply with '1' if the entry meets the criterion or '0' otherwise.\"\n",
    "multiclass_query = \"Reply with a number corresponding to the category.\"\n",
    "post_reasoning_query = \"First, generate a one-sentence reasoning about the classification.\"\n",
    "\n",
    "def construct_prompt(verbatim, theme=None, theme_description=None, binary_classification=True, codebook=None, post_reasoning=False):\n",
    "    \"\"\"\n",
    "    Constructs a prompt for evaluating a specific criterion (theme) for a given verbatim or for multi-class classification.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant tasked with evaluating the following entry.\n",
    "\n",
    "You are provided with data entries in the following format:\n",
    "\n",
    "{data_format_description}\n",
    "\"\"\"\n",
    "\n",
    "    if binary_classification:\n",
    "        assert theme and theme_description, \"Theme and its description must be provided for binary classification.\"\n",
    "        prompt += f\"\"\"\n",
    "**Entry:**\n",
    "{verbatim}\n",
    "\n",
    "**Criterion:**\n",
    "{theme} - {theme_description}\n",
    "{binary_query}\n",
    "\"\"\"\n",
    "    else:\n",
    "        assert codebook is not None, \"Codebook is required for multi-class classification.\"\n",
    "        prompt += \"\"\"\n",
    "The data consists of entries that need to be classified into one of the following categories:\n",
    "------------------------------\n",
    "\"\"\"\n",
    "        for theme_i, (theme, theme_description) in enumerate(codebook.items()):\n",
    "            prompt += f\"{theme_i} - {theme}: {theme_description}\\n\"\n",
    "        prompt += f\"\"\"------------------------------\n",
    "        **Entry Description:**\n",
    "        {verbatim}\n",
    "        {multiclass_query}\n",
    "        \"\"\"\n",
    "\n",
    "    if post_reasoning:\n",
    "        return prompt, post_reasoning_query\n",
    "\n",
    "    return prompt, None\n",
    "\n",
    "def generate_with_reasoning(llm_client, model_name, prompt, reasoning_query=None, temperature=0.0001, verbose=False):\n",
    "    \"\"\"\n",
    "    Handles both single-step and two-step LLM calls based on whether reasoning_query is provided.\n",
    "    \"\"\"\n",
    "    # First call for reasoning (if applicable)\n",
    "    if reasoning_query:\n",
    "        reasoning_prompt = f\"{prompt}\\n\\n{reasoning_query}\"\n",
    "        response_text_1, usage_1 = llm_client.get_response(\n",
    "            prompt=reasoning_prompt,\n",
    "            model=model_name,\n",
    "            max_tokens=500,\n",
    "            temperature=temperature,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        # Second call for classification\n",
    "        classification_prompt = f\"{prompt}\\n\\nReasoning:\\n{response_text_1}\\n\\nProvide the final classification.\"\n",
    "        response_text_2, usage_2 = llm_client.get_response(\n",
    "            prompt=classification_prompt,\n",
    "            model=model_name,\n",
    "            max_tokens=500,\n",
    "            temperature=temperature,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        return response_text_2, usage_1 + usage_2\n",
    "\n",
    "    # Single-step classification\n",
    "    response_text, usage = llm_client.get_response(\n",
    "        prompt=prompt,\n",
    "        model=model_name,\n",
    "        max_tokens=500,\n",
    "        temperature=temperature,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    return response_text, usage\n",
    "\n",
    "# Select subset of verbatims for testing\n",
    "verbatims_subset = verbatims[:15]\n",
    "\n",
    "# Initialize results, total cost, and token counters\n",
    "results = []\n",
    "total_tokens_used = 0\n",
    "total_cost = 0\n",
    "\n",
    "for idx, verbatim in enumerate(verbatims_subset):\n",
    "    print(f\"=== Processing Verbatim {idx + 1}/{len(verbatims_subset)} ===\")\n",
    "\n",
    "    if binary_classification:\n",
    "        for theme, theme_description in codebook.items():\n",
    "            print(f\"\\n--- Evaluating Theme: {theme} ---\")\n",
    "\n",
    "            # Build the prompt\n",
    "            prompt, reasoning_query = construct_prompt(verbatim, theme, theme_description, binary_classification=True, post_reasoning=post_reasoning)\n",
    "\n",
    "            try:\n",
    "                response_content, usage = generate_with_reasoning(\n",
    "                    llm_client=llm_client,\n",
    "                    model_name=model_name,\n",
    "                    prompt=prompt,\n",
    "                    reasoning_query=reasoning_query,\n",
    "                    temperature=0.0001,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                # Track token usage\n",
    "                if usage:\n",
    "                    total_tokens_used += usage.total_tokens\n",
    "\n",
    "                # Parse response\n",
    "                score = extract_code_from_response(response_content)\n",
    "                if score in [0, 1]:\n",
    "                    results.append({\n",
    "                        'Verbatim': verbatim,\n",
    "                        'Theme': theme,\n",
    "                        'Score': score\n",
    "                    })\n",
    "                    print(f\"Extracted Score for '{theme}': {score}\")\n",
    "                else:\n",
    "                    print(f\"Failed to parse a valid score for '{theme}'\")\n",
    "                    results.append({\n",
    "                        'Verbatim': verbatim,\n",
    "                        'Theme': theme,\n",
    "                        'Score': None\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing Verbatim {idx + 1} for Theme '{theme}': {e}\")\n",
    "                results.append({\n",
    "                    'Verbatim': verbatim,\n",
    "                    'Theme': theme,\n",
    "                    'Score': None\n",
    "                })\n",
    "    else:\n",
    "        # Multi-class classification\n",
    "        prompt, reasoning_query = construct_prompt(verbatim, binary_classification=False, codebook=codebook, post_reasoning=post_reasoning)\n",
    "\n",
    "        try:\n",
    "            response_content, usage = generate_with_reasoning(\n",
    "                llm_client=llm_client,\n",
    "                model_name=model_name,\n",
    "                prompt=prompt,\n",
    "                reasoning_query=reasoning_query,\n",
    "                temperature=0.0001,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Track token usage\n",
    "            if usage:\n",
    "                total_tokens_used += usage.total_tokens\n",
    "\n",
    "            # Parse response\n",
    "            score = extract_code_from_response(response_content)\n",
    "            results.append({\n",
    "                'Verbatim': verbatim,\n",
    "                'Label': score\n",
    "            })\n",
    "            print(f\"Extracted Multi-Class Label: {score}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Verbatim {idx + 1} for Multi-Class Classification: {e}\")\n",
    "            results.append({\n",
    "                'Verbatim': verbatim,\n",
    "                'Label': None\n",
    "            })\n",
    "\n",
    "# Print the final summary after all calls\n",
    "print(\"\\n=== Processing Complete ===\")\n",
    "print(f\"Total Tokens Used: {total_tokens_used}\")\n",
    "print(f\"Total Cost for Processing: ${total_cost:.4f}\")\n",
    "\n",
    "# Organize scores by verbatim for binary classification\n",
    "if binary_classification:\n",
    "    verbatim_scores = defaultdict(dict)\n",
    "    for entry in results:\n",
    "        verbatim = entry['Verbatim']\n",
    "        theme = entry['Theme']\n",
    "        score = entry['Score']\n",
    "        verbatim_scores[verbatim][theme] = score\n",
    "\n",
    "    # Determine overall validity\n",
    "    final_results = []\n",
    "    for verbatim, scores in verbatim_scores.items():\n",
    "        overall_validity = 1  # Assume valid\n",
    "        for theme, score in scores.items():\n",
    "            if score != 1:\n",
    "                overall_validity = 0\n",
    "                break\n",
    "        final_results.append({\n",
    "            'Verbatim': verbatim,\n",
    "            'Overall_Validity': overall_validity\n",
    "        })\n",
    "\n",
    "    # Optionally, print the final results\n",
    "    for result in final_results:\n",
    "        print(result)\n",
    "else:\n",
    "    # Print multi-class classification results\n",
    "    for result in results:\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: C:/Users/ocler/Documents/Académique/Inria/Kids_Reflect/Data/outputs\\experiment_gpt-4o-mini_20241218_132349.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# Define the save path\n",
    "outputs_dir = os.path.join(data_dir, 'outputs')\n",
    "os.makedirs(outputs_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "save_path = os.path.join(outputs_dir, f\"experiment_{model_name.replace('/', '_')}_{timestamp}.csv\")\n",
    "\n",
    "# Save results\n",
    "save_results_to_csv(\n",
    "    coding=results_df.to_dict('records'),\n",
    "    save_path=save_path,\n",
    "    fieldnames=['Verbatim', 'Overall_Validity'],\n",
    "    verbatims=None  # Verbatims are included in the results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results loaded from: C:/Users/ocler/Documents/Académique/Inria/Kids_Reflect/Data/outputs\\experiment_gpt-4o-mini_20241218_132349.csv\n"
     ]
    }
   ],
   "source": [
    "# Load results\n",
    "loaded_results = load_results_from_csv(save_path)\n",
    "# The function returns (verbatims, coding)\n",
    "verbatims_loaded, coding_loaded = loaded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Score between human annotations and model: 0.35\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have human annotations in the data\n",
    "human_annotations = data['corr_cycle1'].tolist()  # Replace with actual column name\n",
    "model_coding = results_df['Overall_Validity'].tolist()\n",
    "\n",
    "human_annotations_short = human_annotations[:15]\n",
    "\n",
    "# Compute Cohen's Kappa\n",
    "kappa = compute_cohens_kappa(\n",
    "    human_annotations_short,\n",
    "    model_coding,\n",
    "    labels=[0, 1],\n",
    "    weights='linear'\n",
    ")\n",
    "\n",
    "print(f\"Cohen's Kappa Score between human annotations and model: {kappa:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(human_annotations_short, model_coding, labels=[0, 1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
