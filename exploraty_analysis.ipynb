{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Import functions from your qualitative_analysis package\n",
    "from qualitative_analysis.data_processing import load_data, clean_and_normalize, sanitize_dataframe\n",
    "from qualitative_analysis.prompt_construction import construct_prompt\n",
    "from qualitative_analysis.model_interaction import get_llm_client\n",
    "from qualitative_analysis.response_parsing import extract_code_from_response\n",
    "from qualitative_analysis.evaluation import compute_cohens_kappa\n",
    "from qualitative_analysis.utils import save_results_to_csv, load_results_from_csv\n",
    "import qualitative_analysis.config as config\n",
    "\n",
    "# Additional libraries\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = 'data'  # Adjust the path as needed\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your dataset\n",
    "data_file_path = os.path.join(data_dir, 'datasets', 'divergent_questions_data', 'qa_data_RE.csv')\n",
    "\n",
    "# Load the data\n",
    "data = load_data(data_file_path, file_type='csv', delimiter=',')  # Adjust delimiter if needed\n",
    "\n",
    "# Preview the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text columns to clean\n",
    "text_columns = ['text', 'questions']  # Replace with your actual column names\n",
    "\n",
    "# Clean and normalize text columns\n",
    "for col in text_columns:\n",
    "    data[col] = clean_and_normalize(data[col])\n",
    "\n",
    "# Sanitize the DataFrame\n",
    "data = sanitize_dataframe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine texts and questions\n",
    "data['verbatim'] = data.apply(lambda row: f\"Text: {row['text']}\\n\\nQuestion: {row['questions']}\", axis=1)\n",
    "\n",
    "# Extract the list of verbatims\n",
    "verbatims = data['verbatim'].tolist()\n",
    "\n",
    "print(f\"Total number of verbatims: {len(verbatims)}\")\n",
    "print(f\"Verbatim example:\\n{verbatims[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qualitative_analysis.prompt_construction import construct_prompt as base_construct_prompt\n",
    "\n",
    "def construct_prompt(verbatim):\n",
    "    # Your custom prompt construction logic\n",
    "    codebook = \"\"\"\n",
    "    Instructions for classifying the question based on the text:\n",
    "\n",
    "    - 0: The answer to the question is explicitly stated in the text.\n",
    "    - 1: The answer to the question is not explicitly stated but can be implied from the text.\n",
    "    - 2: The answer to the question is not at all stated in the text.\n",
    "    \"\"\"\n",
    "\n",
    "    examples = \"\"\"\n",
    "    [Include your examples here as in your original prompt]\n",
    "    \"\"\"\n",
    "\n",
    "    instructions = \"You are a helpful assistant tasked with classifying questions with respect to a given text.\"\n",
    "\n",
    "    # Build the prompt using the base_construct_prompt function\n",
    "    prompt = base_construct_prompt(\n",
    "        data_format_description=\"\",\n",
    "        entry_text=verbatim,\n",
    "        codebook=codebook,\n",
    "        examples=examples,\n",
    "        instructions=instructions,\n",
    "        selected_fields=['Score'],\n",
    "        output_format_example={'Score': '0'}\n",
    "    )\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the provider and model\n",
    "provider = 'together'  # or 'azure' if using OpenAI\n",
    "model_name = 'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo'  # Adjust as needed\n",
    "\n",
    "# Initialize the client\n",
    "llm_client = get_llm_client(provider=provider, config=config.MODEL_CONFIG[provider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "total_tokens_used = 0\n",
    "\n",
    "for idx, verbatim in enumerate(verbatims):\n",
    "    print(f\"Processing Verbatim {idx+1}/{len(verbatims)}\")\n",
    "    prompt = construct_prompt(verbatim)\n",
    "    \n",
    "    try:\n",
    "        response = llm_client.get_response(\n",
    "            prompt=prompt,\n",
    "            model=model_name,\n",
    "            max_tokens=500,\n",
    "            temperature=0.0001,\n",
    "            verbose=False\n",
    "        )\n",
    "        # Parse the response using the backend function\n",
    "        score = extract_code_from_response(response)\n",
    "        if score is not None:\n",
    "            results.append({\n",
    "                'Verbatim': verbatim,\n",
    "                'Score': score\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Failed to parse response for Verbatim {idx+1}\")\n",
    "            results.append({\n",
    "                'Verbatim': verbatim,\n",
    "                'Score': None\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Verbatim {idx+1}: {e}\")\n",
    "        results.append({\n",
    "            'Verbatim': verbatim,\n",
    "            'Score': None\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Define the save path\n",
    "outputs_dir = os.path.join(data_dir, 'outputs')\n",
    "os.makedirs(outputs_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "save_path = os.path.join(outputs_dir, f\"experiment_{model_name.replace('/', '_')}_{timestamp}.csv\")\n",
    "\n",
    "# Save results\n",
    "save_results_to_csv(\n",
    "    coding=results_df.to_dict('records'),\n",
    "    save_path=save_path,\n",
    "    fieldnames=['Verbatim', 'Score'],\n",
    "    verbatims=None  # Verbatims are included in the results\n",
    ")\n",
    "\n",
    "print(f\"Results saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "loaded_results = load_results_from_csv(save_path)\n",
    "# The function returns (verbatims, coding)\n",
    "verbatims_loaded, coding_loaded = loaded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have human annotations in the data\n",
    "human_annotations = data['div_rater1'].tolist()  # Replace with actual column name\n",
    "model_coding = results_df['Score'].tolist()\n",
    "\n",
    "# Compute Cohen's Kappa\n",
    "kappa = compute_cohens_kappa(\n",
    "    human_annotations,\n",
    "    model_coding,\n",
    "    labels=[0, 1, 2],\n",
    "    weights='linear'\n",
    ")\n",
    "\n",
    "print(f\"Cohen's Kappa Score between human annotations and model: {kappa:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(human_annotations, model_coding, labels=[0, 1, 2])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
