# OpenAI
OPENAI_API_KEY=your_openai_key_here

# Azure environment variables
AZURE_API_KEY=your_azure_api_key_here
AZURE_OPENAI_ENDPOINT=your_azure_endpoint_here
AZURE_API_VERSION=your_azure_api_version_here

# Together AI Configuration
TOGETHER_API_KEY=your_together_api_key_here

# Anthropic Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google Gemini Configuration
GEMINI_API_KEY=your_gemini_api_key_here

# OpenRouter Configuration
OPENROUTER_API_KEY=your_openrouter_api_key_here

# vLLM Configuration
# Path to the model or HuggingFace model ID (defaults to TinyLlama/TinyLlama-1.1B-Chat-v1.0 if not set)
VLLM_MODEL_PATH=TinyLlama/TinyLlama-1.1B-Chat-v1.0
# Data type for model weights (defaults to float16 if not set)
VLLM_DTYPE=float16
# Target GPU memory utilization (defaults to 0.95 if not set)
VLLM_GPU_MEMORY_UTILIZATION=0.95
# Number of GPUs to use for tensor parallelism (defaults to 4 if not set)
VLLM_TENSOR_PARALLEL_SIZE=4
# Maximum sequence length (defaults to 2048 if not set)
VLLM_MAX_MODEL_LEN=2048
# Enable prefix caching for better performance (defaults to true if not set)
VLLM_ENABLE_PREFIX_CACHING=true
# Worker multiprocessing method (defaults to spawn if not set)
VLLM_WORKER_MULTIPROC_METHOD=spawn
