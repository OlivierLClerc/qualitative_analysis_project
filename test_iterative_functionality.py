#!/usr/bin/env python3
"""
Test script to verify that the iterative prompt improvement functionality works correctly.

This script tests both:
1. Single iteration mode (max_iterations = 1) - backward compatibility
2. Multi-iteration mode (max_iterations > 1) - new functionality
"""

import pandas as pd
from qualitative_analysis.scenario_runner import run_scenarios


def create_test_data():
    """Create a small test dataset for validation."""
    test_data = pd.DataFrame(
        {
            "verbatim": [
                "rÃ©ponse_attendue: The answer is 42\n\nrÃ©ponse_llm: The answer is 42\n\n",
                "rÃ©ponse_attendue: The answer is 42\n\nrÃ©ponse_llm: I don't know\n\n",
                "rÃ©ponse_attendue: The answer is 42\n\nrÃ©ponse_llm: The answer is 43\n\n",
                "rÃ©ponse_attendue: The answer is 42\n\nrÃ©ponse_llm: The answer is forty-two\n\n",
                "rÃ©ponse_attendue: The answer is 42\n\nrÃ©ponse_llm: 42 is the answer\n\n",
            ],
            "Rater_Oli": [1, 0, 0, 1, 1],
            "Rater_chloe": [1, 0, 0, 1, 1],
            "Rater_RA": [1, 0, 0, 1, 1],
            "iteration": [1, 1, 1, 1, 1],
        }
    )
    return test_data


def test_single_iteration():
    """Test single iteration mode (backward compatibility)."""
    print("=== Testing Single Iteration Mode (Backward Compatibility) ===")

    # Create test scenario with max_iterations = 1 (or not specified)
    single_iteration_scenario = [
        {
            # LLM settings
            "provider_llm1": "azure",
            "model_name_llm1": "gpt-4o",
            "temperature_llm1": 0,
            "prompt_name": "single_iteration_test",
            "subsample_size": -1,
            # Prompt
            "template": """
You are evaluating data entries.

Data columns:
- "rÃ©ponse_attendue": Expected answer
- "rÃ©ponse_llm": LLM response to evaluate

Entry to evaluate:
{verbatim_text}

Task: Evaluate if rÃ©ponse_llm matches rÃ©ponse_attendue.
0: Does not match
1: Matches
""",
            # Output
            "selected_fields": ["Classification", "Reasoning"],
            "prefix": "Classification",
            "label_type": "int",
            "response_template": """
Please follow the JSON format:
```json
{{
  "Reasoning": "Your reasoning here",
  "Classification": "Your integer here"
}}
""",
            "json_output": True,
            # Single iteration settings
            "max_iterations": 1,  # This should use process_scenario_raw
            "use_validation_set": False,
            "n_completions": 1,
        }
    ]

    test_data = create_test_data()
    annotation_columns = ["Rater_Oli", "Rater_chloe", "Rater_RA"]
    labels = [0, 1]

    try:
        results = run_scenarios(
            scenarios=single_iteration_scenario,
            data=test_data,
            annotation_columns=annotation_columns,
            labels=labels,
            n_runs=1,
            verbose=True,
        )

        print("âœ… Single iteration test passed!")
        print(f"   - Results shape: {results.shape}")
        print(f"   - Columns: {list(results.columns)}")
        print(f"   - Iteration values: {results['iteration'].unique()}")

        # Check that iteration column shows 1 (single iteration)
        assert all(
            results["iteration"] == 1
        ), "Single iteration should have iteration=1"

        return True

    except Exception as e:
        print(f"âŒ Single iteration test failed: {e}")
        return False


def test_multi_iteration():
    """Test multi-iteration mode (new functionality)."""
    print("\n=== Testing Multi-Iteration Mode (New Functionality) ===")

    # Create test scenario with max_iterations > 1
    multi_iteration_scenario = [
        {
            # LLM settings
            "provider_llm1": "azure",
            "model_name_llm1": "gpt-4o",
            "temperature_llm1": 0,
            "prompt_name": "multi_iteration_test",
            "subsample_size": -1,
            # Prompt
            "template": """
You are evaluating data entries.

Data columns:
- "rÃ©ponse_attendue": Expected answer
- "rÃ©ponse_llm": LLM response to evaluate

Entry to evaluate:
{verbatim_text}

Task: Evaluate if rÃ©ponse_llm matches rÃ©ponse_attendue.
0: Does not match
1: Matches
""",
            # Output
            "selected_fields": ["Classification", "Reasoning"],
            "prefix": "Classification",
            "label_type": "int",
            "response_template": """
Please follow the JSON format:
```json
{{
  "Reasoning": "Your reasoning here",
  "Classification": "Your integer here"
}}
""",
            "json_output": True,
            # Multi-iteration settings
            "provider_llm2": "azure",
            "model_name_llm2": "gpt-4o",
            "temperature_llm2": 0.7,
            "max_iterations": 2,  # This should use process_scenario
            "use_validation_set": True,
            "validation_size": 2,
            "random_state": 42,
            "n_completions": 1,
        }
    ]

    test_data = create_test_data()
    annotation_columns = ["Rater_Oli", "Rater_chloe", "Rater_RA"]
    labels = [0, 1]

    try:
        results = run_scenarios(
            scenarios=multi_iteration_scenario,
            data=test_data,
            annotation_columns=annotation_columns,
            labels=labels,
            n_runs=1,
            verbose=True,
        )

        print("âœ… Multi-iteration test passed!")
        print(f"   - Results shape: {results.shape}")
        print(f"   - Columns: {list(results.columns)}")
        print(f"   - Iteration values: {results['iteration'].unique()}")

        # Check that iteration column shows max_iterations (2)
        assert all(
            results["iteration"] == 2
        ), "Multi-iteration should have iteration=max_iterations"

        # Check for additional columns that should be present in multi-iteration mode
        expected_columns = ["best_accuracy", "total_iterations"]
        for col in expected_columns:
            assert col in results.columns, f"Multi-iteration should have {col} column"

        return True

    except Exception as e:
        print(f"âŒ Multi-iteration test failed: {e}")
        return False


def main():
    """Run all tests."""
    print("Testing Iterative Prompt Improvement Functionality")
    print("=" * 60)

    # Test single iteration (backward compatibility)
    single_success = test_single_iteration()

    # Test multi-iteration (new functionality)
    multi_success = test_multi_iteration()

    print("\n" + "=" * 60)
    print("SUMMARY:")
    print(f"Single iteration test: {'âœ… PASSED' if single_success else 'âŒ FAILED'}")
    print(f"Multi-iteration test: {'âœ… PASSED' if multi_success else 'âŒ FAILED'}")

    if single_success and multi_success:
        print(
            "\nğŸ‰ All tests passed! The iterative functionality is working correctly."
        )
        print("\n" + "=" * 60)
        print("HOW THE SYSTEM WORKS:")
        print("=" * 60)
        print("\nğŸ“‹ SINGLE ITERATION (max_iterations = 1):")
        print("   For each scenario:")
        print("   â”œâ”€â”€ Run 1: Original prompt â†’ predictions")
        print("   â”œâ”€â”€ Run 2: Original prompt â†’ predictions")
        print("   â””â”€â”€ Run N: Original prompt â†’ predictions")
        print("   â†’ All runs use the SAME prompt for proper aggregation")

        print("\nğŸ”„ MULTI-ITERATION (max_iterations > 1):")
        print("   For each scenario:")
        print("   â”œâ”€â”€ ITERATIVE IMPROVEMENT PHASE:")
        print("   â”‚   â”œâ”€â”€ Iteration 1: Original prompt â†’ accuracy_1")
        print("   â”‚   â”œâ”€â”€ Iteration 2: LLM2 improves prompt â†’ accuracy_2")
        print("   â”‚   â””â”€â”€ Iteration N: LLM2 improves prompt â†’ accuracy_N")
        print("   â”‚   â†’ Select BEST prompt based on validation accuracy")
        print("   â”‚")
        print("   â””â”€â”€ CONSISTENCY MEASUREMENT PHASE:")
        print("       â”œâ”€â”€ Run 1: Best prompt â†’ predictions")
        print("       â”œâ”€â”€ Run 2: Best prompt â†’ predictions")
        print("       â””â”€â”€ Run N: Best prompt â†’ predictions")
        print("       â†’ All runs use the SAME best prompt for proper aggregation")

        print("\nâœ… BENEFITS:")
        print("   â€¢ Single iteration: Multiple runs with same prompt = proper metrics")
        print("   â€¢ Multi-iteration: Find best prompt once, then measure consistency")
        print("   â€¢ Metrics can be properly aggregated across runs")
        print("   â€¢ Perfect backward compatibility")

    else:
        print("\nâš ï¸  Some tests failed. Please check the implementation.")

    return single_success and multi_success


if __name__ == "__main__":
    main()
